---
title: 'Architecture-desgin-notes'
date: 2023-10-13
permalink: /posts/2023/10/architecture-Design-notes/
tags:
  - cool posts
  - category1
  - category2
---

System architecture notes lists.



# 架构-解决系统复杂度问题

* **架构设计的关键：判断和取舍【各种约束条件下，求得最优解并运用】**
* **程序设计的关键：逻辑和实现**
* 模块&组件：系统的组成部分，从不同角度拆分系统。
  * 模块：逻辑角度拆分为各个功能模块，如登录功能、验证功能等。【功能分离】
  * 组件：物理角度拆分为各个组件，如nginx、kafka等。【单元复用】

* 框架framework：组件规范or开发规范。
* 架构architecture：系统基础结构。
  * **软件架构：软件系统的<u>顶层</u>结构。**
* **<u>架构设计的目的：解决软件系统复杂度带来的问题。</u>**
  * 针对系统复杂点进行架构设计。
  * 架构设计并不需要面面俱到，有针对性的根据问题进行设计解决。
  * 对比自己业务复杂点，选择参考相似的方案。

**简单案例-大学学生管理系统**

* 需求：登录、注册、成绩管理、课程管理等。
* 性能分析：一个大学：1-2w人，平均每天每个学生访问次数约为1次左右, 性能不算分复杂。
  * 存储：mysql。
  * 缓存：暂不考虑。
  * web服务器：nginx代理转发。

* 可扩展性：学生管理系统功能稳定，扩展空间不大&扩展性不复杂。
* 高可用性：宕机 几小时，对学生管理工作影响不大，可以不做负载均衡，不考虑异地多活等复杂方案。
  * 若学生数据全部丢失，只能人工逐条修复，**需考虑存储高可靠**。**【复杂点-架构重点解决：数据库备份、机房备份】**
* 安全性：信息隐私【nginx提供ACL控制、用户账号密码管理、数据库访问权限控制】
* 成本：系统很简单，几台服务器就能够搞定，无需太多关注。

# 复杂度来源1-高性能

高性能复杂度来源：

* **单台计算机内部为了高性能带来的复杂度**
* **多台计算机集群为了高性能带来的复杂度**

## 单机复杂度

* 单机复杂度最关键点：操作系统。【操作系统发挥硬件性能，操作系统复杂度直接决定软件系统复杂度。】
* 操作系统和性能最相关点：**进程**和**线程**。
  * 进程：每个进程对应一个任务。每个任务有独立的内存空间，进程间互不相关，操作系统统一调度。
    * 多进程：CPU分时调度，已达到模拟多进程并行的作用。
    * 进程间通信：管道、消息队列、信号量、共享存储等。
  * 线程：解决单进程内部只能串行，不支持并行的问题，提高性能。
    * 线程：进程内部的子任务，进程中所有子任务共享进程的数据和资源。
    * 互斥锁：多线程中保证数据正确性-同一时间进程中只能有一个线程获得修改某数据的权限，A线程释放后，B线程才能继续申请修改。
    * **操作系统最小调度单位：线程**
    * **操作系统最小分配资源单位：进程**
  * 多进程&多线程：本质是分时系统，并不能做到时间上真正的并行。
* 真正并行：多个CPU同时执行计算任务。【3种常用架构】
  * SMP（Symmetric Multi-Processor，对称多处理器结构）
  * NUMA（Non-Uniform Memory Access，非一致存储访问结构）
  * MPP（Massive Parallel Processing，海量并行处理结构）

## 集群复杂度

集群并非单纯单机数量增加，需要考虑集群之间如何配合，从而达到高性能的目的。常见集群分配方式如下：

**任务分配**

* 每台机器都可以处理完整的业务任务，不同的任务分配到不同的机器上执行。

* 从1台增加到2台的过程，需要【按照逻辑进行任务分配、负载均衡】。
  
  <img src='/images/img/单机到2台扩展.png'>
  
  * 增加1台任务分配器【硬件网络设备（F5、交换机等）or 软件网络设备（LVS）or负载均衡软件（Nginx、HAProxy）or 自己开发的系统。
  * **任务分配器与真正业务服务器连接和交互**（即图中任务分配器到业务服务器的连接线），需要选择合适的连接方式，并且对连接进行管理。【连接建立、连接检测、连接中断后如何处理等。】
  * 任务分配器的**分配算法**。【轮询，权重分配，按负载分配】

* 上述假设扩展到2台后，理论上性能翻倍，但实际由于各种限制，可能需要打8折计算。
* 若性能要求继续提高，**增加服务器的同时还需增加任务分配器**。
  * 多台任务分配器之间需要明确的分配方式。
  * 任务分配器和服务器之间变成多对多的形式。
  * 多服务器的维护、状态管理、故障处理难度加大。

**任务分解**

* 通过任务分配，能突破单机处理性能的瓶颈，增加更多的机器来满足业务的性能需求。

* 若业务本身复杂度增高，单纯通过任务分配扩展性能的效果会越来越差。
* **分解粒度不能过细，否则调用次数激增导致性能更差。**

<img src='/images/img/任务分解.png'>

以微信架构为例，通过接入服务器后，在后台对各个子业务进行拆分，将复杂且巨大的业务拆分成多个小而简单的子系统，从以下方面提升性能：

* **系统越简单，越容易做到高性能**：简单系统影响性能的点更少，更容易有针对性的优化。
* **可以针对单个任务进行扩展**：各逻辑任务分解到独子系统后，整个系统性能瓶颈更容易发现，且发现后只需针对有瓶颈的子系统进行性能优化或者提升，不需要改动整个系统，风险会小很多。

# 复杂度来源2-高可用

**高可用的实现本质：冗余**【多台备用，增加冗余单元】

## 计算高可用

* **无论在哪台机器上进行计算，同样的算法和输入数据，产出的结果都是一样的**。所以将计算从一台机器迁移到另外一台机器，对业务没有影响。
* m台冗余计算单元的情况下，通过任务分配器后，可以分为：n主m-n备【结合实际业务判断】
* m台冗余计算单元的情况下，复杂度提高。

## 存储高可用

对需要存储数据的系统，整个系统高可用设计关键点和难点在于“存储高可用”。

**存储高可用的难点：不在于如何备份数据，而在于如何减少或者规避数据不一致对业务造成的影响**。

* 与计算相比，存储有一个本质区别：**将数据从一台机器搬到到另一台机器，需要经过线路进行传输**。
  * 不同线路、不同距离的传输速度不同。
  * **ms级延迟对人影响不大，但对高可用系统，在某个时间节点上，数据会存在不一致的情况。**【银行业务数据不同步，用户体验低甚至产生大问题】

* 传输线路本身也存在可用性问题：传输线路中断、拥塞、异常（错包、丢包）
* 传输线路故障时间一般都特别长，十几分钟到几个小时都有可能。【2015 年支付宝因为光缆被挖断，业务影响超过 4 个小时；2016 年中美海底光缆中断 3 小时】

* 存储高可用最多同时满足“一致性、可用性、分区容错性”中的两个，在做架构设计时需结合业务进行取舍。

## 高可用状态决策

* 计算&存储高可用基础都是“**状态决策**”：系统判断当前状态是否正常，若异常就采取行动来保证高可用。
* 若状态决策本身有错误或偏差，后续行动和处理无论多么完美也都没有意义和价值。
* 具体实践中，恰好存在一个本质的矛盾：**冗余实现的高可用系统，状态决策本质上就不可能完全正确**。
  * 独裁式：系统存在唯一决策者进行“状态决策”，不会出现决策混乱，但决策者本身故障时问题出现。
  * 协商式：两个独立的个体通过交流信息，然后根据规则进行决策，**最常用的协商式决策就是主备决策**。【关键点：两者信息交换出现问题-主备切换、增加连接等】
  * 民主式：多个独立个体间信息交换后进行投票决策，按照**多数取胜**的规则确定最终状态。【zk选举leader的方式】。
    * 民主式决策算法复杂：ZK 的选举算法 Paxos
    * 各节点连接切断后, 相互之间无法交互信息，导致可能选举出多个leader。【解决方案：投票结果超过一半为准，可能会因为可投票数量不足而导致永远无法完成选举】

# 复杂度来源3-可扩展性

良好的可扩展系统的两个基本条件：正确预测变化、完美封装变化。

## 预测变化

* 软件发布后还可以不断地修改：**不断有新的需求需要实现**。
* 预测变化的复杂性在于：
  - 不能每个设计点都考虑可扩展性。
  - 不能完全不考虑可扩展性。
  - 所有的预测都存在出错的可能性。

## 应对变化

即使预测很准确，如果方案不合适，则系统扩展一样很麻烦。常见的应对办法有如下几种：

* **将可能的"变化"封装在变化层，稳定不变的单独为一个”稳定层“**
  * 问题1：系统需要拆分出明确的变化层和稳定层。
  * 问题2：需要设计变化层与稳定层之间的接口。

* **提炼“abstract抽象层”和“实现层“：抽象层稳定、实现层根据业务需要变化。**

# 复杂度来源4-低成本、安全、规模

## 低成本

* 当架构方案只涉及几台或者十几台服务器时，一般情况下成本并不是我们重点关注的目标。**如果架构方案涉及几百上千甚至上万台服务器，成本就会变成一个非常重要的架构设计考虑点。**【eg: 两个架构设计方案：A 方案需要 10000 台机器，B 方案只需要 8000 台机器，从数量来看，B 方案节省 2000 台机器，1 台机器成本预算每年大约 2 万元，一年下来节省 4000 万元。】
* 低成本：架构设计的约束条件，非首要目标。【根据高性能、高可用的要求设计出方案时，评估方案是否满足成本目标，如果不行，就需要重新设计架构；如果无论如何都无法设计出满足成本要求的方案，那就只能找老板调整成本目标了。】

**低成本复杂度来源**：引入新技术or创造新技术来解决成本架构问题。常见例子如下：

- NoSQL（Memcache、Redis 等）：为了解决关系型数据库无法应对高并发访问带来的访问压力。
- 全文搜索引擎（Sphinx、Elasticsearch、Solr）：为了解决关系型数据库 like 搜索的低效的问题。
- Hadoop ：为了解决传统文件系统无法应对海量数据存储和计算的问题。

- **Facebook**： 为解决 PHP 低效问题，从 使用HipHop PHP【将 PHP 翻译为 C++ 执行】，改为 HHVM【PHP翻译为字节码后由虚拟机执行，类似与Java和JVM】。
- **新浪微博**：将Redis/MC + MySQL 方式，扩展为 Redis/MC+SSD Cache+MySQL 方式。SSD Cache 作为 L2 缓存使用。【解决MC/Redis 成本过高，容量小的问题 & 穿透 DB 带来的数据库访问压力】
- **Linkedin** ：为了处理每天 5 千亿的事件，开发了高效的 Kafka 消息系统。【**kafka设计出现原因**】

相比来说，创造新技术复杂度更高，因此一般中小公司基本都是靠引入新技术来达到低成本的目标；而大公司更有可能自己去创造新的技术来达到低成本的目标，因为大公司才有足够的资源、技术和时间去创造新技术。

## 安全

**功能安全**

* 防止攻击者对于系统漏洞的利用。
* 多与代码实现相关，与架构关系不大。【很多开发框架内嵌常见的安全功能，减少安全相关功能重复开发】
* 框架内嵌的安全功能无法预知新问题，且框架本身也存在很多漏洞。【Apache Struts2 多次爆出调用远程代码执行的高危漏洞】
* 功能安全是一个逐步完善的过程，且往往都在问题出现后才能有针对性的提出解决方案，我们永远无法预测系统下一个漏洞在哪里，也不敢说自己的系统肯定没有任何问题。

**架构安全**

* 功能安全是“防小偷”，那么**架构安全就是“防强盗”**。强盗会直接用大锤将门砸开，且很多时候就是故意搞破坏，对系统的影响也大得多。【理论上来说系统部署在互联网上时，全球任何地方都可以发起攻击。】
* **传统架构安全主要依靠防火墙**：最基本的功能就是隔离网络，将网络划分成不同的区域，制定出不同区域之间的**访问控制策略**来控制不同信任程度区域间传送的数据流。
  * 防火墙功能强大、性能一般，多用于传统银行&企业领域应用。
  * 对于海量用户访问的高并发情况，防火墙性能无法支撑。【DDoS：几GB-几十GB； 即使部署防火墙，多台的成本极高】
  * **防火墙能够保证内部系统不受冲击，但<u>DDoS最大的影响是消耗机房出口总带宽</u>，当出口带宽耗尽，整个业务从用户端就是不可用的**

基于上述原因，架构安全没有太好的设计手段来实现，更多**依靠运营商或者云服务商强大的带宽和流量清洗的能力，较少自己来设计和实现**。

## 规模

**规模带来复杂度的主要原因：“量变引起质变”**【数量超过一定的阈值后，复杂度会发生质的变化。】常见的规模带来的复杂度有：

* 功能越多，系统复杂度指数级上升：**系统的复杂度 = 功能数量 + 功能之间的连接数量。**

- 数据越多，系统复杂度发生质变：大数据。【目前的大数据理论基础是 Google 发表的三篇大数据相关论文】

  * Google File System：大数据文件存储的技术理论
  * Google Bigtable ：列式数据存储的技术理论
  * Google MapReduce：大数据运算的技术理论

  Mysql 单表数据一般推荐在 5000 万行左右，若单表数据达到了 10 亿行，就会产生很多问题，例如：

  * 添加索引会很慢，可能需要几个小时，这几个小时内数据库表是无法插入数据的，相当于业务停机了。
  * 修改表结构和添加索引存在类似的问题，耗时可能会很长。
  * 即使有索引，索引的性能也可能会很低，因为数据量太大。
  * 数据库备份耗时很长。

# 架构设计&流程

## 设计原则

* **“合适＞业界领先”**：技术的选择优先确保适合当前业务&人员。
* “简单＞复杂”：减少系统组件间故障 & 减少某个变化出现后对其他组件的影响 & 定位排查更简单。
* “演化>一步到位”：软件系统永远存在逐渐变化的过程，一步到位的情况基本不可能发生。

## 设计流程

* step1**识别复杂度**：大部分场景，复杂度只是其中一个，少数情况包含两个，若出现需要同时解决三个+以上的复杂度，说明这个系统之前设计有问题OR架构师判断失误。实际执行过程必须进行**【优先级排序】**。
* step2**设计备选方案**：
  * 备选方案以3-5个为最佳。
  * 各备选方案间差异要比较明显，突出各自区别。
  * 备选方案不局限与已熟悉的技术。
  * 备选方案不必过于详细，主要方案详细即可。
* step3**方案评估和选择**
  * 列出必须要关注的方案质量属性点，从这些点的角度去评估每个方案，再综合挑选最佳。
  * 常见方案质量属性点有：性能、可用性、硬件成本、项目投入、复杂度、安全性、可扩展性等。
  * 如果某质量属性和业务发展有关系，需要评估未来业务发展的规模时，一种简单的方式是将当前的业务规模乘以 2~4 即可。【eg：现在 TPS 是 1000，按照 TPS 4000 来设计方案；现在 TPS 是 10000，按照 TPS 20000 来设计方案。】
  * 按方案质量属性优先级选择最终方案。
* step4**详细方案设计**：此阶段可能遇到的极端情况就是发现备选方案不可行，主要的原因是备选方案设计时<u>遗漏了</u>**某个关键技术点或者关键的质量属性**。

# 高性能架构

## 数据库读写分离

* 本质：将DB访问压力分散到集群多个点，但没有分散存储压力。【**读写操作分散到不同的节点上**】
* 基本实现：
  * DB搭建主从集群。【“主从”而非“主备”】
  * **主服务器：读写；各个从服务器：读。**
  * 数据从主复制同步到各个从，每台服务器都存储了所有业务数据。
* **设计复杂度1-复制延迟**：若业务数据写入主服务器后立刻在从服务器上读取，数据可能尚未完成同步。
  * 解决方案1：读从机失败后再读一次主机【二次读取】
    * 和业务无需强绑定，但二次读取过多时将大大增加主机读操作压力。
  * 解决方案2：关键业务读写操作全部指向主机，非关键业务采用读写分离。
* **设计复杂度2-分配机制**：【读写分开访问不同数据库有两种方式】
  * 代码封装：代码中抽象一个数据访问层【请求实际发送到数据库之前】，实现读写操作分离和数据库服务器连接的管理。
    
    * 特点：实现简单、定制化开发、各语言间无法复用、故障情况需要修改配置。
    
  * 中间件封装：独立一套系统，实现读写操作分离和数据库服务器连接的管理。

    * 特点：支持多种语言、中间件性能要求高【所有请求都要经过中间件】、数据库
    * Mysql中间件：Mysql Router、Mysql Proxy


## 业务分库

**按业务模块将数据分散到不同的数据库服务器。**【将数据分散到不同服务器上，提高每个模块及综合系统的性能】

缺点：

* join操作：分库后，**原本同一个数据库多表之间的数据，无法使用 SQL 的 join 查询一次性联查**。【必须分多次到多个库中查询】
* 事务问题：分库后，无法通过同一个事务统一修改各个相关联的表。【必须分多次到多个库中修改，复杂度增加】
* 成本增加：业务数量+主备机器备份。

## 分表

* 水平分表：将一张表m条数据，n个字段的表拆成： a条数据，n个字段 和 b条数据，n个字段的表【m=a+b】
* 垂直分表：将一张表m条数据，n个字段的表拆成： m条数据，a个字段 和 m条数据，b个字段的表【n=a+b】
* 拆分之后的表之间需要有映射关系，方便后续查询。

**复杂性：**

* 垂直分表：**适合将表中不常用且占了大量空间的列拆分出去**。【eg： 假设我们是一个婚恋网站，用户在筛选其他用户时，主要是用 age 和 sex 字段进行查询，nickname 和 description 字段主要用于展示，一般不会在业务查询中用到。description 本身又比较长，因此可以将这两个字段独立到另外一张表中，查询 age 和 sex 时，能带来一定的性能提升。】
  * 复杂性：表操作的次数增加。
* 水平分表：**适合表行数特别大的表**。【基于表的访问性能，单表超过一定量就必须分表，**数量达到千万级别时注意。**】
  * 复杂性1：某条数据切分后属于哪个子表，需要增加路由算法进行计算。
    * **范围路由**：选取有序的数据列（时间戳）作为路由的条件，不同分段分散到不同的数据库表中。【表1：1-100；表2：101-201 以此类推】
      * 分段大小选取需要合适。
      * 可随数据增加平滑地扩充新的表。
    * **Hash 路由：**选取某个列（或者某几个列组合也可以）的值进行 Hash 运算，然后根据 Hash 结果分散到不同的数据库表中。
      * 初始表数量的选取需要合适，方便后续通过hash进行数据分表。
      * 表分布比较均匀
      * 扩充新的表很麻烦，所有数据都要重新计算hash。
    * **配置路由：**用一张独立的表来记录路由信息。
      * 设计简单灵活。
      * 扩充表时，只需迁移指定数据，然后修改路由表。
      * 缺点：必须多查询一次，会影响整体性能；路由表本身如果太大，性能同样可能成为瓶颈，再次将路由表分库分表【死循环】
  * 复杂性2：多表之间join查询操作次数增加，结果合并次数增加。
  * 复杂性3：多表之间计数需要count时，操作耗时增加。【解决方案：增加一张计数表，专门处理分表后数量统计的问题。】
  * 复杂性4：多表order by 操作无法实现。数据分散后只能通过代码读取各表数据后汇总排序。

## 负载均衡算法

- 任务平分类：收到的任务平均分给服务器处理，可以是绝对数量的平均，也可以是比例或者权重上的平均。
  * 轮询：收到请求后，按顺序轮流分配到各个服务器上。【即使有问题or系统不一样还是会轮询分配】
  * 加权轮询：解决服务器之间处理能力有差异的情况。
- 负载均衡类：根据服务器负载进行分配，可以是CPU 负载、连接数、I/O 使用率、吞吐量等。
  * 负载最低优先：将任务分配给当前负载最低的服务器。【根据实际场景确认负载定义】
- 性能最优类：根据服务器的响应时间来进行任务分配，优先将新任务分配给响应最快的服务器。
- Hash 类：根据任务中的关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上。

## 单服务器高性能

* PPC-process per connection【每次有新的连接就新建一个进程去专门处理这个连接的请求】

  * 模式实现简单，适合服务器连接数不多的情况。
  * 一般情况下，PPC 方案能处理的并发连接数量最大也就几百。

* prefork：解决PPC 模式中，连接进来才 fork 新进程处理连接请求导致访问较慢的情况。【提前创建进程prefork】

  * **多个子进程accept同一个 socket，新连接进入时，操作系统保证只有一个进程能最后 accept 成功**，
  
    PS: 所有未被成功accpet 阻塞的子进程都会被唤醒，导致不必要的进程调度和上下文切换了。[操作系统解决]
  
  * 和PPC 一样，父子进程通信复杂，并发连接数有限。
  
* TPC-Thread Per Connection 【每次有新的连接就新建一个线程去专门处理这个连接的请求】

  * 线程比进程更轻量级，创建线程的消耗比进程要少得多。
  * 多线程共享进程内存空间的，线程通信相比进程通信更简单。【解决PPC fork 代价过高和进程通信复杂的问题】
  * CONS1:高并发时（例如每秒上万连接）还是有性能问题。
  * CONS2: 无须进程间通信，但线程间互斥和共享又引入了复杂度【死锁】。
  * 多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出。
  * 在并发几百连接的场景下，采用 PPC。 
  
* prethread：提前创建线程来处理请求。

# 高可用架构

**存储高可用方案的本质：将数据复制到多个存储设备，通过数据冗余来实现高可用。**

复杂性来源：如何应对复制延迟和中断导致的数据不一致问题。

因此，对任何一个高可用存储方案，需要从以下方面分析：

- 数据如何复制？
- 各个节点职责？
- 如何应对复制延迟？
- 如何应对复制中断？

常见的高可用存储架构：主备、主从、主主、集群、分区。

## 主备复制

最常见、最简单的一种存储高可用方案，几乎所有的存储系统都提供了主备复制的功能。

* 基本实现：主备架构中 “备机”主要起备份作用，并不承担实际业务。【主备切换需要手动修改配置】
* 优点：【简单】
  - 对客户端来说：不需感知备机存在，即使灾难恢复，备机被修改为主机后，客户端只是认为主机地址更换，无须知道是备机升级为主机。
  - 对主机和备机来说：双方只需进行数据复制，无须状态判断和主备切换这类复杂的操作。

* 缺点：
  * 备机只为备份，不提供读写操作，硬件成本浪费。
  * 故障后需要人工干预，无法自动恢复。
* 使用场景：内部的后台管理系统【学生管理系统、员工管理系统、假期管理系统等。这类系统的数据变更频率低，在某些场景下丢失数据，也可以通过人工的方式补全。】

## 主从复制

* 基本实现：主机负责读写操作，从机只负责读操作，不负责写操作。【从主机写入的数据会复制到从机】
* 优点【与主备复制相比】：
  - 主机故障时，读操作相关的业务可以继续运行。
  - 从机提供读操作，发挥了硬件的性能。
* 缺点：
  * 客户端需要感知主从关系，将**不同操作发给不同机器处理**，复杂度比主备复制要高。
  * 若主从复制延迟比较大，业务会因为数据不一致出现问题。
  * 故障时需要人工干预。

* 使用场景：**写少读多的业务**【论坛、BBS、新闻网站等】

## 双机切换

**设计关键**：解决主备&主从的共性问题【1) 主机故障后无法写入 2）主机无法恢复时需要人工介入】

- 主备间状态判断：
  * **状态传递渠道**：相互连接，还是第三方仲裁？
  * **状态检测的内容**：例如机器是否掉电、进程是否存在、响应是否缓慢等。

- 切换决策
  * **切换时机**：什么情况下备机应该升级为主机？【机器掉电后备机升级？主机上进程不存在就升级？主机响应时间超过 2 秒就升级？3分钟内主机连续重启 3 次就升级等。】
  * **切换策略**：原来的主机故障恢复后，要再次切换，确保原来的主机继续做主机，还是原来的主机故障恢复后自动成为新的备机？
  * **自动程度**：全自动还是半自动？是否需要人工做最终的确认操作。

- 数据冲突解决

当原有故障的主机恢复后，新旧主机之间可能存在数据冲突。例如，用户在旧主机上新增了一条 ID 为 100 的数据，这个数据还没有复制到旧的备机，此时发生了切换，旧的备机升级为新的主机，用户又在新的主机上新增了一条 ID 为 100 的数据，当旧的故障主机恢复后，这两条 ID 都为 100 的数据，应该怎么处理？

以上设计点并没有放之四海而皆准的答案，不同的业务要求不一样，所以切换方案比复制方案不只是多了一个切换功能那么简单，而是复杂度上升了一个量级。形象点来说，如果复制方案的代码是 1000 行，那么切换方案的代码可能就是 10000 行，多出来的那 9000 行就是用于实现上面我所讲的 3 个设计点的。

# Ref

## 常见系统性能量级

常见系统的性能量级表可作为压测等情况的一个参考值

| 系统          | 性能量级（per second） |
| ------------- | ---------------------- |
| nginx负载均衡 | 3w左右                 |
| mc读取性能    | 5w左右                 |
| kafka         | 号称百万级             |
| zk写入读取    | 2w+                    |
| http请求访问  | 2w左右                 |



****
















































------

