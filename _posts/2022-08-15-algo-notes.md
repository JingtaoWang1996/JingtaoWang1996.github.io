---

title: 'Algo-notes'
date: 2022-08-15
permalink: /posts/2022/08/Algo-notes/
tags:
  - cool posts
  - category1
  - category2
---

Algorithm learning notes

# 排序

## 插入排序

* 假设一个list的值之前的值已经排好，新的值从最后开始搜索比对位置，找到对应位置后进行插入【即：从第二个开始与第一个比对，第三个开始和2，1比对，…】lcd147

## 归并排序

* 稳定排序方式，能利用完全二叉树特性。（数据分解成小任务后再进行合并）

# 动态规划DP 

DP-  dynamic programming

优化问题

* 只要**问题可以划分为规模更小的问题**，并且原问题的最优解中包含了子问题的最优解，则可以考虑用动态规划解决。（eg：后一步的总数=前面两步可能的总数之和）
* 实质：**将问题示例分解为更小的相似子问题，并存储子问题的解，是的子问题只求解1次，最终获得原问题的答案，以解决最优化问题的算法策略**
* 类似于贪心问题，都是将问题归纳为更小的、相似子问题，并通过求解子问题产生一个全局最优解。另：贪心通过不断选择当前最优解来逐步近似全局最优；动态规划通过求解局部子问题的最优解来达到全局最优。

# 集成学习

* **同质集成**：集成算法中的学习器相同，比如集成模型当中都是决策树。
* **异质集成**：集成算法当孩子工的个体学习器都是有不同类型的学习期组成。
* **期望目标**：弱学习器有较好的准确性且各个学习器之间有较大差异（实际情况满足较难）
* 常见两类算法：
  * 个体学习器之间强依赖，必须串行化生成每个学习器：boosting【adaboost，GBDT】
  * 个体学习器之间不存在强依赖，可以并行生成每个学习器：bagging【random forest】

## boosting

顾名思义，从弱学习器出发，反复训练学习得到一系列弱分类器，然后组合弱分类器，构成一个强分类器。

* **工作机制**：从初始训练集开始，得到一个基学习器，根据基学习器的表现对训练样本分布进行调整，使得序列模型前一个模型中被分错的样本，在这一轮模型训练过程中能够得到更多的关注【通常通过增加样本权重来控制】，进而得到下一个分类器。反复执行直到达到设定的基学习器数量【该数量是需要调试的超参数，通过cross validation 来进行选择】
* 多适用于而分类问题。

## bagging

* 通过在原始数据集上进行采样分类，获得多种不同的样本集，用这些不同数据集来训练基学习器。
* 自助采样方法：bootstrap【有放回采样，可能出现重复采样 & 未被采样的部分】（**给定包含m的样本，按照有放回采样后，最终获得的样本集大致包含2/3的数据样本。**）
* 可根据参数调配适用于多分类问题。
* Random Forest ：主要是用来提高基学习器之间的多样性：传统决策树是在整个属性集上选择最优的属性来划分样本集合。RF 是先在属性上组成一个子属性集，在其中选择最优属性来进行划分。

# 随机森林-分类&集成算法



* 通过集成学习的思想将多棵决策树集成的一种算法。基本单元是决策树，通过多棵树集成成为森林，每棵树都是一个分类器，N棵树就会有N个分类结果，随机森林将 **所有结果中投票次数最多的作为最终输出** 。

* 生成规则：

  * 将样本输入每棵树的过程中，99%不相关的树会在2分类的过程中，做出完全相反的决策之后相互抵消，少数优秀的分类器会做出正确的预测结果。
  * 训练集大小为n，每棵树随机且有放回的从中抽取m个作为该树的训练集，m<n 保证每棵树的训练样本均不同。【随机抽取过程可能包含重复样本】
  * 特征维度为M，指定一个常数m (m<<M),随机从M中选m个子集，每次分裂的时候，选取m个特征中最优的。
  * 每棵树在迭代过程中尽可能生长，不存在剪枝过程。

* 特点如下：

  * 处理高维样本且不需降维；
  * 生成过程做到无偏估计；
  * 能够评估特征在分类问题的重要性；
  * 缺省问题也能获得好结果：某几位度缺数据。
  * **多棵树集成可以避免仅仅依赖于单个树造成的误判。**
  * 比单棵树有更强的分类能力

* python 库

  ```python
  from sklearn.ensemble import RandomForestClassifier
  model = RandomForestClassifier(n_estimators=self.nTree, criterion=self.criterion, max_depth=self.maxDepth)
  model.fit(xData, xLabel)
  ```

* 重要参数说明

  * **n_estimators:** 最大弱分类器(决策树)个数，太小欠拟合，太大计算量不足
  * oob_score: 是否采用袋外样本评价好坏，默认False，袋外评价反映模型拟合后的泛化能力
  * **maxDepth**: 决策树最大深度：默认为None，决策树**最大深度不能太深**，否则最终结果会较差。常用值在**10-100**
  * **max_feature:**最大特征数，默认auto（最多考虑√N个特征），‘log2’：最多考虑‘√2’个特征
  * max_leaf_nodes: 最大叶节点数，防止过拟合，特征不多，可选None,特征多可加以限

* 分类效果相关因素

  *  任意两棵树：相关性越大，错误率越高（越相关投票结果越一致，因此尽量保证树之间的相关度越低越好）

  * 森林中两棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

    **PS:减少特征选择个数m**，树的相关性和分类性能降低，反之增大m，两者也增大，因此需要选择适当的特征值个数m

* 袋外错误率 oob error

  * 用于判断选取特征的数目。随机森立主要采用内部评估，在生成过程就可以建立误差的无偏估计。因生成过程中，对训练集使用了不同的bootstrape sample（随机有放回的抽取），所以对每棵树而言，**大约1/3 的训练实例没有参与第k棵树的生成，它们成为第k棵树的oob样本**。
  * 对于上述oob样本，计算方式如下：
    * 对于每个样本，计算它作为oob样本的树对它的分类情况
    * 简单多数投票作为分类结果，
    * 最后用错误分类个数占比作为oob误分率。

# Naïve Bayes

分类问题当中的基准模型，模型简单值得尝试，分类效果不一定好，可以用于基准方法。

* 基本背景：Naïve Bayes 是基于贝叶斯定理及特征条件独立假设的分类方法。
* 优点：所需估计参数少，对数据不敏感，算法理论简单。
* 缺点：需要假设模型的属性之间相互独立，实际中往往不成立。
* 三种贝叶斯模型及python实现

```
from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB
```

   **高斯（特征是连续变量，假设模型正态分布）、伯努利（离散特征）、多项（常用语文本分类，特征是单词，值是出现次数的情况）**

# Adaboost

* 迭代算法，用相同训练集训练不同的弱分类器，最终集合构成一个强分类器

* **同一个数据集但需要每次训练改变数据权重值分布**：它根据每次训练集之中每个样本是否分类正确，以及上次的分类正确率，来确定**每个样本的权值**。将修改过权值的新数据送给下层分类器进行训练，最后将每次得到的分类器融合作为最终的分类器。

* 算法概述：

  * 先通过对N个训练样本的学习得到第一个弱分类器；

  * 分错的样本和其他的新数据一起构成一个新的N个的训练样本，通过对这个样本的学习得到第二个弱分类器；

  * 将1和2都分错了的样本加上其他的新样本构成另一个新的N个的训练样本，通过对这个样本的学习得到第三个弱分类器

  * 最终经过提升的强分类器。即某个数据被分为哪一类要由各分类器权值决定。

    PS: 直观感受---当每个分类器的准确度以及分类器数量足够多的时候，能够提高总体准确度。

* 增加权重后能够将训练集中在分类较难的数据样本上。

* 加权投票机制能够替代平均投票机制，提高分类效果。

* python实现

  ```
  from sklearn.ensemble import AdaboostClassifier 
  ```

  若不指定base_estimator 默认使用决策树

# 时序模型

* RNN & LSTM

## 问题记录

* 时序模型一般情况不使用dropout，因为RNN存在累乘效应，使用dropout会破坏学习过程。需要使用的话只能在RNN or LSTM之外使用。

# Markov Model

* 一阶马尔科夫假设（First order markov assumptions）：**N 时刻的观测值仅依赖于N-1时刻的观测值**
  $$
  p(x_n|x_{n-1},x_{n-2},...,x_1,x_0) = p(x_n|x_{n-1})
  $$

* 转移概率：从状态Si到Sj的概率
  $$
  a_{i,j} =p(x_n=S_j|x_{n-1}=S_i)
  $$

* 时不变性：
  $$
  p(x_n=S_j|x_{n-1}=S_i)=p(x_{n+T}=S_j|x_{n-1+T}=S_i)
  $$

## 隐马尔科夫模型HMM

统计模型，用来描述一个含有隐含未知参数的马尔科夫过程，难点在于如何从可观察的参数中确定隐含参数。

* 问题模型：假设有一个人被锁房间里很多天，但想知道外面的天气是什么。他猜测天气的唯一途径是看每天来照顾他的人有没有带伞。
  * 观察状态（observed state）={带伞、不带伞}
  * 隐含状态（hidden state） = {晴、雨}
  * 由hidden state 得到的observed state 是可观察的，需要通过倒推来确认状态
  * 能够解决的问题：已知初始化模型a = {A,B,prob},观测序列： 1）计算某个序列出现的概率；2）已知观测序列，求最可能的hidden state

# 生成模型

遇到过的生成模型包括：GAN、AutoEncoder、VAE 等。

## GPT: generative pre-Training Model

[参考](https://paperswithcode.com/method/gpt)

* A Transformer-based architecture and training procedure for NLP.
* 1st stage procedure: using unlabeled data to learn the initial parameters of network model.
* 2nd stage producre: adapt these parameters to a corresponding supervised task.
* 无标签文本语料很丰富，但可以通过无标签文本训练出一个“通用语言模型”，再使用**特定任务**的有标签语料数据进行迁移学习，GPT模型在12个任务中的9个都实现了SOTA的效果，且个别任务效果提升明显。
* 预训练使用语言模型LanguageModel的训练方式，模型使用的是transformer变体，多层transformer-decoder，

# 优化算法 

## 变邻域搜索算法VNS

**总思路：在给定的邻域序列当中，不断更新搜索每一个邻域的最优解，直到序列当中的所有邻域都被搜索过一遍之后，剩下的就是最优**

* 利用不同的动作构成的邻域结构进行交替搜索.
* 一个邻域的局部最优解不一定是另一个邻域结构的局部最优解
* 全局最优解是所有可能邻域的局部最优解。
* **领域动作**：一个函数，对当前解进行一个操作，可以得到所有解的集合。

<img src='/images/img/VDN搜索示例.png'>

# 并查集

* 适用范围：找不相交的数据合并及查询问题   lcd 547/684/685

* 树形数据结构，用于处理相交的合并及查询问题，树的根节点唯一标识了一个集合，只要找到某个元素的树根，就能确定它在哪个集合里。[参考](https://zhuanlan.zhihu.com/p/93647900/)

* 主要用于解决**元素分组**问题，管理一系列**不相交的集合**，同时支持：

  * 合并Union：将两个不相交的集合合并为1个集合
  * 查询Find：查询两个元素是否在同一个集合中。

* 问题示例：若某个家族人员过于庞大，给出某个亲戚关系图后，确定任意两人是否具有亲戚关系。（x是y亲戚，y是z亲戚，则x是z亲戚

* 解决思路：**用集合的一个元素来代表集合**，通过一定规律将元素相互合并之后，构成类似于树和图的搜索结构，判断两个数之间的关系只需要判断他们是否属于同一个根节点即可。

* 路径压缩：在构成并查集的过程中，可能会出现长链结构，为了简化查询，**可以将长链的每个节点父节点都设为根节点，降低查询复杂度**

  PS: 合并过程---**尽量将简单树向复杂树上合并**

* 具体步骤：

  * 每个元素初始化为个1个集合，其根节点为自身，建一个parents = [i for i in range (n)]
  * 根据提供的元素之间的关系表格，将直接相连的两个元素进行合并（合并条件自己定义），更新上述list。
  * find 函数主要用于寻找当前位置的根元素

```python
# 并查集class
class solution:
    def find_redundant_connection(self, edges):
        # 根据给定关系建立表
        def finds(index):
            # 查找根节点
            if parents[index] != index:
                parents[index] = finds(parents[index])
            return parents[index]

        # 合并
        def union(node1, node2):
            parents[finds(node2)] = finds(node1)
        
        n = len(edges)
        parents = [i for i in range(n)]
        
        for data in edges:
            if finds(data[0]-1)!=finds(data[1]-1):
                union(data[0]-1,data[1]-1)
        else:
            return data
```

# 图类模型

## 二分图

* [模型定义](https://web.ntnu.edu.tw/~algo/BipartiteGraph.html)：设G=(V,E)是无向图，若顶点V可分割为两个互不相交的子集(A,B),且图中的每条边（i,j)所关联的两个顶点i和j分别属于这两个子集，则图G为二分图。【decision variable 和变量值各在一个子集，通过边相连】
* matching：二分图中的任意两条边，不连接在相同的顶点上的情况。
* maximum matching：若二分图中所有matching等于变量个数，那么该解有效（变量和值一一对应）
  * maximum matching的寻找过程：从任意边开始，对所有未match的边进行替换，直到无法再优化。

# 其他

## 回溯

* 类似枚举的搜索尝试过程，在搜索尝试过程中寻找问题的解，**当发现已不满足求解条件时，就回溯，尝试别的路径**。回溯法是一种选优搜索法，按选优条件向前搜索，已达到目标。
* 把问题的解空间转化成图或者树的结构表示，然后使用深度优先搜索进行遍历，遍历过程中记录和寻找所有可行解或者最优解。基本思想类同于：1) 图的深度优先搜索 2）二叉树的后续遍历
* **实现思路**：按照需求构建满足要求的递归序列，找准结束条件后，统一返回。

## 摩尔投票法

* 对拼消耗，剩下最后一个是结果。 lcd229

## 二叉树序列化

* 序列化 Serialization: 将数据结构或对象转换成可以取用的格式，以待后续在当前或者其他计算机环境中恢复原先状态的过程。【Java 数据当中也有类似的】
* 反序列化 De-serialization：序列化后数据恢复为原数据的过程。
* 二叉树序列化：3种遍历方式已知两种就能恢复原来的二叉树。 lcd331

## 点击率通过预测模型

* 点击率预测：**对某个广告将要在某个情形下展现前，系统预估其可能的点击率**，如果大概率被点击，就展示广告，否则就不展示。 同时，通过广告来完成价格评定。
* 网络广告点击到达率=该广告实际点击次数/广告展现量  （点击100次/出现1000次）

## 蓄水池抽样

**用来从N个样本种随机选择k个，【N极大无法全部放入内存 or N 未知】，时间复杂度O(N)**

蓄水池抽样原理

* 创建长度为k的蓄水池（array）来存放结果，初始化蓄水池种元素为N个样本的前k个。
* 每次迭代过程生成一个随机数 j∈[0,N]，若j∈[0,K]，则蓄水池中的第j个元素被更新。
* 重复上述步骤直到最后，通过一次遍历O(N) 就能保证所有N个元素，都以k/N的概率被抽样到。

## 拒绝采样

针对复杂问题的随机采样方法：（lcd470：用rand7 实现rand10）

* 方法思路：**若生成的随机数满足要求，那就返回该随机数，否则一直生成直到满足。**

* lcd470例子：rand7*rand7 生成10个等概率的不同数即可【+—\*/均可，此处考虑两个数相乘为例子】

|      | 1      | 2       | 3       | 4       | 5       | 6       | 7       |
| ---- | ------ | ------- | ------- | ------- | ------- | ------- | ------- |
| 1    | 1-1/49 | 2-2/49  | 3-2/49  | 4-3/49  | 5-2/49  | 6-4/49  | 7-2/49  |
| 2    | 2-2/49 | 4-3/49  | 6-1/49  | 8-2/49  | 10-2/49 | 12-1/49 | 14-2/49 |
| 3    | 3-2/49 | 6-1/49  | 9-1/49  | 12-1/49 | 15-2/49 | 18-1/49 | 21-2/49 |
| 4    | 4-3/49 | 8-2/49  | 12-1/49 | 16-1/49 | 20-2/49 | 24-2/49 | 28-2/49 |
| 5    | 5-2/49 | 10-2/49 | 15-2/49 | 20-2/49 | 25-1/49 | 30-2/49 | 35-2/49 |
| 6    | 6-1/49 | 12-1/49 | 18-1/49 | 24-2/49 | 30-2/49 | 36-1/49 | 42-2/49 |
| 7    | 7-2/49 | 14-2/49 | 21-2/49 | 28-2/49 | 35-2/49 | 42-2/49 | 49-1/49 |

**题目要求rand10，则从中选10个概率相等的数即可，也就是可以模拟rand更多**

## 拜占庭将军问题

* 点对点基本通信：存在消息丢失的不可靠信道上试图通过消息传递的方式达到两点的一致性是不可能的。
* 问题模型：拜占庭位于如今的土耳其的伊斯坦布尔，是东罗马帝国的首都。由于当时拜占庭罗马帝国国土辽阔，为了达到防御目的，每个军队都分隔很远，**将军与将军之间只能靠信差传消息**。在战争的时候，**拜占庭军队内所有将军和副官必须达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营**。但是，在**军队内有可能存有叛徒和敌军的间谍，左右将军们的决定又扰乱整体军队的秩序。在进行共识时，结果并不代表大多数人的意见。**这时候，在**已知有成员谋反的情况**下（信道不可靠），其余忠诚的将军在不受叛徒的影响下如何达成一致的协议。
* 问题延伸：在互联网大背景下，当需要与不熟悉的对方进行价值交换活动时，如何才能防止不会被其中的恶意破坏者欺骗、迷惑从而作出错误的决策；在缺少可信任的中央节点和可信任的通道的情况下，分布在网络中的各个节点应如何达成共识 。
* 解决方法：增加信息确认轮数 （若将军数n，叛徒m，n>=3m+1，且进行m+1轮协商）or 减少干扰噪声比例 or 消息增加签名。

## 注意力机制

attention [mechanism/model](https://blog.csdn.net/qq_40027052/article/details/78421155)，注意力机制模型命名借鉴了人类注意力机制。

* 人类注意力机制：视觉通过快速扫描得到全局图像，获**得需要重点关注的目标区域（注意力焦点）。抑制其他无用信息，从而获取更多关注点的细节。**

* 深度学习的注意力机制从本质上和人类的选择性视觉注意机制类似，核心目标也是**从众多信息当中选择对当前任务更加重要的信息。**

* Encoder-decoder框架 ：注意力机制常见框架之一。常见应用示例：翻译通过编码和解码进行。

* Attention 模型(机器翻译常见的soft attention)：

  * 正常encoder-decoder模型无法体现“注意力”，因为每个单词在句子中成分一致，但实际使用中，会给不同的词一个权重（tom 0.3，chase 0.2,jerry 0.5) ，一定程度上增强翻译效果。

  * 注意力机制将当前固定1中间值修改为权重变化占比。示例如下

    <img src='/images/img/注意力机制翻译权重示例.png'>

## 特征工程

* 模型外部的注意力机制：将原始数据、信号转化为数值向量，在数据科学中，帮助模型有效选取适当的特征，进而完成任务。
* 良好的特征工程需要特征工程师对任务内容有一定了解，才能更好的进行特征选取。




------

