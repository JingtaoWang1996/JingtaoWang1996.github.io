---
title: 'Algo-notes'
date: 2022-08-15
permalink: /posts/2022/08/Algo-notes/
tags:
  - cool posts
  - category1
  - category2
---

Algorithm learning notes

# 排序

## 插入排序

* 假设一个list的值之前的值已经排好，新的值从最后开始搜索比对位置，找到对应位置后进行插入【即：从第二个开始与第一个比对，第三个开始和2，1比对，…】lcd147

## 归并排序

* 稳定排序方式，能利用完全二叉树特性。（数据分解成小任务后再进行合并）

# 动态规划DP 

DP-  dynamic programming

优化问题

* 只要**问题可以划分为规模更小的问题**，并且原问题的最优解中包含了子问题的最优解，则可以考虑用动态规划解决。（eg：后一步的总数=前面两步可能的总数之和）
* 实质：**将问题示例分解为更小的相似子问题，并存储子问题的解，是的子问题只求解1次，最终获得原问题的答案，以解决最优化问题的算法策略**
* 类似于贪心问题，都是将问题归纳为更小的、相似子问题，并通过求解子问题产生一个全局最优解。另：贪心通过不断选择当前最优解来逐步近似全局最优；动态规划通过求解局部子问题的最优解来达到全局最优。

# 集成学习

* **同质集成**：集成算法中的学习器相同，比如集成模型当中都是决策树。
* **异质集成**：集成算法当孩子工的个体学习器都是有不同类型的学习期组成。
* **期望目标**：弱学习器有较好的准确性且各个学习器之间有较大差异（实际情况满足较难）
* 常见两类算法：
  * 个体学习器之间强依赖，必须串行化生成每个学习器：boosting【adaboost，GBDT】
  * 个体学习器之间不存在强依赖，可以并行生成每个学习器：bagging【random forest】

## boosting

顾名思义，从弱学习器出发，反复训练学习得到一系列弱分类器，然后组合弱分类器，构成一个强分类器。

* **工作机制**：从初始训练集开始，得到一个基学习器，根据基学习器的表现对训练样本分布进行调整，使得序列模型前一个模型中被分错的样本，在这一轮模型训练过程中能够得到更多的关注【通常通过增加样本权重来控制】，进而得到下一个分类器。反复执行直到达到设定的基学习器数量【该数量是需要调试的超参数，通过cross validation 来进行选择】

## bagging

* 通过在原始数据集上进行采样分类，获得多种不同的样本集，用这些不同数据集来训练基学习器。
* 自助采样方法：bootstrap【有放回采样，可能出现重复采样 & 未被采样的部分】


------

