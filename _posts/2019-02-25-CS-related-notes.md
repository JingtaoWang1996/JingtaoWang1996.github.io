---
title: 'CS-related-notes'
date: 2019-02-25
permalink: /posts/2019/02/CS-related-notes/
tags:
  - cool posts
  - category1
  - category2
---

Computer Science related notes included.

# 分布式

* **将处理错误的代码当成正常功能代码来处理**
* 对比传统单体架构

|              | 传统单体           | 分布式                   |
| ------------ | ------------------ | ------------------------ |
| 部署         | 不经常&容易部署    | 经常发布&部署复杂        |
| 故障影响范围 | 大                 | 小，分布式隔离性更好     |
| 架构设计     | 难度小             | 难度随服务器数量指数增加 |
| 系统性能     | 响应快，吞吐量小   | 响应慢，吞吐量大         |
| 运维         | 简单               | 复杂                     |
| 扩展性       | 差                 | 好                       |
| 上手难度     | 应用逻辑学习曲线大 | 架构逻辑学习曲线大       |
| 系统管理     | 开发成本           | 服务治理和调度           |

## 架构思想-amazon

* 所有团队的程序模块都要通过 Service Interface 方式将其数据与功能开放出来。
* 团队间程序模块的信息通信，都要通过这些接口，不允许其他通信方式（eg：A直接访问B的数据库等），**只能调用这些Service interface**。
* 任何技术都可使用。
* 所有的service interface 做好设计成为能对外开放的接口。

经典资料

* 8条分布式被推翻的基本假设：网络稳定、传输延迟为0、带宽无穷大、网络是安全的、网络拓扑不变、只有一个系统管理员、数据传输成本为0、整个网络同构。

<details>
    <summary>经典图书</summary>
        An introduction to distributed systems【分布式系统提纲】
        Distributed Systems for fun and profit【免费电子书】
        Distributed Systems: Principles and Paradigms
        Scalable Web Architecture and Distributed Systems
        Principles of Distributed Systems【苏黎世联邦理工教材：分布式用到的算法】
        Making reliable distributed systems in the presence of software errors
        Designing Data Intensive Applications
</details>

## 需要注意的问题

* 异构系统标准性：软件、应用、通信协议、数据格式、开发和运维过程是否标准。
* 好的配置管理应该分为3层：底层&操作系统、中间层和中间件、最上层&业务应用。
* 服务依赖性问题：**整个服务依赖链中的稳定性由最差的决定**
  * 非关键服务被关键服务依赖，则非关键业务可被定义为关键业务。
  * 一个服务链上的某个服务挂掉，整个链条可能会一起挂掉。
* 服务治理：定义出服务关键程度，关键业务或服务调用的主要路径。同时确保：应用层上的业务隔离以及数据库上的隔离。
* 多层架构的运维复杂度更大：系统通常分为4层（基础层-硬件设备、平台层-服务中间件层、应用层-业务软件、接入层-接入用户请求的网关等）
  * 任何一层出问题都会导致服务整体出问题
  * **<u>不要相信上游系统不出问题，不能因为对方系统问题，把我们系统影响到</u>**。

## 技术栈

* 提高性能

  | 加缓存   | 负载均衡 | 异步调用     | 数据镜像   | 数据分区   |
  | -------- | -------- | ------------ | ---------- | ---------- |
  | 缓存系统 | 网关系统 | 异步系统     | 数据镜像   | 数据分区   |
  | 缓存分区 | 负载均衡 | 消息队列-kfk | 数据同步   | 分区策略   |
  | 缓存更新 | 服务路由 | 消息持久化   | 读写分离   | 数据访问层 |
  | 缓存命中 | 服务发现 | 异步事务     | 数据一致性 | 数据一致性 |

  * 缓存系统：提高系统访问能力
  * 负载均衡：系统水平拓展，多台机器共同分摊流量请求
  * 异步调用：通过消息队列来对任务做排队处理，减少前端请求峰值，增加系统吞吐量（以实时性作为代价，且需要对消息做持久化。）

* 提高稳定性

| 服务拆分           | 服务冗余             | 限流降级 | 高可用架构 | 高可用运维 |
| ------------------ | -------------------- | -------- | ---------- | ---------- |
| 服务治理，服务调用 | 服务调度、弹性伸缩   | 降级控制 | 多租户系统 | 自动化运维 |
| 隔离故障，模块复用 | 故障迁移，防单点故障 | 服务熔断 | 灾备多活   | 全栈监控   |

## 容错设计

弹力设计（system resilience measurement）：容错能力、可伸缩性、一致性、应对大流量的能力。

* 系统可用性测量（系统在不健康，甚至出错的情况下能够维持运行的能力）：

  ​                                  **Availability = MTTF/(MTTF+MTTR)** 

  * MTTF: mean time to failure，平均故障时长（多长时间出现一次故障）
  * MTTR:mean time to recover，平均恢复时间（故障出现到恢复的时间）

  | 系统可用性      | 宕机时间/年 | 宕机时间/月 | 宕机时间/周 | 宕机时间/天 |
  | --------------- | ----------- | ----------- | ----------- | ----------- |
  | 90%（1个9）     | 36.5d       | 72h         | 16.8h       | 2.4h        |
  | 99%（2个9）     | 3.65d       | 7.2h        | 1.68h       | 14.4m       |
  | 99.9%（3个9）   | 8.76h       | 43.8m       | 10.1m       | 1.44m       |
  | 99.99%（4个9）  | 52.56m      | 4.38m       | 1.01m       | 8.66s       |
  | 99.999%（5个9） | 5.26m       | 25.9s       | 6.05s       | 0.87s       |

* 有计划不可用：日常任务、运维、升级。

* 无计划宕机：系统故障、数据中间故障、自然灾害。

* 故障是正常&常见且不可预测的，容错设计之初就是要考虑 "隔离"来防止故障蔓延。

  * 以服务种类来隔离 & 以用户来隔离（多租户模式： 对于大客户，单独设置专门的服务实例，小客户共享实例，在服务隔离的情况下，确保资源的节约）。

  * 每个服务都有一个自己的数据库（物理隔离），同时准备好对外暴露提供服务。

  * 业务到达每一步可以暂停保存，以确保恢复之后可以从当前位置继续。

* 隔离设计的重点：隔离业务的大小和颗粒度的定义; 系统的复杂度和成本；与设计模式配套使用；运维复杂度；监控系统。

## 关键技术-全栈监控&服务调度

### 全栈监控

* 需要完成的功能：监控、关联分析、跨系统
* 具体监控：主机和底层资源（cpu、内存等）、中间件状态、应用层（访问量、吞吐量、响应时间等）；同时添加好标准化的日志及日志分析
* 好的监控：故障快速定位、关联聚合指标后展示

### 服务调度

* 定义服务的关键程度（基于对业务的理解）

* 微服务是服务依赖最优解，至少：**服务不能有依赖环**，若有：考虑通过消息中间件等解环。

* 服务状态&生命周期：通过服务中心中间件来管理。

* 整个架构版本管理：VersionSet in Amazon(一堆服务版本集所形成的整个架构的版本控制)

  ​    除了各个项目的版本管理之外，还需要再盖一层版本管理（Build过Linux分发包的话，Linux分发包中各个软件的版本上会再盖一层版本控制，毕竟分发包也有版本依赖，同时解决版本兼容性的问题。）

## 流量调度

* 自动调度，无需人工干预，在弹性计算扩容较长事件窗口内或底层资源消耗殆尽的情况下，保持系统平稳运行。
* API gateway：高性能语言、集群技术、业务逻辑简单，服务化(Admin API 来不停机管理变更配置)
* 状态数据的调度：通过第三方服务进行存储。
* 分布式事务一致性问题：各个节点上的数据备份问题，数据冗余解决数据不丢失的问题。（备份、master-slave、master-master、多阶段提交、Paxos）
  * 凡通过业务补偿或在业务应用层上做的分布式都是两阶段提交

## 数据调度

* 分布一致性真正算法：**Paxos**(Lesile Lamport 1990 提出的基于消息传递且具有高度容错性的一致性算法)
* 逻辑钟和向量钟（检测响应的数据冲突）：分布系统钟为了解决消息有序问题而布置的时钟，**由于不同的机器有不同的本地时间，同步这些时间相对困难导致消息乱序。因此，每个系统维护一个本地计数器，每执行一个事件，计数器**+1，跨系统传递时，接收端同步更新自己的计数器。

## 弹性设计

### 幂等性-去除重复请求

* 幂等设计：一次或多次请求某个资源应具有相同的副作用。
  * 系统解耦隔离后，服务间的调用可能会有三个状态：success, failed,timeout.前两者是明确的状态，第三种不知道是什么状态。【超时可能产生副作用：订单超时是多创建一笔？还是多扣一次钱？】
  * 处理方式：下游系统提供相应查询接口。上游timeout后，下游差一次，查到就不做了，没查到再重复；**幂等性：**查询交给下游系统，上游只管重试，下游保证次和多次重试一致。
* 幂等设计具体实现
  * **全局id**：同一笔交易使用相同的id来保证幂等性。（ID的分配：使用id冲突极小的算法--UUID、snowflake【Twitter提出，long型64位id】）
  * **过滤收到的id是否重复**：幂等性要求下游对收到的id进行存储，用于后续查询。
* http幂等性
  * HTTP get 方法获取资源，无副作用，是幂等的。
  * http head 和 get 本质是一样的，区别在于head 只有头部信息，也是幂等的。[可以用来判断某个资源是否存在—用head 做探活]
  * http options：获取当前URL所支持的方法，幂等。
  * http delete：删除资源，同样幂等。
  * http put：用于创建或更新操作，幂等性。

### ACID

* A-atomicity 原子性：事务里的所有操作，要么完成，要么不做，只要有一个操作失败，整个事务就失败，此时回滚。
* C-consistency 一致性：数据库处于一致状态，事务的运行不会改变数据库原本的一致性约束。
* I-Isolation 独立性：并发的事情不会相互影响，如果一个事务访问的数据正在被另一个事务修改，只要另一个事务未提交，它访问的数据就没有影响。
* D-Durability 持久性：一旦提交后的事务，它所做的修改将会永久保存在数据库上，即使宕机也不会丢失。

### CAP 

对于一个分布式计算系统来说，不可能同时满足以下3点，最多2个

* 一致性Consistency：所有节点同一时间具有相同的数据【要么获得最近写入的数据，要么获得一个错误。】
* 可用性Availability：保证每个请求不管成功或失败都有响应。【每次请求都能获得一个非错误的响应，但不保证返回是最新写入的数据。】
* 分隔容忍Partition tolerance：系统任意信息的丢失或失败不会影响系统运行操作【尽管任意数量的消息被节点间的网络丢失或延迟，系统仍然继续运行。】

### BASE






------

