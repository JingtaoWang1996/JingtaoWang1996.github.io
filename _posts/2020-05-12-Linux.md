---
title: 'linux'
date: 2020-06-15
permalink: /posts/2020/06/linux-notes/
tags:
  - Technical Refer notes
---

Some linux usage experience.

# 常用客户端工具

## mobaXterm

破解版可无限保存ssh记录

### 问题记录

* linux 服务器连接断开

  * 修改 /etc/ssh ssh_config 与 sshd_config 区别

    ssh_config 是针对客户端的配置文件  &  Sshd_config 是针对服务器的配置文件

  *  sshd_config 可以配置是否需要密码登录：PasswordAuthentication yes

* ssh 连接报错：Network error software caused connection abort

  * vim /etc/ssh/sshd_config 
  * 找到 TCP KeepAlive yes 把前面的 # 去掉
  * 找到ClientAliveInterval 参数去掉前面的#【该参数指定了服务器端向客户端请求消息的时间间隔，默认是0，不发送。ClientAliveInterval 60 表示每分钟发送一次，然后客户端相应，这样就保持长连接，比较奇怪的地方是，这里需要服务器主动发出请求，而不是客户端。正常情况下，ClientAliveCountMax】

# 文件操作命令

* 统计某文件中某内容出现次数：grep -o 内容 文件名 \| wc -l
* 统计某文件行数：wc -l 文件名
* 查看某文件中某内容对应的行：grep -n "内容" 文件名
* 复制文件：cp 当前文件路径 新复制文件路径 
* 将某个文件前m行移到另一个文件中：head -m a.txt > b.txt
* 删除文件/文件夹
  * 删除文件：rm -f 文件路径
  * 删除文件夹：rm -rf 文件夹路径 
* 移动/重命名文件
  * 重命名文件：mv 当前文件名称 重命名名称
  * 移动文件到目标路径，询问是否覆盖重复文件：mv -i 当前文件路径 目标文件路径
  * 移动文件到目标路径，直接覆盖重复文件：mv -f 当前文件路径 目标文件路径
  * 移动文件到目标路径，备份目标路径重复文件：mv -b 当前文件路径 目标文件路径
* 文件解压
  * tar.gz 文件：tar -zxvf Python-3.8.5.tgz
  * .zip 文件： unzip 文件名
  * unrar e xxx.rar
* 查看文件第n行内容：
  * 第1行：head -n 1 文件名
  * 第m行： head -m 文件名 
* 文件编辑：vim
  * 进入并编辑：cd到指定目录    vim 文件名     按insert 或 i 之后开始编辑
  * 退出：先按ESC之后      直接退出（：q）强制退出（：q!）保存并退出（：wq）
  * **切换到编辑模式在复制粘贴文件**：按照1）vim 进入之后   按i（进入编辑模式）ctrl+c/v
* 文件or 日志搜索（关键词匹配）：tail -f 
  * 实时刷新日志：tail -f 文件名 \| grep 关键词
  * 实时刷新日志的最新100行：tail -100f 文件名 \| grep 关键词
* 传文件/文件夹: scp
  * 远程服务器上的文件复制到本机: scp root@ip:/path/ /localpath
  * 远程服务器上的文件夹复制到本机：scp -r root@ip:/path/ /localpath
  * 本地文件复制到远程服务器：scp /localpath root@ip:/path
  *  本地文件夹复制到远程服务器：scp –r /local/path root@ip:/path
  *  直接从本地电脑上传文件到远端服务器：scp -p 9854 /cygdrive/待上传文件本地目录 root@localhost：/上传的目标路径
* vim 之后删除某一行：在esc状态下，直接dd，表示删除当前行。
* stat 文件路径：查看文件最近访问和改动时间【eg：stat data.db】
* 查看文件大小：
  * 进入目录后：ls -lh【以MB显示指定文件大小】
  * 进入目录后： ls -ll【以字节显示指定文件大小】
* 按时间倒序输出当前目录下文件：ls -lrt

## ssh

### sshpass

解决ssh登录无法在命令行指定密码也不能通过shell指定密码的问题

#### 安装配置

* yum install sshpass -y     |   apt install sshpass

#### 使用

* 格式：**sshpass “明文密码” 正常命令**

  eg: sshpass "Meiyou.mima" rsync -avz root@47.241.107.35:/opt/wjt/maliciousdomaincollection/malicious_data.db /opt/wjt/malData

## rsync （remote synchronize）

​        linux 下的数据镜像备份工具，通过LAN/WAN 快速同步多台主机间的文件和目录，并适当利用rsync算法（差分编码）减少数据的传输。

### 原理和特点

**远程数据传输步骤**

* 先验证用户身份
* 检测需要传输的文件【默认quick check算法：源路径与目标路径下对应文件的大小或修改时间是否有变动），检测出需要传输的文件后，只传文件改动的部分（增量）。传到远端后，远端依据之前的内容与传过来的新内容按照算法拼成一个新文件，所以说rsync并不耗费IO，但是耗cpu】
* 文件传输

**特点**

* IO 效率高，但耗费CPU资源。【与其他文件传输工具不同的是，会检查发送方和接收方已有文件，仅传输变动部分；传到接受端后再根据算法将文件拼成新文件。】
* rysnc 远程同步时跨平台的。

### 安装配置

* 传输双方都必须安装rsync：yum install rsync -y   |   apt install rsync

### 远程模式

远程模式认证远程主机账号和密码的方式：ssh认证 & rsync-daemon

| ssh                       | rsync-daemon                                                 |
| ------------------------- | ------------------------------------------------------------ |
| 本地与目标主机都安装rsync | 本地与目标主机都安装rsync                                    |
| 远程主机打开sshd服务      | 远程主机打开rsync守护进程： systemctl start rsyncd（/usr/bin/rsync --daemon） |
| 远程主机root账号          | 虚拟账号+模块名                                              |

**ssh**

不需要远程主机安装rsync，仅需要本地安装rsync并执行如下命令即可【**需要交互输入密码**】

* **拉取：rsync -avz root@ip:/src/path /local dst path**
* **推送：rysnc -avz /local src path root@ip:/dst/path**
* 命令完成后可以执行：stat 传输文件绝对路径 【查看文件修改信息】

* 若不支持免密，则需要先安装sshpass，再按照sshpass部分格式执行上述命令。

## 文件分发工具

* 阿里dragonfly：代替wget 在大规模集群节点进行更新时，通过应用层多播，提高传输效率。
  * superNode 一旦宕机，仍然会造成系统不可用。
* Gossip协议（epidemic 传染病协议）：分布式网络中，没有任何中心化节点，但只要第一个节点被感染成红色后，每个节点只需感染相邻，有限的几个节点，最终就能感染所有节点【最终一致性---PS:此处感染即为数据传输】
  * Cassandra数据库、Fabric 区块链、Consul 系统均采用上述方式。

# 进程相关

​      当iowait升高，进程可能因为得不到硬件响应而长时间处于不可中断状态。从PS或top命令的输出中可以发现，他们都处于D状态（不可中断状态）。

**进程包含的状态**

* R：Running/Runable, 进程在CPU就绪队列中，正在运行或正在等待运行。
* D：Disk Sleep，不可中断的睡眠状态，一般表示进程正在跟硬件交互且交互过程不允许被其他进程打断或中断打断。
* Z：Zombie，僵尸进程，进程实际已结束但父进程还未回收资源（描述符、pid等）
* S：Interrupt Sleep，可中断睡眠状态，进程因为等待某个事件而被系统挂起，事件发生时会被唤醒进入R状态。
* I：Idle，空闲状态，用于不可中断睡眠的内核线程上。
* T/t: Stopped 或 Traced，进程处于暂停或跟踪状态。向进程发送SIGCONT信号进程会恢复运行。【使用调试器调试代码时，使用断点进程也会变成跟踪状态。】
* X：Dead，进程已死亡，不会在top或ps中看到它。
* Ss+：第一个s表示可中断睡眠状态，第二个 s 和 + 表示这个进程是一个会话的领导进程，而 + 表示前台进程组。
  * 进程组：一组相互关联的进程，比如每个子进程都是父进程所在组的成员。
  * 会话：共享同一个控制终端的一个或多个进程组。

**进程操作命令**

* PS查看进程状态：ps -ef\|grep 进程字段or文件名（eg: ps -ef\|grep python3)

  执行后的参数说明例子：

  | 用户名 | pid    | child_pid | CPU占比 | 进程开始时间 | \    | 使用CPU时间 | 进程名  | 文件名 |
  | ------ | ------ | --------- | ------- | ------------ | ---- | ----------- | ------- | ------ |
  | root   | 进程号 | 进程号    | cpu占比 | 具体时间     | \    | 具体时间    | python3 | a.py   |

* 杀死、批量杀死进程

  * 杀死某个进程：kill -9 pid
  * 正常退出某个进程：kill -15 pid
  * 批量杀死某文件名对应的所有进程（cd 到文件所在路径后）：pkill -f 文件名 

* 后台不挂断运行代码：nohup

  * nohup 可以不挂断同时执行多个代码  & 执行后的打印全部存入nohup.out文件，一直积累不自动删除

  * 命令格式：nohup 解释器 文件名 & (eg:nohup python3 new_main.py & 或 nohup java -jar xxx.jar &）

  * 输出重定向：nohup 输出重定向到 /dev/null() 后不会产生 nohup.out 文件

    ​    eg: nohup python3 new_main.py >/dev/null 2>&1 & 

* 查看后台运行任务：jobs -l 

* 查找某个进程pid：pidof python3

* 最大进程数：sysctl kernel.pid_max

## 僵尸进程

**子进程正常创建逻辑**

* 当一个进程创建子进程后，它应该通过系统调用 wait() 或者 waitpid() 等待子进程结束，回收子进程的资源。

* 子进程在结束时，会向它的父进程发送 SIGCHLD 信号。父进程可以注册 SIGCHLD 信号的处理函数，异步回收资源。

* 若父进程没这么做or子进程执行太快，父进程还没来得及处理子进程状态，子进程就已经提前退出，那这时的子进程就会变成僵尸进程。

* 通常，僵尸进程持续时间都比较短，在父进程回收它的资源后就会消亡；或者在父进程退出后，由 init 进程回收后也会消亡。【若父进程没有处理子进程的终止，那么子进程就会一直处于僵尸状态。大量的僵尸进程会用尽 PID 进程号，导致新进程不能创建】

* 统计僵尸进程命令：ps -ef | grep defunct | grep -v grep | wc -l

# 系统资源查看命令

* 端口占用：netstat –anp\|grep 端口号

* 端口是否已对外开放: netstat -ntulp \| grep 端口号

* 指定端口是否已开放：firewall-cmd –query-port=666/tcp（yes 已开启）

* cpu信息：lscpu

* linux 系统版本号：cat/proc/version

* 服务器cpu数
  * 物理CPU：cat /proc/cpuinfo\|grep “physical id”\|sort –u\|wc –l
  * 每个物理CPU中的核数：cat /proc/cpuinfo\|grep “cpu cores”\|uniq
  * 逻辑cpu：cat /proc/cpuinfo\|grep “processor”\|wc -l
  * Cpu名称型号：cat /proc/cpuinfo\|grep “name”\|cut –f2 –d:\|uniq

* 内存占用

  * 最大命令：free -h   【free -h -s 2 每2s自动执行一次该命令】

  * 某个进程的内存占用：ps -aux\|grep python3(进程名)

  * 更详细的内存占比： cat /proc/进程号/status: VmRSS为进程所占用的内存

  * top命令，之后按M：所有进程按照内存占用大小排序

    top 命令，之后按P：所有进程按照CPU占用排序。
    
    * VIRT：进程虚拟内存大小，只要是申请过的内存，即使还未被分配，也会被算在内。
    * RES : 常驻内存大小，即实际使用的物理内存大小，不包括swap和共享内存。
    * SHR: 共享内存大小。
    * %MEM：进程使用物理内存占系统内存的百分比。

* 查看进程打开的文件列表：lsof

  * 基于进程pid查看进程打开的文件列表：lsof -p pid

* 磁盘占用情况：df 

  执行之后可以用于查看具体文件的占用

  * 返回根目录：cd /
  
  * 确认具体那个文件占用磁盘较大：du -h -x --max-depth=1
  
      PS: df -h 没有发现大文件之后的查询思路：**lsof -n grep deleted**，找到对应进程号后杀掉[参考](https://www.cnblogs.com/muchengnanfeng/p/9554993.html)
  
  * 查看索引节点占据磁盘空间的情况：df -i
  
    * 索引节点容量（Inode个数）：在磁盘格式化时设定好的，一般由格式化工具生成。
    * 当发现索引节点空间不足，但磁盘空间充足时，大概率是过多小文件导致的。【删除这些小文件，或把它们移到索引节点充足的其他磁盘中，就可以解决这个问题。】

* 进程运行在哪个逻辑CPU上：ps -eo pid，args，psr \|grep nginx
* linux 下的python console 交互
  * 查看当前python连接：ll /usr/bin/grep python
  * 进入命令行模式：python3
  * 退出命令行模式：ctrl+D

## 系统资源查看工具

### dstat

同时查看CPU 和 I/O的情况。

# 扫描命令

## masscan

* masscan 1.0.5不支持ipv6 ，需要升级到1.3.1

* 1.3.1安装配置命令：

  * sudo apt update 
  * sudo apt install git gcc make libpcap-dev

  * wget https://github.com/robertdavidgraham/masscan/archive/refs/tags/1.3.1.tar.gz
  * tar -xvf 1.3.1.tar.gz
  * cd masscan-1.3.1
  * make 
  * sudo make install 

# 网络相关

## 网络模型

**7层开放式系统互联通信参考模型**（Open System Interconnection Reference Model）：

* 应用层：为应用程序提供统一的接口。
* 表示层：把数据转换成兼容接收系统的格式。
* 会话层：维护计算机间的通信连接。
* 传输层：为数据加上传输表头，形成数据包。
* 网络层：数据的路由和转发。
* 数据链路层：MAC 寻址、错误侦测和改错。
* 物理层：在物理网络中传输数据帧。

4层TCP/IP网络模型：

- 应用层（应用层+会话层+表示层）：向用户提供应用程序。
- 传输层（传输层）：端到端的通信。
- 网络层（网络层）：网络包的封装、寻址和路由。
- 网络接口层（物理层+数据链路层）：网络包在物理网络中的传输。

基于TCP/IP 模型，在网络传输时，数据包会按照协议栈，对上一层发来的数据进行逐层处理；然后封装上该层的协议头，再发送给下一层。网络包在每一层的处理逻辑，都取决于各层采用的网络协议。【eg：应用层中，一个提供 REST API 的应用，可以使用 HTTP 协议，把它需要传输的 JSON 数据封装到 HTTP 协议中，然后向下传递给 TCP 层。】

封装：在原负载前后，增加固定格式的元数据，原负载数据并不会被修改。

* 传输层：TCP头+应用数据
* 网络层：IP头+TCP头+应用数据
* 网络接口层：帧头+IP头+TCP头+应用数据+帧尾

## Linux 网络栈

* 网络接口配置最大传输单元MTU：1500。超过1500的包，就会在网络层分片，以保证分片后的IP包不大于MTU。（MTU越大，需要的分包越小，网络吞吐能力越好）。
* 网卡：发送和接收网络包的基本设备。通过内核中的网卡驱动程序注册到系统后，在网络收发过程中，通过中断跟网卡进行交互。【网卡硬中断只处理最核心的网卡数据读取或发送；协议栈中大部分逻辑，都会放到软中断中处理。】

**网络包接收流程**

* 一个网络帧到达网卡后，网卡通过 DMA 方式，将这个网络包放到收包队列，然后通过**硬中断**，告诉中断处理程序已经收到了网络包。
* 网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区，再通过软中断，通知内核收到了新的网络帧。
* 内核协议栈从缓冲区中取出网络帧，通过网络协议栈，从下到上逐层处理这个网络帧：
  - 链路层：检查报文合法性，找出上层协议类型、去掉帧头帧尾交给网络层。
  - 网络层：取IP 头，判断网络包下一步走向(交给上层处理or转发)。若这个包要发送到本机后，取出上层协议类型，去掉 IP 头，交给传输层。
  - 传输层：取TCP orUDP 头后，根据<src_ip、src_port、dst_ip、dst_port > 四元组作为标识，找出对应的Socket并把数据拷贝到 Socket的接收缓存中。

* 应用程序通过socket接口读取新接收到的数据。

**网络包发送流程**

网络包的发送方向，正好跟接收方向相反。

* 应用程序调用 Socket API（比如 sendmsg）发送网络包。
  * 因为是系统调用，所以会陷入到内核态的套接字层中。套接字层会把数据包放到 Socket 发送缓冲区中。

* 网络协议栈从 Socket 发送缓冲区取出数据包，按照 TCP/IP 栈，从上到下逐层处理。

* 处理完成的网络包，送到网络接口层，进行物理地址寻址，以找到下一跳的 MAC 地址。添加帧头和帧尾后，放到发包队列中。完成后，软中断通知驱动程序：发包队列中有新的网络帧需要发送。
* 驱动程序通过 DMA ，从发包队列中读出网络帧，通过网卡发送出去。

## 基本命令

* 通断性：ping ip （python 多目标异步发送icmp包进行ping的库：multiping）

  * ping 基于ICMP包

* 网络跳数：traceroute ip/域名
  * 说明：通过向目标主机发送一条消息并通过会带来判断主机状态，遍历由源主机到目的主机的交互线路上所有的路由器并判断状态，一般以30为最大TTL，即以该线路上不超过29台路由器为前提进行遍历。
  * 基本原理：当路由器收到一份IP数据报，若该报文的ttl字段是1，则该报文的生存周期消耗殆尽，本路由处理后还未到达目标主机则需要将该数据丢弃，并给信源主机发送一份ICMP超时报文（包含中间路由器地址），这意味着：通过发送一份TTL字段为n的IP数据报给目的主机，就得到了该路径中的第n个路由器的IP地址，那么我们使IP数据报的TTL字段值从1开始递增，就可以获得所有中间路由的IP地址。由于已经到达目的主机，因此不会再发送ICMP应答报文，通过区分收到的ICMP报文是超时报文（type = 11）还是应答报文（type = 0）以判断程序应该何时结束。[每一次发包ttl从1开始增加，在一次traceroute过程当中，从源主机出发，每次经过1个路由则ttl-1，为0时返回本次所有路由状态，然后进入下一次发包。]
  * **最大30跳**，每个记录1跳，每一跳表示一个网关，3个时间表示发送的三个包网关相应后的返回时间。 

* mtr命令：（ping+traceroute)

  * 命令格式：mtr ip

  * <img src='/images/img/mtr示例.png'>

    查看traceroute 路径，丢包率，平均解析时延等，通过这条命令结合实际情况观察路径上的丢包情况等。

* 网络是否通畅\|：\| ip

  * 通过url执行上传或下载

  * \| --h  查看help 文件命令执行方式。

  * \| --k  跳过ssl 认证环节。

  * \| –data-urlencode()  url 编码的数据

  * \| url：port/path  查看当前路径内容。

    使用示例

  * \| –k https://ip:443

* bgpdump：linux解析bgp报文命令，[安装步骤](https://blog.csdn.net/weixin_35708669/article/details/89442180)

* iptable

  【iptables丢包：和内核的连接跟踪机制也可能导致丢包。丢包最大的可能，被filter规则丢弃-DROP or Reject的包是否会被执行到】

  * 查看filter 表的统计数据：iptables -t filter -nvL

    说明：其中statistic模块显示随机丢包的概率

  ifconfig 确定网卡名称后

  * 53的tcp 和 udp **重定向**到5053

    iptables -t nat -A PREROUTING -i enp125s0f0 -p tcp --dport 53 -j REDIRECT --to-ports 5053

    iptables -t nat -A PREROUTING -i enp125s0f0 -p udp --dport 53 -j REDIRECT --to-ports 5053

  * 查看iptable 规则：iptables -t nat -L -v -n
  
  * 删除某条规则
  
    规则行号：iptables -t nat -L -n --line-numbers
  
    根据num行号确定删除规则： iptables –t nat –D INPUT 第几条规则

​              PS: 若报错iptables：index of deletion too big，则需要明确要删除的表（ iptables -t nat -D PREROUTING 第几条规则）

* 防火墙操作

  * 查看防火墙状态: systemctl status firewalled

  * 开启防火墙：systemctl start firewalled // service firewalled start

  * 关闭防火墙：systemctl stop firewalled

    若遇到无法开启防火墙的情况

  * 先 systemctl unmask firewalled.service

  * 再 systemctl start firewalled.service

* **telent**：telnet ip port

* **ifconfig**： 查看网卡信息 & 确认ip等内容

  * 服务器若出现ping不同网关等情况：1）背后网线没插对口 2）ifconfig 后出现了类似于“brixxx”开头的网卡，ifconfig brixxxx down 停用后解决。
  * 设置网卡ip：
  
  ```
  # ifconfig 设置网卡eth0 的ip
  sudo ifconfig eth0 10.0.0.1/24 
  sudo ifconfig eth0 up
  ```

* **ip addr**：类似上述情况查看网卡ip

  ```
  root@test:~# ip addr
  1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default 
      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
      inet 127.0.0.1/8 scope host lo // host 则该网卡不可对外
         valid_lft forever preferred_lft forever
      inet6 ::1/128 scope host 
         valid_lft forever preferred_lft forever
  2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
      link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff  // MAC地址
      inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0 // global 则该网卡可对外
         valid_lft forever preferred_lft forever
      inet6 fe80::f816:3eff:fec7:7975/64 scope link 
         valid_lft forever preferred_lft forever
  ```

  PS: 

  * ip 地址后的scope 字段**【host：该网卡仅可提供本机相互通信；global：该网卡可对外】**
  * lo 全称 loop back，回环接口，多分配127.0.0.1，经内核处理后直接返回，不会在任何网络中出现。
  * 在 IP 地址的上一行是 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff。此行为网卡的物理地址，16进制，6byte表示。
  * <BROADCAST,MULTICAST,UP,LOWER_UP>: net_devic_flags, 网络设备的状态标识。
    * UP: 表示网卡处于启动状态
    * BROADCAST: 表示网卡有广播地址
    * MULTICAST: 表示网卡可以发送多播包
    * LOWER_UP: L1是启动的-网线已插入。
  * mtu 1500： 以太网默认最大传输单元MTU为 1500.
  * qdisc pfifo_fast：qdisc-queue discipline; pfifo_fast-在组成的3个波段中，每个波段使用先进先出规则.

  ```
  # 命令配置ip
  sudo ip addr add 10.0.0.1/24 dev eth0
  sudo ip link set ip eth0
  ```

  

* netstat/ss: 查看套接字、网络栈、网络接口、路由表信息

  * netstat/ss -s：查看及统计协议栈信息。

* sar -n DEV：网络吞吐和PPS统计信息查看

* tcpdump：apt-get install tcpdump

  基于libpcap，利用内核中的AF_PACKET套接字抓取网络接口中传输的包

  * 输出手册：man tcpdump 【tcpdump选项类】

  | 选项                 | 说明                        |
  | -------------------- | --------------------------- |
  | tcpdump -i eth0      | 指定网络接口，默认为0       |
  | tcpdump -nn          | 不解析ip地址和端口号名称    |
  | tcpdump -c5          | 限制抓包个数为5             |
  | tcpdump -A           | 以Ascii格式显示网络包内容， |
  | tcpdump -w file.pcap | 保存到文件中，              |
  | tcpdump -e           | 输出链路层头部信息          |

  * tcpdump 输出格式：时间戳 协议 源ip.源port > 目的ip.目的port 包详细信息

  | 表达式&示例                         | 说明          |
  | ----------------------------------- | ------------- |
  | host\|src host\|dst host ip         | 主机过滤      |
  | net\|src net\|dst net               | 网络过滤      |
  | port\|portrange\|src port\|dst port | 端口过滤      |
  | 直接加上协议名称                    | 协议过滤      |
  | and 、or、not等                     | 逻辑表达式    |
  | tcp[tcpflags]                       | 特定状态TCP包 |

  * 执行抓包过程需要增加过滤参数，否则很多不相干的包会被抓住。

    eg：tcpdump -nn udp port 53 or host 35.190.27.188

    * -nn：不解析抓包的域名、协议及端口。

  * 网络缓慢的情况下，可通过tcpdump对协议进行过滤之后，**计算具体时间戳差值，判断具体缓慢的点。**

* wireshark: windows软件使用方式具体搜，类似tcpdump的命令。

## 系统评估网络性能

**转发性能**

* 网络接口层和网络层：负责网络包的封装、寻址、路由以及发送和接收。【Packets Per Seconds是最重要的性能指标。特别是 64B 小包处理能力】
* 测试网络包处理能力的工具：hping3、pktgen。
  * pktgen：linux系统不能直接找到pktgen命令，需要加载pktgen模块后通过/proc/ 文件系统交互。
  * modprobe pktgen   &   ls /proc/net/pktgen

**TCP/UDP性能**

* 网络性能测试工具：iperf、netperf
  * apt-get install iperf3【输出：bandwidth 和对应的时间间隔】

**HTTP性能测试工具**

* ab：apt-get install -y apache2-utils
  * ab命令测试http性能：ab -c 1000 -n 10000 http://后端服务器ip

**应用负载性能**

上述各类协议的请求过程中，会附带很多payload，对于负载的监测可以有很多工具：

* wk: 编译安装，具体进行搜索。

## 网络监控&带宽查看

* ifconfig 确认网卡具体使用的哪一张
* ethtool 网卡名称：speed行为网卡带宽。
* 实时统计网卡带宽使用率：nload
* 实时监测网络状态：dstat -n

### iftop

实时观察带宽变化的工具，nethogs无法获得对应的pid【无法确认确实不占带宽还是其他情况】

**安装配置**

* 安装依赖：yum -y install flex byacc libpcap ncurses nc libpcap-devel
* 下载安装包：wget http://www.ex-parrot.com/pdw/iftop/download/iftop-0.17.tar.gz

* tar zxvf iftop-0.17.tar.gz
* cd iftop-0.17，执行命令配置：./configure --build=arm-linux
* make & make install 
* 执行iftop确认命令安装成功。

**字段含义及命令解析**

执行iftop后输出如下：

<img src='/images/img/iftop命令输出1.jpg'>

<img src='/images/img/iftop命令输出2.jpg'>

字段含义说明：

* 界面顶部：流量刻度尺，刻度分为五个大段显示，对应下面每行的白色横条
* 界面中部：
  * 最左边的 域名或ip表示当前服务器
  * 中间数据为“外部ip或域名”，=> 表示发送数据；<=表示接收数据。
  * 最右边3列为实时参数，表示“外部ip或域名”连接到当前服务器的2s，10s和40s内的平均流量值。
  * 白色横条（每行从最左边开始的白色横条）：对流量大小进行动态展示，以头部刻度尺为基准，通过这个流量图形条可以看出哪个ip流量最大，定位网络问题。
* 界面底部：
  * TX: 发送数据。
  * RX: 接收数据。
  * TOTAL: 发送和接收的全部流量。
  * cum：从运行iftop到目前的发送、接收和总数据量。
  * **peak：发送、接收及总的流量峰值。**【有时间限制】
  * rates：过去2s，10s，40s的平均流量。

**[命令参数示例](https://www.cnblogs.com/liulianzhen99/articles/17620576.html)**

* 显示特定网段的进出流量：iftop -F 106.11.79.0/24

  * 上述命令+将刻度尺设置为最大100M: iftop -F 106.11.79.0/24 -m 100M

* **每隔5s打印一次监听网卡流量信息（不实时监听），打印两次输出：**

  **iftop -i eth0 -n -t -s 5 -L 2**

* <u>**基于端口过滤：iftop -i enp125s0f0 -f 'port 53'**</u>

* 基于目的端口过滤：iftop -i enp125s0f0 -f 'dst port 53'

* 基于源端口过滤：iftop -i enp125s0f0 -f 'src port 53'

### nethogs

安装简单，可获得网卡中某个进程占用的带宽，但通过代码进行同步python测试，没有看到对应的pid出现。【无法确定是确实不占带宽还是什么情况】

**安装**

* yum install nethogs -y
* ifconfig 确定网卡后：nethogs 网卡

### sar

实时查看系统当前带宽信息，具体命令：sar -n DEV  1 【每隔1s输出一组网络收发报告数据】

### 网络发送拥塞窗口大小

```
# ss-查看当前服务器拥塞窗口大小
ss -nli | fgrep cwnd
# ip route change 修改初始拥塞窗口
ip route | while read r; do 
          ip route change $ r initcwnd 10;
     done
```

# 其他

## NAT网络地址转换

NAT（network Address Translation）

**原理**

* **<u>重写 IP 数据包源 IP 或目的 IP</u>**，普遍地用来**解决公网 IP 地址短缺问题**。
* **主要原理：多台主机共享同一个公网 IP 地址，来访问外网资源**。
* NAT 屏蔽了内网网络，也就为局域网中的机器提供了安全隔离。

* 既可以在支持NAT的路由器（NAT 网关）中配置 NAT，也可以在 Linux 服务器中配置 NAT(Linux 服务器实际上充当“软”路由器的角色)。

三类NAT：

- 静态 NAT：内网 IP 与公网 IP 是一对一的永久映射关系；

- 动态 NAT：内网 IP 从公网 IP 池中，动态选择一个进行映射；

- 网络地址端口转换 NAPT（Network Address and Port Translation）：内网 IP 映射到公网 IP 的不同端口上，让多个内网 IP 可以共享同一个公网 IP 地址。【Linux中配置的也是这种类型】

  根据转换方式不同，可以将NAPT分为三类：

  * 源地址转换 SNAT：目的地址不变，只替换源 IP 或源端口。【主要用于：多个内网 IP 共享同一个公网 IP ，来访问外网资源的场景。】
  * 目的地址转换 DNAT：源 IP 不变，只替换目的 IP 或者目的端口。【主要通过公网 IP不同端口号，来访问内网多种服务，同时隐藏后端服务器真实 IP 地址。】
  * 双向地址转换：同时使用 SNAT 和 DNAT。收包时，执行 DNAT，把目的 IP 转换为内网 IP；发包时，执行 SNAT，源 IP 替换为外部 IP。

## crontab

### 说明

crontab 多用于linux下定时执行脚本

* linux 一般情况下自带crontab，可通过**查看crontab状态命令**来确认是否安装。

* cron 、crond、crontab：

  * cron: **the general name for the service** that runs scheduled actions

  * crond: **the name of the daemon** that runs in the background and reads crontab files.

  * crontab：a crontab is a file containing jobs in the format:

    ```
    minute hour day-of-month month day-of-week  command
    ```

**安装**

* ubuntu：
  * 安装：apt-get install cron
* centos：
  * yum install vixie-cron【vixie-cron软件包是cron的主程序】
  * yum install crontabs

**常用命令**: 命令中cron 和 crond可互换，具体区别见上，通常情况操作cron

* **查看crontab状态**：service cron status
* 启动：service cron start
* 重启：service cron restart
* 停止：service cron stop
* 检查状态：service cron status
* 查询cron 可用命令：service cron
* 重新载入配置：service crond reload
* centos 加入开机自启动：chkconfig --level 345 crond on

**基本语法**

crontab [-u user] file     |      crontab [-u user] [-i] {-e | -l | -r}

* -u user：指定user的时间表
* **-l：列出当前系统的时程表**
* -r：删除当前时程表
* -e：执行编辑器来设定时程表，内定vi编辑器

### 时间格式

**minute hour day-of-month month day-of-week  command**

* 上述5个值取值为*时表示，每分钟/小时/天... 都要执行。
* min: 取值范围0-59。当其值为a-b时，表示a-b分钟这段时间内都要执行；当其值为：*/n时，表示每间隔n分钟执行一次；当其值为a,b,c时，表示第a,b,c分钟要执行。
* hour：取值范围0-23，当其范围为a-b时，表示每天从第a-b小时都要执行；当其值为：*/n时，表示每间隔n小时执行一次；当其值为a,b,c时，表示第a,b,c小时要执行。
* day-of-month：1-31
* month：1-12
* day-of-week：0-6【周天为0】
* 也可以将所有设定放入文件中，用crontab file 来设定执行时间

| 执行时间                 | 格式           |
| ------------------------ | -------------- |
| 每分钟执行一次           | \* \* \* \* \* |
| 每小时执行一次           | 0 \* \* \* \*  |
| 每天执行一次             | 0 0 \* \* \*   |
| 每周执行一次             | 0 0 \* \* 0    |
| 每月执行一次             | 0 0 1 \* \*    |
| 每月最后一天定时执行一次 | 0 0 L \* \*    |
| 每年执行一次             | 0 0 1 1 \*     |

**实例**

* 每分钟执行一次/bin/ls:   \* \* \* \* \* /bin/ls

* **每周执行一次 python脚本：0 0 \* \* 0 python3 /opt/wjt/main.py**

* 在 12 月内, 每天早上 6 -12 点，每隔 3 个小时 执行一次 /usr/bin/backup：

  0 6-12/3 * 12 * /usr/bin/backup

* 周一到周五每天下午 5:00 寄一封信给 alex@domain.name：0 17 * * 1-5 mail -s "hi" alex@domain.name < /tmp/maildata

PS: crontab 当程序在指定时间执行后，系统会发送一封邮件给当前用户，显示该命令执行内容，**若不希望收到类似邮件，在每一行最后空一格加上\> /dev/null 2>&1**

### 配置文件

* 全局配置文件：/etc/ 下 
  * 每天/周/月/小时需要执行的任务：cron.daily  cron.weekly  cron.monthly  cron. hourly  
  * 系统定期执行的任务：cron.d

### 用户自行配置任务

* 查看当前的时程表：crontab  -l

* 编辑crontab时程表：crontab -e

  * **完成编辑后，保存并退出：ctrl+x,按提示输入Y N 是否保存写入，enter【nano编辑器】**

* 查看系统日志中是否有相关定时任务日志：tail -f /var/log/syslog

  ```
  eg： CRON[pid]: (root) CMD (python3 /opt/wjt/test_cron.py)
  ```

* 重新启动crontab: service cron restart

* **停止任务：crontab -e 进入编辑界面后注释or删除掉后保存并退出。**

### 问题记录

* crontab 执行python3 xxxx.py 或执行了没输出【经过测试】

  * ubuntu查看crontab 系统日志： tail -f /var/log/syslog 【观察其中是否有相关定时任务的日志】

  * 问题定位：**crontab执行的定时python脚本中有：1）读取配置文件  2）读写文件的动作。一般定时任务都不会执行.** 

    ps: 脚本在执行时,由于是通过crontab去执行的,他的执行目录会变成当前用户的home目录,如果是root,就会在/root/下执行.

  * 解决方案：将执行的python命令放到shell脚本里，然后crontab 定时执行.sh 文件

    * 新建一个shell 脚本： run.sh
    * 开放权限：chmod +x run.sh
    * crontab 每分钟执行：\* \* \* \* \* /bin/sh /opt/wjt/run.sh

## ssh 建隧道

* 建隧道例子：ssh -f -N -L 9988:211.144.18.3:20022 root@10.42.25.13  
  *  将跳板机（211.144.18.3：20022）通过9988端口映射到本地13
  * ssh -f -N -L 9854:172.20.10.46:22 common@localhost -p 9988 
  * 将通过跳板机连接的目标服务器（172.20.10.46:22）通过9854端口映射到跳板机的端口9988，进而映射到本地

* 直接从本地电脑上传文件到远端服务器---完成上述映射之后: scp -p 9854 /cygdrive/待上传文件本地目录 root@localhost：/上传的目标路径

* 跳板机（13）传文件到远程目录下
  *  文件传到13 root 目录下
  *  scp -P 20022 dnspython.conf common@211.144.18.3:/home/common
  * 控制节点 /home/common 再scp

## 任务绑定CPU

当多个进程争抢一个CPU资源（或1个CPU资源分配了多个进程，前几个进程执行时资源耗尽）导致代码耗时不稳定，[参考](https://blog.csdn.net/qq_30683329/article/details/88779390)

* CPU 亲和力：CPU affinity是一种调度属性(scheduler property), 它可以**将一个进程"绑定" 到一个或一组CPU上**.在SMP(Symmetric Multi-Processing对称多处理)架构下，Linux调度器(scheduler)会根据CPU affinity的设置让指定的进程运行在"**绑定"**的CPU上,而不会在别的CPU上运行。**Linux调度器同样支持自然CPU亲和性(natural CPU affinity): 调度器会试图保持进程在相同的CPU上运行, <u>这意味着进程通常**不会在处理器之间频繁迁移,进程迁移的频率小就意味着产生的负载小。</u>

  因为**程序的作者比调度器更了解程序,所以我们可以手动地为其分配CPU核，而不会过多地占用CPU0，或是让我们关键进程和一堆别的进程挤在一起,**所有设置CPU亲和性可以使某些程序提高性能。

* CPU核心表示方法

  CPU affinity 使用位掩码（bitmask）表示，每一位都表示一个CPU

  最低位表示第一个逻辑CPU,最高为表示最后一个逻辑CPU

  CPU affinity 典型的表示方法是使用16进制：

  1）0x00000001   is processor #0

  2）0x00000003   is processors #0 and processor #1

  0Xfffffffff      is all processors

* 如何确定绑核成功

  top命令 -> F -> last used CPU ->ESC ->退回正常top界面，就可以看到哪些进程泡在哪些核上，若lastCPU一直保持不动，则绑核成功。

* Taskset 使用方法

  通过该命令，可以将一个启动的进程直接绑定在某个核上运行。

  * 命令格式taskset-cp cpu(cpu-list) pid

    eg：taskset -cp 1(1-3) 1927 将进程号为1927的进程绑定在核1/1-3上

  * taskset -h 查看具体参数说明

## close 和 shutdown 关闭连接在linux上的区别

* close() ： 让连接变成孤儿连接。
* shutdown() : 允许在半关闭的连接上长时间传输数据。【互联网中往往服务器是主动关闭连接的一方：服务器发送完请求的数据后就半关闭连接，但此时数据还可以发送。】

* 大多数应用程序并不使用shutdown，因为shutdown 没有限制半关闭状态接收数据的时间。【netstat 发现大量close_wait的进程时，要么代码有bug，没有close；要么负载过高。close() 被延迟执行。】

## linux设置时间命令

* sudo date --set="2024-07-15 17:08:00"

# 操作系统

centos、debian等操作系统使用时遇到的问题记录

## Debian 装GUI图形界面

* sudo su –
* 更新debian系统： apt update & apt –y upgrade
* GNOME 使用一下命令安装桌面环境【耗时】：apt –y install task-gnome-desktop
* 上一步完成后，分配graphical runlevel：systemctl set-default graphical.target
* 默认情况通过GNOME 显示管理器GDM禁用root用户登录，去掉限制：
  * vi /etc/pam.d/gdm-password
  * 注释掉：# auth  required pam_succeed_if.so user !=root quite_success
* 重启debian服务器：reboot
* 参考：搜索“如何为debian11安装图形用户界面GUI"

## Centos

### VM 安装centos 7.9 无ip，ping不通主机的情况下ip配置

[centos7.9阿里云下载](http://mirrors.aliyun.com/centos/7.9.2009/isos/x86_64/)

* VMware 虚拟机默认连接类型是NAT，需要将其修改为 “桥接模式”，这样虚拟机也可以获得1个ip
* 命令查看网卡ip：ip addr 【此时ens33仍然没有ip，因为centos默认不开启ens33】
* 配置动态ip：vi /etc/sysconfig/network-scripts/ifcfg-ens33
  * 只需将最后一行onboot = NO 修改为 onboot = yes 即可。【因为默认dhcp协议】
  * 之后systemctl restart network 重启网卡即可看到ens33有ip

**PS: 若动态生成的主机ip与本机不在一个网段，可能会ping不通，需要使用静态ip配置**

* **配置静态ip**：vi /etc/sysconfig/network-scripts/ifcfg-ens33

  * BOOTPROTO=dhcp 修改为 BOOTPROTO=static

  * ONBOOT=no 修改为 ONBOOT = yes

  * 添加下面信息

    ```
    IPADDR=10.1.4.111
    NETMASK=255.255.255.0
    GATEWAY=10.1.4.254
    ```

  * 重启网卡后查看ip：systemctl restart network 

### ulimit值调整并修改最大链接数方法

* 暂时修改：ulimit -n 100001 【服务器重启后失效】

* 永久修改，[参考]( https://www.jianshu.com/p/1d3d368365bf)

  * vim /etc/security/limits.conf：在文件end of file 之后加上：【noproc -最大进程数，nofile-最大文件打开数】

    ```shell
    * soft nofile 65535
    * hard nofile 65535
    * soft nproc 65535
    * hard nproc 65535
    ```

  * ulimit -n： 查看确认最大链接数已被修改

### 问题记录

* centos yum 无法使用

  解决方案：/etc/yum.repos.d/ 下Centos-Base.repo 无法正常使用，需更新yum源，[参考](https://zhuanlan.zhihu.com/p/430561706)

* arm centos yum 无法正常使用---yum源只有x86

  解决方案：更新yum源，[参考](https://zhuanlan.zhihu.com/p/430561706)

## Linux

### 安装虚拟机

[参考](https://www.tecmint.com/install-vmware-workstation-in-linux/)

* 下载vmware workstation [安装文件](https://www.vmware.com/products/workstation-pro/workstation-pro-evaluation.html)

* 上传到15对应目录 /root/vmware/

* 修改权限否则报错: chmod a+x VMware…xx.boundle
* 安装: **./VMwarexxxxx.boundle**
* 启动vmware

# 系统配置相关

## /etc/hosts/

​      在通过服务器IP连接某个部署的文件或进程时，可直接使用IP 或 linux /etc/hosts/文件下写好的别名：

```
 vim /etc/hosts/
 --------------------
 ip 别名
```

## /dev/null

* /dev/null 称为空设备，是一个特殊的设备文件，它将丢弃一切写入其中的数据（但报告写入成功），读取该文件会立即得到一个EOF。类似于一个黑洞，常用于丢弃不需要的输出流。使用这些操作通常由重定向完成。

* 使用nohup 重定向的命令行方式： **nohup python3 new_main.py >/dev/null 2>&1 &**

## /etc/resolv.conf

若上述DNS配置文件无法解析DNS，则在**文件最后增加一个空格**：eg-- nameserver 8.8.8.8 

# shell 脚本

[参考内容](https://www.runoob.com/linux/linux-shell.html)

在一般情况下，人们并不区分 Bourne Shell 和 Bourne Again Shell，所以，像 **#!/bin/sh**，它同样也可以改为 **#!/bin/bash**

## 脚本运行

* cd 到脚本目录下后 执行 sh xxxx.sh
* 使用脚本绝对路径：sh /DNS/XXX.sh
* 增加权限后执行，cd到具体路径后：chmod +x ./test.sh #使脚本具有执行权限
* 将shell 脚本作为解释器参数运行（这种运行方式，不需要在第一行指定解释器信息， #！/bin/bash 这个）eg： /bin/sh test.sh              /bin/php test.php

## 脚本命令

* \#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 程序。
* Shell 注释： 和python 一致，通过 # 注释
* echo 命令： shell 脚本的字符串输出命令，eg ：echo "Hello World !"，显示和输出命令
* shell字符串：单双引号，不用引号均可
  * 单引号：单引号当中的所有字符都会原样输出，单引号字符串引变量无效
  *  双引号：可以引变量，转义字符
* shell变量定义：不加美元符号定义，可重复定义已定义变量。
* 使用一个定义过的变量，只需要再其前面加美元符号即可： $
* **只读变量** 添加关键词 readonly 将变量定义为只读变量，只读变量的值不能被改变
* 脚本传参：执行shell 脚本向脚本传递参数，脚本内获取参数的格式： $n, n 为1个数字，1为执行脚本的第一次参数，2 为执行脚本的第二个参数

# CPU

## 平均负载

* **uptime**：系统负载情况查看命令：top、uptime【系统运行时间】

```
$ uptime
  09:10:23 up 2 days,20:14, 1 user, load average: 0.63,0.88,0.68
//当前时间 系统运行时间  正在登录用户数  过去1min，5min,10min 平均负载
```

**平均负载定义**

系统处于**可运行状态**和**不可中断状态**的平均进程数，即**平均活跃进程数**，和CPU使用率没有直接关系。

* 可运行状态：正使用或正等待CPU的进程。【PS命令处于R状态的进程】
* 不可中断状态：正处于内核态关键流程中的进程，且这些流程不可打断。【PS命令中看到D状态的进程，常见为等待硬件设备的I/O相应，不能】
* 最理想情况为一个CPU刚好运行一个进程【平均进程为2：2CPU机器刚好全被占用，4CPU机器50%空闲】

**合理值**

* 首先确定CPU个数：top  或 cat/proc/cpuinfo。
* 平均负载大于CPU个数时，数据过载。
  * 实际情况，平均负载高于70%CPU数量时，开始出现进程响应慢、影响服务正常功能等问题。
* uptime当中average load的三个值合理范围：
  * 若1min、5min、10min值相差不大，说明负载很平稳。
  * 若1min值远小于15min值，说明最近1min系统负载在减小，过去15min有很大负载。
  * 若1min值远大于15min，说明最近1min负载在增加，可能是临时性也可能是持续增加，一旦1min值超过CPU即为过载。

**平均负载&CPU使用率**

* 平均负载：单位时间内活跃进程数，不仅包括正在使用的CPU，还包括等待CPU和等待I/O的进程。
* CPU使用率：单位时间CPU繁忙情况的统计，不一定一致
  * CPU密集型进程：大量使用CPU导致平均负载升高，此时两者一致。
  * I/O密集型进程：等待I/O也会导平均负载升高，但CPU使用率不一定高；
  * 大量等待CPU的进程调度【上下文切换】：也会使平均负载升高，此时CPU使用率也会较高。

**平均负载升高分析命令**

linux自带的分析命令：top、ps、lsof【若无法联网安装其他工具】

mpstat、pidstat：sysstat 常用linux性能工具包中的两个命令。

* sudo apt install sysstat
* mpstat：常用多核CPU性能分析工具，实时查看每个CPU性能能指标及所有CPU平均指标【iowait】。【监测所有CPU&每5s输出一组数据：mpstat -P ALL 5 】
  * %usr：显示对应cpu的使用率。
  * %iowait：io等待占用率。
* pidstat：常用性能分析工具，实时查看进程CPU、内存、IO、上下文切换。【查询具体哪个进程导致CPU占用率为100%】间隔5s后输出一组数据：pid -u 5  1
  * %cpu：显示pid对应cpu的使用率。
  * %wait：进程等待CPU的时间比例。
  * Command：pid对应的命令。

## 上下文切换&如何排查

​       CPU 上下文切换，是保证 Linux 系统正常工作的核心功能之一，一般情况下不需要我们特别关注。但过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，从而缩短进程真正运行的时间，导致系统的整体性能大幅下降。

​      多进程竞争CPU产生CPU上下文切换导致延时&负载上升。

* Linux 多任务操作系统，支持任务数量远大于CPU数量，通过操作系统轮流将任务分配给各CPU模拟多任务同时运行。
* 每个任务运行前，需要系统设置好**CPU寄存器和程序计数器**用于加载任务。
* **CPU上下文**：CPU寄存器&程序计数器都是在CPU运行任何任务前必须依赖的环境。
* **CPU上下文切换**
  * 保存前一个任务的CPU上下文，加载新任务上下文到寄存器和程序计数器，最后跳转到程序计数器所指新位置，运行任务。 
  * 保存下来的上下文会存储在系统内核中，在任务重新调度执行时再次加载，保证状态&连续运行。
  * 3类常见"任务"：进程、线程、**中断**

### 进程上下文切换

​       根据[ Tsuna ](https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html)测试报告，**每次上下文切换需要ns级到微秒级 CPU 时间。**在进程上下文切换次数较多时，容易导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短真正运行进程的时间，导致平均负载升高。

​       linux 按照权限将进程运行空间分为内核空间Ring0&用户空间Ring3。

* 内核空间Ring0：具有最高权限、可直接访问所有资源。
* 用户空间Ring3：只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统内核调用后才能访问。

​      进程从用户态到内核态需要通过**系统调用**来完成。系统调用本身也存在上下文切换【执行不同调用命令都需要先加载指令位置】。

* **<u>用户态</u>**：进程在用户空间运行时。
* **<u>内核态</u>**：进程在内核空间运行时。进程由内核管理调度，**进程切换只能发生在内核态**。
* **一次系统调用，发生两次CPU上下文切换**:切过去之后还要恢复。

* **进程上下文**：不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。【因此，进程上下文切换就比系统调用多一步：在保存当前进程内核状态和 CPU 寄存器前，先把该进程的虚拟内存、栈等保存下来。加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。】

**进程上下文切换时间**

* 在进程切换时才需要切换上下文。
* 每个 CPU 维护一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，**选择优先级最高和等待 CPU 时间最长的进程来运行。**

**进程被调度到CPU上运行的情况**

* 公平调度：CPU时间划分为时间片，轮流分配各个进程，分配时间耗尽时，切换下一个进程。
* 系统资源不足时，当前进程挂起，调度其他进程运行。
* 优先级更高进程出现时，当前进程挂起，保证优先级更高进程运行。
* 硬件中断时，执行内核中断服务程序。

### 线程上下文切换

* **线程：调度基本单位，**内核中任务调度实际调度对象是线程。

* **进程：资源拥有基本单位**，给线程提供虚拟内存、全局变量等资源。

  **<u>同一个进程中的各个线程资源共享，不同进程中的线程资源不共享</u>**

**线程切换具体情况**

* 前后两个线程属于不同进程：因为资源不共享，所以切换过程等价于进程上下文切换。
* 前后两个线程属于相同进程：因为资源共享，所以切换时虚拟内存等资源保持不变，**需要保存后切换的为：私有数据、寄存器等不共享数据。**

Conclusion：**同进程中线程上下文切换消耗更小**【多线程代替多进程的优势】【**python因为GIL的存在，多线程实际是单进程，多进程才能避开GIL实现真正并发**】

### 中断上下文切换

* 快速响应硬件事件：打断进程正常调度和执行，转而调用中断处理程序。【打断时需要保存被打断进程状态以便恢复】
* 中断比进程有更高优先级。

### 查看系统上下文切换情况-vmstat

```
$vmstat 5 # 每隔5s输出一组数据
procs    ---memory---       -swap- --io-- -system--  ---cpu---
r  b  swpd free buff cache  si so  bi bo in   cs     us sy id wa st
0  0  0    1024 1024 1024   0  0   0  0  25   33     0   0 100 0  0
```

其他部分可以根据意思确认，主要关注以下4列

* **CS: context switch 每秒上下文切换次数。**
* in：interrupt 每秒中断的次数。
* r：Running or Runnable 正在运行or等待CPU的进程数。
* b：Blocked处于不可中断睡眠状态的进程数。

**pidstat：加上-w选项，可以查看每个进程上下文切换情况**

```
$ pidstat -w 5
linux 4.15.0 ubuntu 09/23/18 _x86_64_ (2cpu)
09:03:31   UID   PID   cswch/s nvcswch/s Command
09:04:33   0     1     0.20    0.00      systemd
```

两行重点关注对象

* cswch：每秒自愿上下文切换【进程无法获取所需资源导致的上下文切换】
* nvcswch：非自愿每秒上下文切换次数。【进程时间片已到等原因被系统强制切换】

**根据cpu性能，上下文切换次数在1w以内比较稳定。**

- 自愿上下文切换变多，说明进程都在等待资源，有可能发生了 I/O 等其他问题；
- 非自愿上下文切换变多，说明进程都在被强制调度【争抢 CPU】，说明 CPU 的确成了瓶颈；
- 中断次数变多，说明 CPU 被中断处理程序占用，查看 /proc/interrupts 分析具体的中断类型。

## CPU使用率

**节拍率**

* 为了维护 CPU 时间，Linux 通过事先定义的**节拍率**（内核中表示为 HZ），触发时间中断，并使用全局变量 Jiffies 记录了开机以来的节拍数。每发生一次时间中断，Jiffies 的值就加 1。

* 节拍率 HZ 是内核可配选项，可以设置为 100、250、1000 等，通过查询 /boot/config 内核选项来查看它的配置值。【eg：节拍率设置为 250，也就是每秒钟触发 250 次时间中断。】
* 因为节拍率HZ是内核选项，所以用户空间无法直接访问【内核向用户空间提供了一个用户空间节拍率USER_HZ,固定为100，也就是1/100s】

**CPU统计信息查看命令**:  cat/proc/stat|grep ^cpu

**CPU使用率=（1-空闲时间/总CPU时间）**

* 实际计算中，性能工具一般会计算：间隔一段时间的两次值作差后的这段时间平均CPU使用率。

**CPU使用率的具体查看**

* top：系统总体CPU和内存使用情况&各个进程资源的使用情况。
* ps：每个进程的资源使用情况。
* pidstat：区分用户态和进程态【%usr-用户态CPU使用率；%system-内核态CPU使用率；%guest-运行虚拟机CPU的使用率;%wait-等待cpu使用率；%CPU-总cpu使用率】

**CPU使用率过高**

*  top、ps、pidstat 等工具能找到 CPU 使用率较高（比如 100% ）的进程。
* 寻找具体占CPU高的函数：
  * GDB（The GNU Project Debugger）：找到大致函数后中断调试。
  * **perf：**以性能事件采样为基础，可以线上使用。
    * **<u>perf top: 类似top，实时显示占用CPU时钟最多的函数或指令，查找热点函数。</u>**   [perf top -g -p pid]   -g 开启调用关系分析， -p指定进程号
    * **perf record**/perf report: 实时展示性能及是否保存。

### 火焰图Flame Graph

针对perf数据汇总问题，Brendan Gragg通过矢量图展示结果。-自行搜图

* 横轴：采样数和采样比例【**一个函数占用横轴越宽，表示执行时间越长，同一层多个函数按照字母排序】**
* 纵轴：由下到上根据调用关系逐个展开。【**上下相邻的两个函数中，下面的函数是上面函数的父函数，调用栈越深，纵轴越高。**】
* 图中的颜色：没有特殊意义，仅用于区分不同函数。

常见几种火焰图

* on-cpu flame graph：表示CPU繁忙情况，用于CPU使用率较高的情况。
* off-cpu flame graph：表示cpu 等待 I/O、锁等各种资源阻塞的情况。
* 内存火焰图：内存分配和释放的情况。
* 热/冷火焰图：on-cpu/off-cpu 结合的情况
* 差分火焰图：两个火焰图的差分情况，红色-增长，蓝色-衰减，用于对比。

**安装配置**

* git clone https://github.com/brendangregg/FlameGraph
* cd FlameGraph[安装路径为执行git clone的路径]

**生成火焰图**

* step1：**执行perf record 生成数据。【当前目录生成perf.data】**
* step2：ctrl+C 结束后执行perf script：将perf record 转为可读的采样记录：perf script -i perf.data &> perf.unfold
* step3：将perf.unfold 中的符号进行折叠：./stackcollapse-perf.pl perf.unfold &> perf.folded
* step4：生成svg图，可直接用浏览器打开xxx.svg 文件：./flamegraph.pl perf.folded > perf.svg

**CPU使用率过高但找不到高CPU应用的情况**

启动服务后，发现系统CPU占用较高，但top、pidstat命令直接观察CPU相关参数，没有发现占用CPU较大进程的情况。

* 观察实际业务与top中进程状态是否为R（运行）或S（sleep）判断实际占用CPU的进程。

## 某进程pid不停改变可能的情况

**若top观察发现某个进程pid不停改变**，可能的情况有如下几种：

* 1st：进程不停崩溃重启---进程在退出后被监控系统自动重启【supervisor】
* 2nd：进程都是短时进程，一般只运行极短时间就结束，很难用top发现。

**查找父进程命令pstree**

```
$ pstree | grep 进程名
```

通过输出的进程调用关系，进入对应的源码中，找到这部分调用的源码进一步判断。

## execsnoop-短时进程性能查询工具

通过ftrace实时监控进程的exec()行为，并输出短时进程的基本信息，包括PID、父进程PID、命令行参数及执行结果。

```
$ execsnoop
输出结果包含所有短时启动的进程
```

**小结**

CPU 使用率是最直观和最常用的系统性能指标，更是我们在排查性能问题时，通常会关注的第一个指标。 比如：

- 用户 CPU （%user）和 Nice CPU （%nice）高，说明用户态进程占用了较多的 CPU，所以应该着重排查进程的性能问题。
- 系统 CPU 高，说明内核态占用了较多的 CPU，所以应该着重排查内核线程或者系统调用的性能问题。
- I/O 等待（%iowait） CPU 高，说明等待 I/O 的时间比较长，所以应该着重排查系统存储是不是出现了 I/O 问题。
- 软中断（%softirq）和硬中断（%irq）高，说明软中断或硬中断的处理程序占用了较多的 CPU，所以应该着重排查内核中的中断服务程序。
- 当碰到无法解释的 CPU 使用率问题时，先要检查一下是不是短时应用在捣鬼。【短时应用的运行时间比较短，很难在 top 或者 ps 这类展示系统概要和进程快照的工具中发现，你需要使用记录事件的工具来配合诊断，比如 execsnoop 或者 perf top。】

## 中断

* 除了iowait，软中断softirq CPU使用率升高也是最常见的一种性能问题。

* 中断实质：异步事件处理机制，可以提高系统的并发处理能力。【尽可能快的处理完毕，减少对正常进程的影响】
* 中断处理程序在响应中断时，还会临时关闭中断。这就会导致上一次中断处理完成之前，其他中断都不能响应，也就是说中断有可能会丢失。

**软中断**

为解决中断执行过长和中断丢失的问题，linux将中断分成上下半部：

* 上半部【硬中断】：快速处理中断，中断禁止模式下运行，处理和硬件相关or时间敏感的工作。
  * 
* 下半部【软中断】：延迟处理上半部未完成的工作，以内核线程的方式运行。
  * 每个cpu对应一个软中断内核线程，名字为：**“ksoftirqd/0”**
  * 一些内核自定义事件也属于软终端【内核调度、RCU锁等】

**软中断查看命令&内核线程查看命令**

* 软中断运行情况：cat /proc/softirqs
  * 上述命令能够看到各种类型中断在不同CPU上的累计运行次数。
* 硬中断运行情况：cat /proc/interrupts 

## 内核线程CPU利用率过高

ksoftirqd 在高并发场景的CPU利用率通常较高。【网络收发的软中断导致】

### 内核线程

* pid = 0：idle进程，系统创建的第一个进程，在初始化1号和2号进程后，演变为空任务。cpu上没有其他任务执行时，就会运行他。
* pid = 1：<u>用户态</u>进程的“祖先”：PID=1 的init进程，通常是systemd进程，管理其他用户态进程。
* pid = 2：**kthreadd** 进程，内核态运行，管理内核进程。

  因此，要查找内核线程，只需从2号进程开始，查找它的子孙进程：ps -f --ppid 2 -p 2.

   其他常见内核线程如下：

* kswapd0：内存回收。
* kworker：执行内核工作队列，分为绑定cpu和未绑定cpu.
* migration: 负载均衡过程中，将进程迁移到cpu上。每个cpu都有一个migration内核线程。
* jbd2/sda1-8:jbd（journaling block device), 为文件系统提供日志功能，保证数据完整性。每个使用了ext4文件系统的磁盘分区，都会有一个jbd2内核线程。
* pdflush：用于将内存中的脏页(修改过还未写入磁盘的文件页)写入磁盘。

## 快速分析CPU瓶颈套路

CPU性能-根据指标找工具

| 性能指标          | 工具                                 | 说明                                                         |
| ----------------- | ------------------------------------ | ------------------------------------------------------------ |
| 平均负载          | uptime 、top                         | uptime最简单 、top更全面                                     |
| 系统整体CPU使用率 | vmstat、mpstat、top、sar、/proc/stat | top、vmstat、mpstat仅支持动态查看，sar还可以输出历史数据，/proc/stat 数据来源 |
| 进程cpu使用率     | top、pidstat、ps                     | top、ps按cpu使用率给进程排序，pidstat显示实际使用了cpu的进程 |
| 系统上下文切换    | vmstat                               | 上下文切换次数&运行状态和不可中断状态进程数量                |
| 进程上下文切换    | pidstat                              | -w 选项参数需要加上                                          |
| 软中断            | top、/proc/softirqs                  | /proc/softirqs 提供各种软中断在cpu上运行的累计次数；top提供软中断CPU使用率 |
| 硬中断            | vmstat、/proc/interrupts             | /proc/interrupts提供各种应中断在cpu上运行的累计次数          |
| 网络              | iftop、tcpdump                       | 网络收发、带宽等 & 抓包情况                                  |
| I/O               | dstat、sar                           | 都提供I/O整体情况                                            |
| CPU               | /proc/cpuinfo                        | lscpu更直观                                                  |
| 事件剖析          | perf                                 | 分析cpu缓存及内核调用                                        |

# 内存

通常情况下的内存：物理内存【主存（动态随机访问内存DRAM）】

* 只有内核才可以直接访问物理内存，进程访问的是虚拟内存。
* 虚拟空间=内核空间+用户空间【不同字长的处理器地址空间范围不同】

​       PS: 进程用户态访问用户空间，内核态访问内核空间。(每个进程地址空间都包含了内核空间，但这些内核空间都是相同的物理内存。因此虚拟内存大于分配的物理内存，只有实际使用虚拟内存的进程才会分配物理内存，通过内存映射来管理。

* **内存映射**：将虚拟内存地址映射到物理内存地址。内核为每个进程维护一张页表，记录虚拟地址和物理地址的映射关系。
  * 页表实际存储在CPU内存管理单元MMU中，确保CPU通过硬件找到要访问的内存。
  
  * MMU不以字节为单位来管理内存，而是规定一个内存映射最小单位（页），通常4kB，每次内存映射，都需要关联4KB整数倍内存空间。
  
  * 对于页表项过多的问题，linux提出两种解决方案：多级页表&大页。
  
    PS: 多级页表【内存分区管理，通过索引对内存块进行访问。】
  
    PPS:大页【比普通更大的内存块，通常用于使用大量内存的进程上，oracle，dpdk等】
  
* **虚拟内存空间分布**：从低到高分为五种不同内存段

  * 只读段：包括代码和常量。
  * 数据段：包括全局变量。
  * 堆：包括动态分配的内存，从低地址开始向上增长。
  * 栈：包括局部变量和函数调用的上下文。【固定8MB】
  * 文件映射段：包括动态库、共享内存等，从高地址开始向下增长。

## 内存分配与回收

* malloc() 是C标准库提供的内存分配函数，在系统调用上有两种实现方式：
  * brk()：减少缺页异常发生，提高内存命中率；工作繁忙时频繁释放内存会造成内存碎片，释放后不立即归还系统，缓存起来重复利用。【分配的内存在释放时不立即规划】
  * mmap（）：在释放时直接归还系统，每次mmap都会发生缺页异常；工作繁忙时，频繁内存分配会导致大量缺页异常【仅建议对大块内存使用mmap】
* 上述两种调用发生时没有真正分配内存，这些**内存都是在首次访问时分配【缺页进入内核，由内核分配】**
* linux 使用伙伴系统来管理内存分配：以页为单位管理内存，并通过相邻页合并，减少内存碎片化。
* 对内存来说，若只分配不释放就会造成内存泄露，因此在代码最后一定要调用：free() 或 unmap()

系统不会任由某个进程用完所有内存，在内存紧张时，会通过一系列方法回收：

* 回收缓存：Least Recently Used 回收最近最少的内存页面。
* 杀死进程：内存紧张时，通过OOM，直接杀掉大量占用内存的进程。
* 回收不常访问的内存，将内存通过交换分区swap直接写入磁盘。

**swap**

swap 其实就是将一块磁盘空间当做内存使用，将进程中暂时不用的数据存入磁盘（换出）；当进程访问这些内存时，再从磁盘读出这些数据到内存（换入）。

**OOM**

out of memory，内核保护机制，监控进程的使用情况，使用oom_score 为每个进程的内存使用情况进行评分。

* 一个进程内存消耗越大，oom_score 越大；
* 一个进程运行占用cpu越多，oom_score 越小；

用户内存空间包括多个不同的内存段，这些内存段为正是应用程序使用内存的基本方式。

* 栈内存：代码中定义一个局部变量， *int data[64]* ，就有了一个可以存储 64 个整数的内存段。由于这是一个局部变量，它会从内存空间的栈中分配内存。
  * **栈内存由系统自动分配和管理**。一旦程序运行超出这个局部变量作用域，**<u>栈内存就会被系统自动回收，不会产生内存泄漏的问题</u>**。
* 堆内存：若事先不知道数据大小，就需要标准库函数malloc() 在代码中**动态分配内存，此时系统会从<u>内存空间的堆中分配内存</u>。**
  * **堆内存由应用程序自己来分配和管理**。<u>**除非程序退出，堆内存并不会被系统自动释放，需要应用程序明确调用库函数 *free()* 来释放它们**</u>。如果应用程序没有正确释放堆内存，就会造成内存泄漏。
* 只读段内存：包括代码和常量。**因为只读，不会再去分配新内存，也不会产生内存泄露。**
* 数据段内存：包括全局变量和静态变量。**变量在定义时就已经确定了大小，所以也不会产生内存泄漏。**
* 最后一个内存映射段：包括动态链接库和共享内存。**共享内存由程序动态分配和管理**。若程序在分配后忘了回收，就会导致跟堆内存类似的泄漏问题。

## 内存中的buffer 和 cache

内存查看命令：free 和 top。buffer 和 cache 从 free 命令得到。因此，执行man free 后，可以得到如下说明：

* buffer：memory used by kernel buffers (buffers in /proc/meminfo)
* cache:   memory used by the page cache and slabs. (cached and SReclaimable in /proc/meminfo)

buffer 和 cache 数据来源均为 /proc/, 具体区别如下：

* Buffers 是**对原始磁盘块的临时存储**，也就是用来**缓存磁盘的数据**，通常不会特别大（20MB 左右）。这样，内核就可以把分散的**写**集中起来，统一优化磁盘的写入，将把多次小的写合并成单次大的写等。
* Cached 是**从磁盘读取文件的页缓存**，也就是用来**缓存从文件读取的数据**。下次访问这些数据时，可直接从内存中获取，不需再次访问缓慢的磁盘。
* SReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。

**proc文件系统**

/proc 是linux内核提供的特殊文件系统，是用户跟内核交互的接口。

* 用户可从/proc 中查询和修改内核运行状态和配置选项：cat /proc/xxxx
* /proc 也是很多性能工具数据来源，通过： man proc 搜索对应解析。

### 利用系统缓存优化代码运行效率

**根据实际案例观察**

* **Buffer：对磁盘数据的缓存，读写都会用上**

* **Cache: 对文件数据的缓存，读写都会用上**

Buffer和Cache设计目的是为了提升系统的I/O性能，因此，从读写角度可以：

* 写：代码在数据真正落盘前就返回做其他工作。
* 读：提高频繁访问数据的代码部分读取速度，降低频繁I/O对磁盘的压力。

**可行的思路**

* **缓存命中率**（直接通过请求缓存获取数据的次数占总请求次数的百分比）：缓存命中率越高，使用缓存带来的收益越高，代码性能越好。

  * **cachestat：整个操作系统缓存的读写命中情况。**
  * **cachetop：每个进程的缓存命中情况。**
  * 两者均为bcc软件包的一部分,因此需要先安装bcc

* **指定文件在内存中的缓存大小**：通过pcstat来查看文件在内存中的缓存大小及比例

* 如果为系统设置直接I/O的标志，就会绕过系统缓存，导致速度极慢。

  * strace：判断应用程序是否用了直接 I/O。

    命令：strace -p $(pgrep app)   查找示例app的进程号并输出结果

    【结果若包含：O_RDONLY-只读打开，O_DIRECT-直接读绕过缓存】

## 内存泄露-动态内存未释放引起

### 定位和处理

* 定位内存泄露：

  执行：vmstat 3 # 每隔3s输出一组数据

  * 若memory 列free 不停变化且为下降趋势 & buffer、cacahe 基本不变，则还需进一步使用工具来确认是否存在内存泄露的问题。
  * 专门检测内存泄露的工具：memleak【跟踪系统或指定进程的内存分配、释放请求、定期输出未释放内存和相应调用栈的汇总情况】
  * 通过memleak 的输出结果定位到未释放内存的代码后，进一步就是查看源码进行解决。【添加free(变量)释放内存】

* Memleak 安装配置及使用命令

  * bcc 软件包中的一个工具。

  * **执行：/usr/share/bcc/tools/memleak**

    eg： /usr/share/bcc/tools/memleak -a -p 进程pid 【-a 显示每个内存分配请求的大小及地址】

    **从输出结果可看出pid对应进程是否在不停分配内存且内存未回收，同时可以看到具体进程中占用内存未释放的函数。**

    ```
    $ docker cp app:/app /app
    $ /usr/share/bcc/tools/memleak -p $(pidof app) -a
    Attaching to pid 12512, Ctrl+C to quit.
    [03:00:41] Top 10 stacks with outstanding allocations:
        addr = 7f8f70863220 size = 8192
        addr = 7f8f70861210 size = 8192
        addr = 7f8f7085b1e0 size = 8192
        addr = 7f8f7085f200 size = 8192
        addr = 7f8f7085d1f0 size = 8192
        40960 bytes in 5 allocations from stack
          fibonacci+0x1f [app] #app进程中fibonacci 函数未释放
    ```

    代码新增free()修复后重新执行memleak不再显示具体的addr，证明内存已释放。

## swap

当内存泄露or运行大内存应用程序导致内存资源紧张时，系统可能采取两种方式：内存回收 or OOM杀死进程（杀死内存占用过大的进程）。

**内存回收【文件页&匿名页都在内存回收范围内】**

* 文件页回收：直接回收缓存，或者把脏页写回磁盘后再回收。
* 匿名页回收：通过 Swap 机制，把它们写入磁盘后再释放内存。

* **缓存和缓冲区-文件页（File-backed Page）**：可回收内存。在内存管理中。大部分文件页可直接回收，需要时，再从磁盘重新读取。

* **脏页**：被应用程序修改过，且还没写入磁盘的数据。需先写入磁盘，再进行内存释放。写入方式：
  * 应用程序中，通过系统调用 fsync ，把脏页同步到磁盘中。
  * 系统，内核线程pdflush刷新脏页。
* **内存映射获取的文件映射页（文件页）**：它也可以被释放掉，下次再访问的时候，从文件重新读取。

* **应用程序动态分配的堆内存（匿名页Anonymous Page）**：可能还要被访问，无法直接回收。【若内存在分配后访问较少，也可以暂时先存放在磁盘里，释放给其他内存更需要的进程】

**降低 Swap 的使用，可以提高系统的整体性能。**

- 禁止 Swap，服务器的内存足够大，除非有必要，禁用 Swap 就可以了。随着云计算的普及，大部分云平台中的虚拟机都默认禁止 Swap。
- 如果实在需要用到 Swap，可以尝试降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。
- 响应延迟敏感的应用，如果它们可能在开启 Swap 的服务器中运行，你还可以用库函数 mlock() 或者 mlockall() 锁定内存，阻止它们的内存换出。

### 原理

* Swap 将不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。
* 实质：把一块磁盘空间或者一个本地文件，当成内存来使用。它包括换出和换入两个过程。
  * **换出:进程暂时不用的内存数据存到磁盘并释放这些数据占用的内存。**
  * **换入:进程再次访问这些内存的时候，把它们从磁盘读到内存中来。**

* Swap 其实是把系统的可用内存变大了。即使服务器的内存不足，也可以运行大内存的应用程序。

【内存再大，对应用程序来说，也有不够用的时候】

* 典型的场景：内存不足时，有些应用程序不希望被 OOM 杀死，而是希望能缓一段时间等待人工介入，或者系统自动释放其他进程的内存，分配给它。

* 笔记本电脑的休眠和快速开机的功能，也基于 Swap 。
  * 休眠时，把系统内存存入磁盘；再次开机时，从磁盘中加载内存就可以。省去很多应用程序的初始化过程，加快了开机速度。

**内存是否需要被回收的场景**

* 有新的大块内存分配请求，但是剩余内存不足：

  * **直接内存回收：**系统回收一部分内存（比如前面提到的缓存），进而尽可能地满足新内存请求。

* **kswapd0:**专门的内核线程用来定期回收内存。

  kswapd0 定义了三个内存阈值（watermark，也称为水位），专门衡量内存使用情况：**页最小阈值（pages_min）、页低阈值（pages_low）、页高阈值（pages_high）**。kswapd0 定期扫描内存的使用情况，并根据剩余内存落在这三个阈值的空间位置，进行内存的回收操作。

  - 剩余内存<**页最小阈值**pages_min: 可用内存耗尽，只有内核才可以分配内存。
  - **页最小阈值**pages_min<剩余内存<**页低阈值**pages_low: 内存压力较大，剩余内存不多了。这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值为止。
  - **页低阈值**<剩余内存<**页高阈值**pages_high:内存有一定压力，但还可以满足新内存请求。
  - 剩余内存>**页高阈值**:剩余内存比较多，没有内存压力。

  **一旦剩余内存小于页低阈值，就会触发内存的回收**。

  /proc/sys/vm/min_free_kbytes 来间接设置。min_free_kbytes 设置了页最小阈值，而其他两个阈值，都是根据页最小阈值计算生成的，计算方法如下 ：

* 页低阈值pages_min查看&设置：cat /proc/sys/vm/min_free_kbytes
  * pages_low = pages_min *1.25
  * pages_high = pages_min*1.5

### swap 开启和关闭

* 查看已开启的swap 分区：swapon
* 关闭swap分区：sudo swapoff -a 【free -h 确认已关闭】

**从头开始创建分区**

* 创建swap文件：fallocate -l 8G /mnt/swapfile
* 修改权限仅根用户可访问：chmod 600 /mnt/swapfile
* 配置swap文件：mkswap /mnt/swapfile
* 开启swap分区：sudo swapon  /mnt/swapfile

### NUMA & Swap

​        处理器的NUMA（Non-Uniform Memory Access）架构导致剩余内存很多的情况还会发生Swap。

* 在 NUMA 架构下，多个处理器被划分到不同 Node 上，且每个 Node 都拥有自己的本地内存空间。同一 Node 内部的内存空间，又可进一步分为不同的内存域：直接内存访问区（DMA）、普通内存区（NORMAL）、伪内存区（MOVABLE）等。【在分析内存时，应针对每个Node单独分析】

  * **numactl**：查看处理器在Node分布情况&每个Node内存使用情况。

    ```
    $ numactl --hardware
    available: 1 nodes (0) # 1个node
    node 0 cpus: 0 1 # node 0 上包含的cpu
    node 0 size: 7977MB #node 0 的内存大小
    node 0 free: 4416MB # node 0 空闲的内存大小
    ```

  * **cat /proc/zoneinfo**

    * pages 处：min、low、high 为上面提到的3个内存阈值；free 为剩余页数
    * nr_zone_active/inactive_anon：活跃/非活跃匿名页数
    * nr_zone_active/inactive_file: 活跃和非活跃文件页数

  * 某 Node 内存不足时，系统可从其他 Node 寻找空闲内存，也可从本地内存中回收。具体模式通过： /proc/sys/vm/zone_reclaim_mode 来调整。它支持以下几个选项：
    * 默认的 0 ，也就是刚刚提到的模式，表示既可以从其他 Node 寻找空闲内存，也可以从本地回收内存。
    * 1、2、4 都表示只回收本地内存，2 表示可以回写脏数据回收内存，4 表示可以用 Swap 方式回收内存。

### swappiness

* /proc/sys/vm/swappiness 选项，调整使用 Swap 的积极程度。

* swappiness 范围是：0-100。【调整swap的积极度权重而非百分比】
  * 数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；
  * 数值越小，越消极使用 Swap，也就是更倾向于回收文件页。

## 小结

| 内存指标                         | 性能工具                    |
| -------------------------------- | --------------------------- |
| 系统已用、可用、剩余内存         | free、vmstat、/proc/meminfo |
| 进程内存分布                     | pmap                        |
| 进程虚拟内存、常驻内存、共享内存 | ps、top                     |
| 进程swap换出内存                 | top、/proc/pid/status       |
| 缓存/缓冲区用量                  | free、vmstat、cachestat     |
| 缓存/缓冲区命中率                | cachetop                    |
| swap 换入换出                    | vmstat                      |
| 内存泄漏检测                     | memleak                     |

# IO

## 文件系统

* 对存储设备上的文件进行组织管理的机制，组织方式不同，就会形成不同的文件系统。
* **Linux 中一切皆文件。不仅普通的文件和目录，就连块设备、套接字、管道等，也都要通过统一的文件系统来管理。**
* 磁盘为系统提供了最基本的持久化存储。
* 文件系统则在磁盘的基础上，提供了一个用来管理文件的树状结构。

**索引节点和目录项**

Linux 文件系统为每个文件分配两个数据结构，**索引节点（index node）和目录项（directory entry**）。用来记录文件的元信息和目录结构。

- **索引节点**：记录文件的元数据（inode 编号、文件大小、访问权限、修改日期、数据的位置等）。**索引节点和文件一一对应，跟文件内容一样，会被持久化存储到磁盘中，<u>所以索引节点同样占用磁盘空间</u>。**
- **目录项**：记录文件的名字、索引节点指针以及与其他目录项的关联关系。**多个关联的目录项，就构成了文件系统的目录结构**。不同于索引节点，目录项是由内核维护的一个内存数据结构，也叫<u>目录项缓存</u>。

* **索引节点是每个文件的唯一标志；目录项维护的是文件系统的树状结构**。

* 一个索引节点可以对应多个目录项（文件一个别名=一个目录项）

**文件数据的存储**

* 磁盘读写的最小单位是扇区（512B）每次都读写这么小的单位，效率很低。
* 文件系统把连续的扇区组成了逻辑块，**每次都以逻辑块为最小单元来管理数据**（常见的逻辑块大小为 4KB，也就是连续的 8 个扇区）。

**虚拟文件系统**

* 文件系统的四大基本要素：目录项、索引节点、逻辑块（4个连续扇区，用于管理数据）、超级块（存储整个文件系统的状态）
* 虚拟文件系统VFS（virtual file system）：为支持各种不同文件系统，linux内核在用户进程和文件系统中间引入的抽象层。VFS 定义了一组所有文件系统都支持的数据结构和标准接口。【用户进程和内核中的其他子系统，**只需与 VFS 提供的统一接口进行交互**，而不需再关心底层文件系统的实现细节。】

​       PS: **底层文件系统，先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。**

文件系统挂载到挂载点后，就能通过挂载点，再去访问它管理的文件。VFS 提供了一组标准的文件访问接口。这些接口以系统调用的方式，提供给应用程序使用。

### 文件系统 I/O

文件读写方式差异，导致I/O分类多种多样，最常见的是以下几种I/O:

**是否利用标准库缓存**

* 缓冲I/O: **利用标准库缓存来加速文件访问**，标准库内部在通过系统调度访问文件。

* 非缓冲I/O: 直接通过系统调用来访问文件，**不通过标准库缓存**。

  PS: 很多代码遇到换行时才真正输出，换行前的内容都是被标准库暂时缓存起来的。

  PPS: 系统调用后，还会通过页缓存减少磁盘I/O。

**是否利用页缓存**

* 直接 I/O：**跳过页缓存**，直接跟文件系统交互来访问文件。
* 非直接 I/O：**文件读写先经过页缓存**，再由内核或额外的系统调用，与磁盘交互。

​       PS: 在数据库等场景中，还会看到**跳过文件系统，直接读写磁盘的情况**【裸I/O】

**应用程序是否阻塞自身运行**

- 阻塞 I/O: 程序I/O 操作后，未获得响应前，会阻塞当前线程。
- 非阻塞 I/O：程序I/O 操作后，不阻塞当前的线程，通过轮询或事件通知的形式，获取调用的结果。

## 磁盘

持久化存储设备，根据存储介质不同，多分为两类：机械磁盘&固态磁盘。

* 机械磁盘（硬盘驱动器Hard Disk Driver，HDD)。组成：盘片+读写磁头组成。**数据存储在盘片的环状磁道中**。读写数据前，需要**移动读写磁头到数据所在的磁道，才能访问数据**。
  * 若 **I/O 请求连续**【连续I/O】，不需要磁道寻址，可获得最佳性能。
  * **随机 I/O**: 需要不停地移动磁头，来定位数据位置，读写速度较慢。

* 固态磁盘（Solid State Disk,SSD）。组成：固态电子元器件。**不需要磁道寻址->不管是连续 I/O，还是随机 I/O 的性能，都比机械磁盘要好得多**。
* 相同磁盘：连续I/O性能>随机I/O
  * HDD 需要寻址；**SSD 存在“先擦除再写入”的限制**。<u>随机读写会导致大量的垃圾回收</u>，所以相对应的，随机 I/O 的性能比起连续 I/O 来，也还是差了很多。
  * 连续 I/O： 可通过预读的方式，来减少 I/O 请求的次数，这也是其性能优异的一个原因。很多性能优化的方案，也都会从这个角度出发，来优化 I/O 性能。

机械磁盘和固态磁盘还分别有一个最小的读写单位。

- 机械磁盘的最小读写单位是扇区，一般大小为 512 字节。【每次读写512KB，效率较低；文件系统会将连续的扇区或页组成逻辑块，并以逻辑块作为最小单位来管理数据。8个扇区=1页=4k=逻辑块】
- 而固态磁盘的最小读写单位是页，通常大小是 4KB、8KB 等。

磁盘接入服务器后，**按照不同的使用方式**，可分为多种不同的架构：

* 直接作为独立磁盘设备来使用。这些磁盘，还会根据需要，划分为不同的逻辑分区，每个分区用数字编号。比如我们前面多次用到的 /dev/sda ，还可以分成两个分区 /dev/sda1 和 /dev/sda2。

* **RAID（Redundant Array of Independent Disks）**：多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列，提高数据访问的性能，增强数据存储的可靠性。一般可以划分为多个级别，如 RAID0（最优的读写性能，但不提供数据冗余）、RAID1、RAID5、RAID10 等。

* 磁盘组合成一个网络存储集群，再通过 NFS、SMB、iSCSI 等网络存储协议，暴露给服务器使用。

**通用块层（抽象块设备层）**

​        磁盘在linux作为块设备来管理，有给定的设备号。Linux 通过统一的通用块层，管理各种不同的块设备。主要功能：

- 为文件系统和应用程序，提供访问块设备的标准接口
- 把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序。
- **将文件系统和应用程序发来的 I/O 请求排队，通过重新排序、请求合并等方式提高磁盘读写的效率**。

**I/O调度（I/O请求排序过程）**

Linux 内核支持四种 I/O 调度算法：

* NONE ：不做任何处理，常用在虚拟机中（此时磁盘 I/O 调度完全由物理机负责）。

* NOOP：最简单的I/O 调度算法。一个先入先出的队列，只做最基本的请求合并，常用于 SSD 磁盘。

* CFQ（Completely Fair Scheduler，完全公平调度器），默认 I/O 调度器：为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。【还支持进程 I/O 的优先级调度，适用于运行大量进程的系统，像是桌面环境、多媒体应用等。

* DeadLine 调度算法：为读、写请求创建了不同的 I/O 队列，提高机械磁盘的吞吐量，确保达到deadline的请求被优先处理。多用在 I/O 压力比较重的场景，比如数据库等。

**I/O栈 **

Linux 存储系统的 I/O 栈，由上到下可分为三个层次：文件系统层、通用块层、设备层。

* 文件系统层：虚拟文件系统和其他各种文件系统的具体实现。为上层的应用程序，提供标准的文件访问接口，并通过通用块层，来存储和管理磁盘数据。
* 通用块层：包括块设备 I/O 队列和 I/O 调度器。**对文件系统的 I/O 请求进行排队，重新排序和请求合并后发送给下一级的设备层。**
* 设备层：包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作。

存储系统的 I/O ，通常是整个系统中最慢的一环。【多使用缓存机制来优化 I/O 效率。】

### 磁盘性能指标

下述为常用5个判断指标，**“不要孤立比较某个指标：结合读写比例，I/O类型（随机or连续）进行判断”**

- 使用率：磁盘处理 I/O 的时间百分比，过高的使用率，通常意味着磁盘 I/O 存在性能瓶颈。
  * 仅考虑有无I/O，不考虑I/O 大小，即使使用率100%仍然可能接收新的I/O请求。
- 饱和度：磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。
- IOPS（I/O Per Second）：每秒的 I/O 请求数。
- 吞吐量：每秒的 I/O 请求大小。
- 响应时间：I/O 请求从发出到收到响应的间隔时间。

**磁盘I/O观测**

* 每块磁盘使用情况：iostat -d -x 【每列具体意义自行搜索】
  * r/s+w/s = IOPS
  * rkB/s+wkB/s = 吞吐量
  * r_await+w_await  = 响应时间
  * %util = 磁盘I/O使用率

**进程I/O观测**

* pidstat：pidstat -d 【所有进程的I/O情况，可以进一步|grep pid】
* iotop：类似于top，对I/O进程进行排序，找到I/O较大的进程。

filetop：基于linux内核eBPF跟踪内核中文件的读写情况。【bcc包中的一个工具】

## 小结

指标2linux工具

| 性能指标                                             | 工具                                |
| ---------------------------------------------------- | ----------------------------------- |
| 文件系统空间容量、使用量、剩余空间                   | df                                  |
| 索引节点容量、使用量、剩余量                         | df -i                               |
| 页缓存&可回收slab缓存                                | /proc/meminfo\| sar -r \|vmstat     |
| 缓冲区                                               | 同上                                |
| 目录项\|索引节点\|文件系统缓存                       | /proc/slabinfo \| slabtop（更直观） |
| 磁盘I/O使用率、IOPS、吞吐量、响应时间、I/O平均大小等 | iostat -d -x\|sar -d                |
| 进程I/O大小及延迟                                    | pidstat -d \| iotop                 |
| 块设备I/O事件跟踪                                    | blktrace -d /dev/sda                |
| 进程I/O系统调用跟踪                                  | strace                              |
| 进程块设备I/O大小跟踪                                | biotop                              |

# 其他

## 本地搭建DNS解析服务器

* [ubuntu20.04安装地址](https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/20.04/)：下载ubuntu-20.04.6-desktop-amd64.iso

* [虚拟机安装ubuntu系统步骤](https://blog.csdn.net/fr16021028/article/details/125888815)，ubuntu虚拟机安装好后，命令行窗口执行如下步骤安装bind9

* [centos 搭建DNS正向解析服务器](https://blog.csdn.net/qq_40707090/article/details/123561997)：在[centos7.9在虚拟机安装完成之后进行]()

  [centos7.9阿里云下载](http://mirrors.aliyun.com/centos/7.9.2009/isos/x86_64/)

### 安装bind9

```shell
# centos 7.9
# 首先DNS服务器端下载bind，正常情况下应该出现“完毕！”
yum install -y bind bind-utils
# 查看配置文件位置
rpm -qc bind
# 执行上述命令找到需要修改的3个配置文件： 
#/etc/named.conf       /etc/named.rfc1912.zones  /etc/named/named.localhost
```

```shell
# ubuntu 20.04
apt-get update
apt-get install bind9
cd /etc/bind # 若目录出现则表示安装成功
```

### 配置文件修改

/etc/bind目录下包含以下配置文件

* /etc/bind/named.conf: 主配置文件，包含bind服务器的全局设置和引用其他配置文件的命令；
* /etc/bind/named.conf.options: 包含bind服务器的全局选项设置，如监听地址、转发器等；
* /etc/bind/named.conf.local: 用于配置本地区域(zone) 和其他定制区域的文件；
* /etc/bind/named.conf.default-zones: 定义了一些默认区域zone，如localhost、反向解析等；

**配置文件修改-ubuntu**

```shell
# Ubuntu
修改bind 目录读写权限：chmod -R 777 /etc/bind，方便改写named.conf 和创建.com.zone 文件
创建日志输出目录：mkdir  /var/log/named
打开读写权限：chmod -R 777 /var/log
# 新建zone文件
vim /etc/bind/testdoh.com.zone
# 配置为以下内容，下方内容中的testdoh.com 需要与上方的文件名称一致
$TTL 10M
@   IN    SOA    ns1.testdoh.com    admin.testdoh.com. (
        0       ; serial
        1D      ; refresh
        1H      ; retry
        1W      ; expire
        3H )    ; minimum

@    IN    NS   ns1.testdoh.com.
; 这里ns1主机的ip地址可以换成本机地址
ns1        A    47.241.107.35
; 这里www主机的ip地址可以换成本机地址
www        A    47.241.107.35
```

```shell
# 编辑/etc/named.conf文件

```

**配置文件修改-centos**

```shell
# 编辑/etc/named.conf 文件
options {
        listen-on port 53 { 127.0.0.1; }; // 这里53为监听端口*******
        listen-on-v6 port 53 { ::1; };
        directory       "/var/named";
        dump-file       "/var/named/data/cache_dump.db";
        statistics-file "/var/named/data/named_stats.txt";
        memstatistics-file "/var/named/data/named_mem_stats.txt";
        secroots-file   "/var/named/data/named.secroots";
        recursing-file  "/var/named/data/named.recursing";
        allow-query     { localhost; }; // 允许查询:localhost; 修改为any;*****
```

```shell
# 编辑/etc/named.rfc1912.zones
zone "localhost.localdomain" IN {
        type master;
        file "named.localhost";
        allow-update { none; };
};
# 在文件中仿照上述给定的例子改写，一定要加上 file后面的文件名，后面需要新建这个文件，示例如下：
zone "yyh.com" IN {  # 此处为测试域名*********
        type master;
        file "yyh.com.zone" # 此处为测试域名的根区文件，后续需要创建
}
```

```shell
# 进入/var/named 目录创建yyh.com.zone 文件-上面一步中的测试域名的根区文件
cd /var/named
cp -p named.localhost yyh.com.zone # 复制文件保留格式，仅修改名称

```



## 动态追踪技术

**在不改变内核和应用程序代码的基础上，通过探针机制，采集内核或者应用程序的运行信息，分析、定位想要排查的问题。**

* 以往进行代码分析，大多需要增加中断、日志并逐步调试，影响程序正常运行，且可能存在复现困难的情况。
* 动态追踪技术既不需要停止服务，也不需要修改应用程序的代码；所有一切还按照原来的方式正常运行时，就可以帮你分析出问题的根源。
* Solaris 系统 DTrace：动态追踪技术鼻祖。【跟踪用户态和内核态所有事件&通过优化措施减小性能开销。】
* DTrace on linux：SystemTap（RedHat）

**动态追踪的事件源**

* 静态探针：**事先在代码中定义好，并编译到应用程序或内核中的探针**。探针仅在开启探测功能时才会被执行到，未开启时不会被执行。常见类型：跟踪点(tracepoint)、USDT(Userland Statistically Defined Tracing)
  * 跟踪点（tracepoints）:源码中插入的一些带有控制条件的探测点，这些探测点允许事后再添加处理函数。比如在内核中，最常见的静态跟踪方法就是 printk，即输出日志。Linux 内核定义了大量的跟踪点，可以通过内核编译选项，来开启或者关闭。
  * USDT 探针:用户级静态定义跟踪，在源码中插入 DTRACE_PROBE() 代码，并编译到应用程序中。不过，也有很多应用程序内置了 USDT 探针，比如 MySQL、PostgreSQL 等。
* 动态探针：**没有实现在代码中定义，运行时动态添加的探针**。eg：函数调用返回等。常见：kprobes、uprobes。
  - kprobes 用来跟踪内核态的函数，包括用于函数调用的 kprobe 和用于函数返回的 kretprobe。
  - uprobes 用来跟踪用户态的函数，包括用于函数调用的 uprobe 和用于函数返回的 uretprobe。
* 硬件事件：通常由性能监控计数器PMC（performance  monitoring counter）产生--包括各种硬件性能情况。

### linux 动态追踪机制

* ftrace：最早用于函数跟踪，后来扩展支持各种事件跟踪功能。通过debugfs 以普通文件形式，向用户空间提供访问接口。挂载目录:/sys/kernel/debug/tracing
* perf: perf record记录静态时间or perf probe 定义动态事件探针。
* eBPF：BPF扩展而来，类似DTrace，C扩展。

## 应用监控

* **首先确定需要监控哪些指标**（不再是资源使用情况，而是请求数、错误率和响应时间）

* 应用进程资源使用、相互调用、内部代码运行等情况。（迅速定位到调用链、应用逻辑等相关问题）

* 开源工具构建全链路跟踪系统（Zipkin、Jaeger、Pinpoint）

* 日志监控：除性能指标监控外，还需对输入输出上下文进行监控--日志。经典方法： ELK （Elasticsearch、Logstash 和 Kibana）。

## 系统监控

* 使用率：系统资源用于服务的时间或容量百分比。
* 饱和度：系统资源请求的繁忙度。
* 错误数：发生错误的事件个数，错误越多，系统越严重。

基于上述3个指标建立监控系统，之后根据监控到的状态，分析定位瓶颈来源并告警。【开源系统:Prometheus、Zabbix、Nagios】

## epoll 机制

 Epoll 是linux内核为了处理大批量文件描述而作了改进的poll，是linux下多路复用IO接口select/poll的增强版本，能够显著提高程序在高并发连接中只有少量活跃的情况下的系统CPU利用率。

* epoll的优点：
  * 支持一个进程打开大数目的socket描述。Select的一个进程打开的FD由FD_SETSIZE来设定，epoll没有这个限制，它所支持的FD上限是最大可打开文件数目，远大于2048.
  * IO效率不随FD数目增加而线性下降：由于epoll只会对“活跃”的socket进行操作，只有“活跃”的socket才会主动调用callback函数，其他idle状态的socket则不会。
  * mmap加速内核与用户空间的消息传递。Epoll 是通过内核与用户空间mmap同存一块内存而实现的。

## libev 机制

* 提供指定文件描述符合事件发生时调用回调函数的机制。Libev是一个事件循环器：向libev

* 注册事件，如socket可读事件，libev会对所注册的事件的源进行管理，并在事件发生时触发相应程序。

* gevent 方法spawn了一些jobs，然后通过gevent.joinall 将jobs加入到微线程的执行队列中等待其完成，设置超时时间为2s。执行后的结果通过检查gevent.greenlet.value 值来收集。
* Monket patching 的 python环境允许我们在运行时修改大部分对象，包括模块、类甚至函数。虽然这样做会产生“隐式副作用”，而且出现问题很难调试，但在需要修改python本身的基础行为时，monkey patching 就排上用场了。
  * 通过monkey.patch_socket()方法，urllib2模块可以在多微线程环境，达到与gevent共同工作的目的。
  * 事件循环：gevent和eventlet 类似，在一个greenlet中隐式循环。没有必须调用run() 或者dispatch() 的反应器（reactor），在wtisted中是有reactor的。当gevent的API函数想阻塞时，它获得Hub实例，并进行切换。
  * 提供的事件循环默认使用系统最快轮询机制，设置LIBEV_FLAGS环境变量可指定轮询机制。LIBEV_FLAGS =1 为select，2为poll，4为epoll，8为kqueue。

* Libev的API位于gevent,core下。注意libevAPI的回调在hub的greenlet上运行，因此使用同步greenlet的API，可以使用spawn() 和 event.set() 等异步API.

## C10k问题—操作系统处理高并发请求的问题

资源有限or代码设计不够良好的程序如何支持1w的并发请求。

* 资源上(2GB 内存和千兆网卡的服务器): 同时处理 1w 个请求，只要每个请求处理占用不到 200KB（2GB/10000）的内存和 100Kbit （1000Mbit/10000）的网络带宽就可以。

* I/O 的模型: 在 C10K 以前，Linux 中网络处理都用同步阻塞的方式，也就是每个请求分配一个进程或者线程。请求数较大时，进程或线程的调度、上下文切换乃至它们占用的内存，都会成为瓶颈。

* problem raiser：Dan Kegel，1999。

* 技术层面定义c10k问题: 设计不够良好的程序，其性能和连接数及机器的性能关系往往是非线性的。

  Eg：基于selector的程序在旧的服务器上能处理1000并发的吞吐量，但在2倍性能服务器上无法处理2000并发的吞吐量。策略不当时，大量操作的消耗和当前连接数n线性相关（单个任务资源消耗和当前连接数的关系是O（n））当服务程序对数以万计socket进行IO处理，资源消耗极大。

* 问题本质: 传统阻塞IO模型处理方式是requests per second，并发10k和100区别在于CPU 【进程/线程多了，数据拷贝频繁、进程线程上下文切换消耗大，导致系统崩溃。】

  解决问题的关键：减少CPU等核心计算资源消耗，从而榨干单台服务器的性能。

* 可能的解决方案: 每个进程、线程同时处理多个连接（IO多路复用）
  * **循环挨个处理连接**：每个连接配一个socket，当某个socket的数据不ready的时候，整个应用阻塞等待。【DNS就是这个问题】
  * **select**： 在读取句柄之前，先查看确认状态，ready才进行处理。
    * 连接数量较小逐个检查句柄可接受，大规模逐个检查很慢。【selector 往往存在管理句柄上限。】
  * **poll**： 通过pollfd 数组向内核传递需要关注的事件消除句柄上限，同时使用不同字段分别标注事件和发生事件，来避免重复初始化。
  * **epoll** ：解决逐个排查所有文件句柄状态效率不高的问题。
    * 使用红黑树，在内核管理文件描述符的集合，就不需要每次操作传入传出这个集合。
    * 调用返回时，只提供状态变化的文件句柄，能提高排查效率。（当文件句柄数量达到10k的时候，epoll已经超过select 和 poll 两个数量级。）

## 文件传输性能提升-零拷贝

传输文件时内存拷贝次数减少。可以直接跳过step2，直接到socket缓冲区。

* step1：数据从磁盘文件拷入内核态。
* step2：内核态拷入用户缓冲区。【不是必须】
* step3：用户缓冲区发到socket缓冲区。
* step4：socket缓冲区发送到网卡进行传输。

零拷贝：如果网卡支持SG-DMA（The Scatter-Gather Direct Memory Access）技术，还可以去掉socket 缓冲区，直接从内核态pageCache 发送到网卡。【拷贝减少到2次】

* 减少拷贝次数 & 降低上下文切换次数 & 最大化利用socket缓冲区内存。

## 拥塞控制

Google 提出的BBR拥塞控制算法已经应用在高版本的linux内核中，在高性能站点上网络延迟有20%降低，传输带宽也有提高。【linux允许我们调整拥塞控制算法，但需要正确的设置参数。】

* 慢启动阶段（TCP连接刚建立时的慢启动阶段）：由于TCP连接会穿越许多网络，最初并不知道网络的传输能力，为避免发送超过网络负载的报文，TCP先调低发送窗口。
  * 慢启动节点会以指数级扩大拥塞窗口(每收到一个ACK报文就增加一个MSS)
  * 慢启动阶段结束：
    * 通过定时器明确探测到了丢包。
    * 拥塞窗口的增长达到了慢启动阈值ssthresh(slow start threshold)。
    * 接收到重复的ACK报文-可能存在丢包。
* 窗口的计量单位是字节，但通常用MSS作为描述窗口大小的单位。【MSS是TCP报文的最大长度】
* 进行拥塞控制的最佳时间点：设备缓冲队列刚出现积压的时刻【**拥塞信号：网络延迟增高、带宽维持不变**】

## 对称加密算法

* 首选AES（Advanced Encryption Standard）：AES只支持3种不同的密钥长度，128位、192位、256位，安全性一次提高，运算时间也依次加长。
* 密钥越长，安全性提升很多，可以弥补性能的不足。
* AES（如果CPU支持AES-NI特性）>CHACHA20（ARX-add rotator xor,cpu 执行更快）










------


