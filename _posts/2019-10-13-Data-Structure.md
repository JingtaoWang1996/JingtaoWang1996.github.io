---
title: 'Data-Structure-notes'
date: 2019-10-13
permalink: /posts/2019/10/Data-Structure-notes/
tags:
  - Theoretical Ground
---

Some data structure related notes.



# Basic take-ways

* 数据结构为算法服务 & 算法要作用在特定的数据结构上。
* 最重要概念-复杂度分析。

PS: 各种类型的数据都可能存在越界的问题，此处以整型为例：

```
# if a,b,c are positive,
a<(b+c) # 此处可能存在b+c超出范围的情况，均为负的话，也有类似的情况。
```

* **参数合法性检验**：对于一个函数，内部产生的数据，可以信任合法性；**外部传入的数据，需要先检验，再使用**（包括类型检验、安全性检验、取值范围检验等）。

# 复杂度分析

不用具体的测试数据来测试，粗略估计执行效率的方法。

* 越高阶复杂度的算法，执行效率越低，
* 从低到高阶：O(1)、O(logn)、O(n)、O(nlogn)、O(n^2)

时间复杂度

**大O复杂度表示法：T(n)=O(f(n))** --- 代码执行时间与代码中表达式成正比

* **大O**-时间复杂度:不具体表示代码真正执行时间，而是**代码执行时间随数据规模增长的变化趋势**，所以又叫**”渐进时间复杂度“**。
* **<u>n表示数据规模的大小</u>**：为了表示增长趋势，当n极大时，**常量、系数、低阶**三部分均忽略不计。

空间复杂度

* 全称：**渐进空间复杂度（asymptotic space complexity**）。
* 表示**<u>算法的存储空间与数据规模之间的增长关系。</u>**

## 时间复杂度分析

常见分析方法：

* **只关注循环执行次数最多的一段代码**：这段代码执行次数的n的量级，就是整段代码的复杂度。
  * 只包含一个执行n次的循环，则：O(n)
  * 一个执行n次的循环当中还包含一个执行n次的循环，则：O(n^2)
* **加法法则：总复杂度等于量级最大的那段代码的复杂度**
  * O(n)  和 O(n^2) 的总复杂度为O(n^2)
* **乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度乘积**
  * 循环内外复杂度乘积

几种常见时间复杂度分析

| 最高系数         | 复杂度   |
| ---------------- | -------- |
| 常量阶[非多项式] | O(1)     |
| 对数阶           | O(logn)  |
| 线性阶           | O(n）    |
| 线性对数阶       | O(nlogn) |
| 指数阶           | O(2^n)   |
| 阶乘阶[非多项式] | O(n!)    |
| 次方阶           | O(n^k)   |

* 常量阶：代码执行时间不随数据规模增大而增大，时间复杂度为O(1)【无：循环、递归】

* 对数阶、线性对数阶：

  * 例子：i从1开始，每循环一次就*2，直到大于n---O(x=log2n)【2的n次方后大于】

    ```python
    i = 1
    while i<n:
       i = i*2
    ```

    PS: 不管以多少为底数，类似上述例子的时间复杂度都为：**O(logn)**

* O(m+n)  &  O(m*n): 代码复杂度由两个数据规模来决定

  * O(m+n)：代码块中包含两个并列的循环块，且**无法评价两个循环的数据量哪个大的情况下**，两个部分都不能省略，因此时间复杂度为O(m+n)
  * 乘法法则仍然有效。

### 最好、最坏情况时间复杂度

* Best case time complexity：最理想情况下执行这段代码的时间复杂度。
  * eg：在list中遍历查找变量，找到后退出；最好的情况，第一个就是要查找的，时间为O(1)
* Worst case time complexity：最坏情况下执行这段代码的时间复杂度。
  * eg：在list中遍历查找变量，找到后退出；最坏的情况，最后一个才是要查找的，时间为O(n)

### 平均情况时间复杂度

* Average case time complexity：

  * eg: 【在list中遍历查找变量，找到后退出--一共n+1中情况=n个位置+不存在】

    将每种情况下需要遍历的元素个数累加然后除以所有的可能，就得到需要遍历的元素平均值。
    $$
    ((1+n)*n/2+n)÷(n+1)=n(n+3)/2(n+1)
    $$
    去掉常数、系数、低阶后：O(n)；这种计算方式存在的问题是：n+1种情况出现的概率不同。

* 一般情况下考虑最好最坏的极端情况即可，计算平均时间复杂度需要考虑不同情况的出现概率。

### 均摊时间复杂度

* 使用情况更加特殊：操作之间存在前后连贯的时序关系时，可以将这一组操作放在一起，看是否能将较高时间复杂度的操作耗时平摊到其他操作上。

* 一种特殊的平均情况时间复杂度。


## 空间复杂度分析

* 常见的空间复杂度： O(1)、O(n)、O(n2 )，像 O(logn)、O(nlogn) 对数阶复杂度平时都用不到。
* 空间复杂度计算：申请一个list，根据代码（eg：循环中不断向list存入数据）数据量扩大可能导致的不同量级的空间复杂度。

# 数组Array

* 定义：**线性表**数据结构，用一组**连续的内存空间**，来存储一组具有**相同类型的数据**。

  * 线性表：数据排列成像一条线一样的结构，每个线性表上最多只有前后两个方向。【数组、链表、队列、栈也是线性表结构】
  * 非线性表：数组不是简单的前后关系【二叉树、堆、图等】
  * **连续的内存空间&相同类型的数据**：有这两个条件能确保随机访问。但同时导致若想要在数组中删除、插入一个数据，需要做大量数据迁移【为保证数据连续性】

* 数组实现下标随机访问数组元素--连续内存空间&相同类型数据为前提

  * 每个数组元素对应的内存地址计算公式

    ​           a[i]_address = base_address + i * data_type_size

    PS: 已知基地址数组开头元素的内存地址，要求第几个元素，数据类型对应的data_type_size

* 数组和链表的区别

  * 数组适合查找，时间复杂度为O(logn)
  * 数组支持随机访问，根据下标随机访问的时间复杂度为O(1)

## 安全问题



## 低效的插入和删除 & 改进方法

插入（长度为n的数组，将元素插入到第k个位置）

* 有序的情况下：k-n的所有元素往后移动一位，时间复杂度：末尾O(1),开头O(n),平均O(n)
* 无序：为了避免数据大规模搬动，直接将原来第k位的元素放到最后，然后插入新的，O(1)。

删除（长度为n的数组，删除第k个位置元素）

* k-n的所有元素往前移动一位，时间复杂度：末尾O(1),开头O(n),平均O(n)
* **将多次删除操作集中到一起执行，可以减少数据搬动：记录所有需要执行的删除操作后，最终统一搬动数据。------JVM标记清除垃圾回收的核心思想**

## 数组访问越界的问题

* 下述述代码中，数组大小为3，但循环中访问了a[3], 在C语言中，只要不是受限制内存，所有内存都可自由访问，根据基地址计算公式，访问到另一个不属于数组的内存地址。这个地址正好是i的内存变量地址，因此a[3]=0 相当于i=0，导致代码无限循环

```c
int main(int argc, char* argv[]){
    int i = 0;
    int arr[3] = {0}; // 数组大小为3,a[0]-a[2]
    for(; i<=3; i++){
        arr[i] = 0;
        printf("hello world\n");
    }
    return 0;
}
```

PS: C中，数组越界没有明确规定编译器如何处理，可能导致逻辑错误&代码攻击等等。

PPS: Java 中本身就会做数据越界检查：**java.lang.ArrayIndexOutOfBoundsException**

## 容器能否完全替代数组

针对数组类型，很多语言都提供了容器类，以Java为例：ArrayList

ArrayList VS 数组

* ArrayList

  * 封装数组操作细节 
  * **支持动态扩容**【每次空间不够，都会自动扩容为1.5倍】。

  * 但无法定义存储基本类型，eg：int->Integer; long->Long【特别关注性能的情况，可以考虑数组】

* 数组--部分语言中数组需要预先指定大小。

  PS: **扩容涉及的内存申请&数据搬迁较多，最好在创建的之初就能够大致估计一个范围。**

* 为什么数组要从0开始编号而不是从1开始？

​      从0开始编号，能够直接明确第k个元素的地址就是：首地址+k*每个元素所占字节，避免从1开始多一个k-1的运算。【PS: C语言从0开始，之后java等仿照c也从0开始】

# 链表Linked list

通过地址指针将**零散的内存块**串联使用，申请的内存地址不必非要连续。

* 单链表：链表中每个节点包含两部分：该节点的数据data+下一个节点的内存地址next【链表尾节点的next=null空地址】
  * 插入、删除：O(1)【只需要修改前后节点next的值】--插入删除较多，优选链表
  * 随机访问：O(n)【必须按照地址顺序从头开始遍历】--随机访问较多，优选数组
* 循环链表：单链表尾节点next指向头结点
  * 方便从链表尾到链表头的访问过程：特别适合问题数据中具有环形结构特点的情况。
* 双向链表：链表中每个节点包含3部分：上一个节点的内存地址pre+该节点的数据data+下一个节点的内存地址next
  * 需要的存储空间增大，但支持双向遍历，一定情况下操作比单链表更加高效。
  * 删除操作时，已知上下节点的地址，无需在遍历过程保存上一个节点地址，简化遍历过程。
* **空间换时间**：当内存空间充足时，若追究代码执行速度，可选择空间复杂度高，但时间复杂度低的算法or数据结构，反之依然（时间换空间）。

## LRU缓存淘汰算法

Q:缓存大小有限的情况下，当缓存数据被用满的时候，哪些数据应该先被清理，哪些数据应该被保留？

A：根据淘汰策略来定：先进先出、最少使用策略、最近最少使用策略（Least Recently Used）

具体操作【以链表为例，也可以通过其他数据结构实现，主要是要一个**访问时间的先后顺序**】

* 维护一个**有序单链表，越靠近链表尾部的节点是越早之前访问的**，当新数据被访问时，从头遍历一次链表。
* 若在链表中找到对应数据，则将原数据从链表中删除，重新放入链表头【访问时间更新】
* 若没有在链表中：1）缓存未满，则新数据放入链表头 2）缓存已满：删除尾节点，数据放入头结点【缓存淘汰】

双向链表+散列表：

* O(1) 查找数据散列表，在遍历到链表尾部后访问最后一个数据，进行删除、添加等操作。

## 链表的操作

可以画图理好顺序后再具体实现

* 注意**指针的操作顺序**，防止链表丢失or内存泄漏
  * 删除链表节点时，需要手动释放内存空间，防止内存泄露。
* 链表的第一个和最后一个节点在具体进行插入和删除操作的时候，需要对特殊情况进行考虑（空链表的头结点，删除尾节点）
* **头节点**：不存储数据。
* 边界条件检查：**链表为空/只含一个节点/含两个节点/头尾节点处理逻辑/**

## 常见使用-快慢指针！！

* **单链表翻转**：
* 3个指针每次按照顺序修改原链表指针指向
  * 头插法：创建一个新链表，每次从要翻转的链表中取头结点插入到新链表的头结点位置。
  
* **链表中环检测**：快慢指针【快每次2步，慢每次一步-有环（快一定追上慢）；无环（快先到null）】

* 两个有序链表合并：按照顺序选择其中一个链表作为头结点，每次比较两个链表值后移动指针。

* **删除链表倒数第n个节点**：快慢指针【快指针先走n步，然后快慢一起每次一步直到快指针到结尾时删除慢指针对应的链表节点。】

* **求链表的中间节点**：快慢指针【快每次2步，慢每次一步-奇数：快到尾的时候，慢到中间；偶数：快到尾的时候，慢到中间&慢和慢的下一个都是中间】

# 栈 stack

* 浏览器页面前进后退场景【用两个栈来实现】：依次访问页面 a-b-c 之后【X栈存入abc】，点击浏览器的后退按钮，就可以查看之前浏览过的b 和 a。当你后退到 a【Y栈存bc】，点击前进，就可以重新查看 b 和 c。如果后退到 b 后，点击了新页面 d【清空Y栈，X栈留下abd】，那就无法再通过前进、后退功能查看c 了。

* 场景数据抽象特征：**后进先出，只允许在一端插入和删除数据**

* 栈未被数组&链表替代的原因：数组or链表暴露过多操作接口，操作灵活性高，但不可控性增大。

* 主要操作：出栈**pop()**（删除）&入栈**push()**（新增）

* 两种常见的栈：

  * **顺序栈**：数组实现的栈-数组尾部进行新增和删除操作（计算数组长度确认是否能够入\出栈)
  * **链式栈**：链表实现的栈-入栈（头插）&出栈（头节点next指向下一个orNull）
* **<u>入栈 & 出栈：时间复杂度---O(1)  & 空间复杂度---O(1)</u>**【额外使用的内存空间or时间步骤】
  * 动态扩容栈：通过数组动态扩容【链表实质可以一直增加，不存在扩容问题】

## 函数调用中

每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完毕之后，将这个函数对应的栈帧出栈。

## 表达式求值

* 创建两个栈：一个存数，另一个存运算符

* **从左向右遍历表达式，遇到数字，就压入操作数栈；遇到运算符就与操作运算符栈顶元素比较优先级：1）若比栈顶元素优先级高，则将当前元素压入栈；2）若优先级小于等于当前栈顶元素，则取运算符栈顶元素&操作数栈顶两个元素，计算后放入操作数，继续比较。最后在相同级别的运算符当中，两个元素一个符号出栈运算。**
* eg：3+5*8-6【按照上述思路手写一遍即可】

## 括号匹配

假设表达式中只包含三种括号，圆括号 ()、方括号 [] 和花括号{}，并且它们可以任意嵌套

* 表达式从左向右依次扫描：左括号则入栈，右括号则和栈顶取出一个左括号看是否匹配（匹配则继续扫描，直到全部表达式扫描完后判断栈是否为空）

# 队列Queue

* 操作受限的数据表结构，需要两个指针，一个指向队头head，一个指向队尾tail。
* 场景数据抽象特征：**先进先出**。
* 包含的两个操作：在队列两头分别操作。
  * **入队 enqueue()**：**队尾加元素**。
  * **出队 dequeue()**：**队头取元素**。
* **顺序/链式队列**:用数组/链表实现的队列。
* 当tail 到队尾时，再增加元素，可以将 head 到 tail 之间的数据，整体搬移到数组中 0 到 tail-head 的位置。出队操作的时间复杂度仍然是 O(1)，但入队操作的时间复杂度是 O(n)

使用场景：线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？

* 非阻塞的处理方式，直接拒绝任务请求。
* 阻塞的处理方式，通过队列将请求排队，等到有空闲线程时，取出排队的请求继续处理。

## 循环队列

用数组来实现队列的时候，在 tail==n 时，会有数据搬移操作，这样入队操作性能就会受到影响，使用循环队列（首尾相连的队列）能够避免数据搬移。【tail =n，再入队一个元素后tail = 1】

* **队空条件判定：head=tail**
* **队满条件判定：（tail+1）%n=head**【当队列满时，tail实际没有数据，因此循环队列会浪费一个存储空间】
* 循环队列长度需要对并发量有一定的预测，否则容易丢失请求。

## 阻塞和并发队列

* 阻塞队列：在队列为空的时候，从队头取数据会被阻塞；队列已经满了，那么插入数据的操作就会被阻塞。【consumer-producer model：队头消费者，队尾生产者】
* 并发队列：多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题。最简单直接的实现方式是直接在入队、出队的地方加锁or采用原子操作的方式。

# 递归 Recursion

## 场景：推荐注册返佣金

* 用户 A 推荐用户 B 来注册，用户 B 又推荐了用户 C 来注册。我们可以说，用户 C 的“最终推荐人”为用户 A，用户 B 的“最终推荐人”也为用户 A，而用户 A 没有“最终推荐人”。
* 基于上述映射关系，给定用户id，如何查找其“最终推荐人”

## 递归的三个条件

* 一个问题可以分解为几个子问题的解
* 分解后的子问题除了数据规模不同，求解思路完全一致
* 存在递归终止条件

**-------------------------写出递归公式 & 找到终止条件-------------------------------**

## 楼梯问题

假如有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，走这 n 个台阶有多少种走法？

* 递归公式：f(n) = f(n-1)+f(n-2)【到当前位置的走法等于到前1阶和前2阶走法之和】
* f(1) = 1, f(2)=2

## 警惕堆栈溢出

实际编写递归代码时，会遇到很多问题，比如堆栈溢出。堆栈溢出会造成系统性崩溃，后果非常严重。

* 递归代码造成堆栈溢出的原因

  函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大,如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。

* 如何预防堆栈溢出

  **限制递归调用的最大深度的方式来解决这个问题**。【超过一定深度后，不再递归，直接返回报错。】--- 最大允许递归深度可估计且较小时，可以使用这种方法，否则不实用（最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。实时计算，代码过于复杂。

## 警惕重复计算

在递归结构计算当中，可能出现某一个子问题的值在多处都需要用到，可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算。

PS: dp动态规划也是同样的思路。

# 排序

经典排序：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。

根据时间复杂度划分：

| 排序算法                                     | 时间复杂度 | 是否基于比较 |
| -------------------------------------------- | ---------- | ------------ |
| 冒泡、插入、选择**（使用优先级：插入>冒泡)** | O（n^2）   | yes          |
| 快排、归并                                   | O（nlogn） | yes          |
| 桶、计数、基数                               | O（n）     | no           |

## 算法分析

原地排序：特指空间复杂度为O(1)的算法【排序额外使用的内存空间位常量级】

**执行效率**

* 最好、最坏、平均情况的时间复杂度

  数据无序程度不同，时间复杂度不同。

* 时间复杂度的系数、常数、低阶

  实际开发中，数据规模较小时需要考虑上述三个情况。

* 比较次数和交换/移动次数

**稳定性**：若待排序元素中存在值相等的元素，经过排序后相等元素之间的先后顺序不变。

* 考虑稳定性的场景：电商交易系统中的“订单”排序。订单有两个属性【下单时间、订单金额】。10 万条订单数据，按照金额从小到大对数据排序。金额相同的订单，按照下单时间从早到晚有序。
  * method1：先按金额排序，再遍历排序后的数据，对金额相同的小区间再按照下单时间排序。
  * **method2【稳定排序】：先按时间排序。之后，按金额重新排序。两遍排序之后，得到的数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的[稳定排序确保排序前后顺序不变]。**

## 如何选择合适的排序算法

* 小规模数据：可以选择O(n^2)的算法
* 大规模数据：O(nlogn)

### 快排优化

* privot 区分点选择要合理，尽可能使两侧的数据总量相差不大。时间复杂度可以降为O(n)
* 限制递归深度，防止栈溢出。

## 冒泡 Bubble sort

* **每次只操作相邻的两个数据【相邻两个元素进行比较，看是否满足大小要求，若不满足就互换这两个元素】**
* 一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。

<img src='/images/img/一次冒泡.jpg'>

根据上图，经过1次冒泡之后，6已经到正确为止，因此，长度为6的数组只需6次冒泡即可完成排序。

<img src='/images/img/6次冒泡完成后.jpg'>

* **冒泡优化：**每次记录是否有数据交换，若某次没有数据交换，表示数据已经全排列，结束冒泡。

* **空间复杂度：O(1),只涉及相邻数据的交换操作，需要常量级的额外空间，原地排序算法。**
* 冒泡是稳定排序算法，因为相邻两个元素相等不做交换，保证了顺序。
* **时间复杂度：最好情况：O(n) 顺序已排好,一次冒泡即可；最坏情况：O(n^2) 完全倒序排列，需要n次冒泡操作；平均：O（n^2）次操作**。
  * **有序度**是数组中具有有序关系的元素对的个数。【当一个序列完全有序时成为满有序度】
  * 逆序度 = 满有序度-有序度

## 插入排序 Insertion Sort

* 将数组分为两个区间，**已排序和未排序区间**。初始已排序区间只有一个元素【数组第一个元素】
* <u>插入算法的核心思想：**取未排序区间中的元素，在已排序区间中找到合适的插入位置插入，并保证已排序区间数据一直有序**。重复这个过程，直到未排序区间中元素为空，算法结束。</u>

* 包含两种操作：比较（找到插入位置）、移动（插入位置元素后移一位，空出当前位置）
* 对于给定的初始序列，移动次数=逆序度
* **空间复杂度：O(1),不需要外部空间辅助，原地排序算法。**
* 插入是稳定排序算法，因为可以将后出现的元素插入相等元素的后面，保证了顺序。
* **时间复杂度：最好情况：O(n) 顺序已排好,逐个插入即可；最坏情况：O(n^2) 完全倒序排列，需要n次插入；平均：O（n^2）次操作**。

## 选择排序 Selection Sort

* 类似插入排序，区分**已排序和未排序区间**，每次从未排序区间找最小元素放入已排序的区间末尾。

  <img src='/images/img/选择排序.jpg'>

* **空间复杂度：O(1)，是一种原地排序算法**

* **时间复杂度：最好情况：O(n) 顺序已排好；最坏情况：O(n^2) 完全倒序排列，需要n次插入；平均：O（n^2）次操作**。

* 选择排序：不稳定排序算法，根据上图，每次找最小值，找法不同，前后顺序也不同。

## 归并排序 Merge Sort-O(nlogn)

* 如果要排序一个数组，先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。【适合大规模数据排序】

<img src='/images/img/归并排序.jpg'>

参见上图，将数组不停分为前后两部分的过程中逐渐排序，最后一层就是合并两个有序数组。

<img src='/images/img/归并排序合并示例.jpg'>

* 根据上图可以看出，归并在merge过程中优先选取前方数据即为稳定排序。
* **时间复杂度【递归代码】：O(nlogn)--任何情况，可根据递归公式进行推导**
* **空间间复杂度：O(n)--n个单位的临时空间用于进行归并**

## 快速排序Quick Sort-O(nlogn)

* 基本思想：排序数组中下标从 p 到 r 之间的一组数据
  * 选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。
  * **遍历** p 到 r 之间的数据，小于 pivot 的放左边，大于 pivot 的放右边，pivot 放中间。【遍历之后数据前后顺序不变，**稳定排序**】
  * 之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。
  * 在左右两个区间重复进行第二步，直到完成排序。【适合大规模数据排序】

<img src='/images/img/快排.jpg'>

快排示例如上图所示

* 涉及交换操作，分区之后相同值的先后顺序会改变，因此**快排不是一个稳定算法。**

* **<u>时间复杂度：O(nlogn)</u>--每次分区都基本在中间的情况下，极端情况每次都选在一边：时间复杂度O(n^2)。**
* **空间间复杂度：O(1)--可以考虑原地排序**

## 桶排序Bucket Sort

* <u>**核心思想：将要排序的数据分到几个有序的桶里，每个桶中的数据再单独进行排序，桶内序完之后，再把每个桶里的数据按照顺序依次取出，组成有序的序列。**</u>

```
原序列：22,5,11,41,45,26,29,10,7,8,30,27，42,43，40
桶1 0-9:5,7,8
桶2 10-19:10,11
桶3 20-29:22，26,27,29
桶4 30-39：30
桶5 40-49:40,41,42,43,45
每个桶中单独排完序之后再按顺序取出。
```

* **时间复杂度：O(n)，不涉及元素比较**

  * n 个待排序数据，均匀划分到 m 个桶内，每个桶里就有 k=n/m 个元素。

  * 每个桶内快排，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，整个桶排序时间复杂度就是 O(n*log(n/m))。
  * 当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。

* 桶排序对数据的要求：

  * **要排序数据需很容易划分成 m 个桶且桶与桶之间有天然大小顺序。**这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。
  * **数据在各桶之间的分布比较均匀。**【若数据经桶划分之后，有些桶数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。】

* **桶排序适合用在外部排序中**：

  * 外部排序【大量数据存储在外部磁盘中，因内存有限，无法全部加载到内存的情况。】

  示例：10GB 的订单数据，希望按订单金额（假设金额都是正整数）进行排序，但内存有限，无法一次性把 10GB 的数据都加载到内存中。

  * 先扫描一遍文件，看订单金额所处的数据范围。【假设扫描后，订单金额最小是 1 元，最大是 10 万元。将所有订单根据金额平均划分到 100 个桶里-每个桶范围1k。
  * 理想的情况下，若订单在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。所有文件都排好序之后，只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。

  * 实际情况，金额不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。可能某个金额区间的数据特别多，划分后对应的文件就会很大，没法一次性读入内存。对于这些大文件，可以继续进行划分，直到能够读入为止。

## 计数排序Counting Sort

* **核心思想：**要排序的 n 个数据所处的**范围并不大时**，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同值，省掉了桶内排序的时间【k个桶一定所有可取的单个值都包含在内】。
  * **示例：高考查分（通过成绩快速排序得出名次）** 考生成绩范围：0- 900 ，这个数据的范围很小，所以可以分成 901 个桶，对应分数 0 -900 。根据考生的成绩，将考生划分到这 901 个桶里。桶内的数据都是分数相同的考生，并不需要再排序。只需依次扫描每个桶，将桶内的考生依次输出到一个数组中【每一个数组长度即为计数值】，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。
  * 桶排序之后的数组依次输出到一个数组中，从前到后依次求和【dp】，此时每个数组元素中的值就是小于等于分数k的考生个数。
* **时间复杂度：O(n)，只遍历，不涉及元素比较**
* **桶排序的一种特殊情况**

## 基数排序Radix Sort

* 10w个手机号从小到大排序：从后到前的顺序对每一位排序，最后经过11次排序，所有手机号码都有顺序了。【从后到前排序的意义：保证了排序稳定性】
* 在数据长度不一致的情况下，可以通过加“0”补齐。**每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了**。

<img src='/images/img/基数排序.jpg'>



# 二分查找 Binary Search-O(logn)

最省内存的查找方法【Q： 1kw个8字节的整数数据，设计算法快速判断某个整数是否在这1kw个数据中，算法占用内存不超过100MB】

* **<u>使用条件</u>**：

  * **依赖顺序表结构，简单说就是数组.(不能使用链表，需要根据下标随机访问元素)**
  * **有序数据**
  * 有一定数据量，若数据量很小，则一次遍历即可；数据量过大，内存不支持。

* **时间复杂度：O(logn)**

  假设数据大小是 n，每次查找后数据缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止,则被查找区间的变化情况如下：

  ```
  n,n/2,n/4,n/8,...,n/(2^k)
  ```

  可以看出，这是一个等比数列。其中 n/(2^k)=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度就是 O(k)。通过n/(2^k)=1 ，可以求得 k=log2n，所以时间复杂度就是 O(logn)。

* 用二分查找可以解决的问题，用散列表、二叉树都可以解决。

## 对数时间复杂度O(logn)

* 极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 O(1) 的算法还要高效
  * 即使n特别大，logn也会很小
  * 常量级的O(1) 可能表示O(1000)>O(logn)

## 递归与非递归实现

* 最简单的情况：有序数组中不含重复元素

  【非递归】

  ```java
  public int bsearch(int[] a, int n, int value) {
    int low = 0; // 开头下标
    int high = n - 1; // 结尾下标
    // 
    while (low <= high) { // 相遇or相交则退出
      int mid = low + (high-low) / 2; // 计算中间值
      if (a[mid] == value) { // 找到
        return mid;
      } else if (a[mid] < value) { // 大于中间值，则更新low
        low = mid + 1;
      } else { // 小于中间值，则更新high
        high = mid - 1;
      }
    }
    return -1; // 没找到
  }
  ```

  【递归】

  ```java
  // 二分查找的递归实现
  public int bsearch(int[] a, int n, int val) {
    return bsearchInternally(a, 0, n - 1, val);
  }
   
  private int bsearchInternally(int[] a, int low, int high, int value) {
    if (low > high) return -1;
   
    int mid =  low + ((high - low) >> 1); // 位运算方式计算中间值
    if (a[mid] == value) {
      return mid;
    } else if (a[mid] < value) {
      return bsearchInternally(a, mid+1, high, value);
    } else {
      return bsearchInternally(a, low, mid-1, value);
    }
  }
  ```

* 查找第一个值等于给定值的元素

  更新：mid>value, high = mid -1;  mid< value low = mid +1；mid=value, 确定是否为第一个（mid=0 or mid -1 ！=value），若不是，则high = mid -1

* 查找最后一个值等于给定值的元素

  更新：mid>value, high = mid -1;  mid< value low = mid +1；mid=value, 确定是否为最后一个（mid=n or mid+1<n &mid +1 ！=value），若不是，则 low = mid +1

* 查找第一个值大于给定值的元素

  更新：mid<=value,low=mid+1;mid>value: (mid =0 or mid-1<=value) 否: high = mid -1

* 查找最后一个值小于于给定值的元素

  更新：mid>=value,high=mid-1;mid<value: (mid =n or mid+1>=value) 否: low = mid +1

# 跳表Skip list

链表改造后用于二分法查找的数据结构

* 动态数据结构：支持快速插入、删除、查找。
* Redis 中 Sorted Set 就是通过跳表实现。

对于单链表：查找 O(n)

**跳表：将链表增加索引层，减少遍历次数**

<img src='/images/img/跳表.jpg'>

如果要查某个结点，比如 16。可以先遍历索引层，当索引层中值为 13 ，下一结点是 17时，要查找的结点 16 肯定就在这两个结点之间。然后我们通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历 2 个结点，就可以找到值等于 16 的这个结点了。这样，原来如果要查找 16，需要遍历 10 个结点，现在只需要遍历 7 个结点。

* **并不是查询效率越高，提升越明显。**

* **查询任意数据的时间复杂度：O(logn)**
* **空间复杂度： O(n)**

高效的动态插入和删除

* 插入、删除操作的时间复杂度也：O(logn)。

# 散列表 Hash Table-O(1)

应用示例1: Word 文档中的单词拼写检查功能实现

* 定义：散列表，又叫<u>hash表</u>。**支持按照下标随机访问数据的特性，其实是数组的一种扩展，由数组演化而来。没有数组，就没有散列表。**
* **通过key以O(1**)来搜索value的值---{0：info，1：info，2：info，...}

## 散列函数

* hash(key):key是元素键值，hash(key)是计算完成的散列值。

* 散列函数计算得到的散列值是一个非负整数。
* 要想找到一个不同key对应hash值不同的散列函数几乎不可能 & 数组存储空间有限也会增大散列冲突的概率。

## 散列冲突

**再好的散列函数也无法避免散列冲突**

常用的解决散列冲突的两类方法：**开放寻址Open address    链表法 chaining**

### 开放寻址法

* 核心思想：若出现散列冲突，就重新探测一个空闲位置插入。

**线性探测Linear probing**

往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用，就从当前位置开始依次往后查找，直到找到第一个空闲位置插入【相当于一个数组往后查找还没有被占用的空间位置】。

* 散列表中查找：从散列值开始向后查找散列表，若到空闲位置还是没有，则该值不存在。

* 散列表中删除：不能单纯将要删除的元素设置为空【可以将删除的元素，特殊标记为 deleted。当线性探测查找的时，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。防止数据丢失。】

  PS: 这样当删除和插入越多，空闲越少，冲突越大，探测时间越长。

**二次探测Quadratic probing**

* 线性探测每次探测的步长是 1，它探测下标序列： hash(key)+0，hash(key)+1，hash(key)+2
* 二次探测探测的步长就变成了原来的“二次方”，探测的下标序列就是 hash(key)+0，hash(key)+1^2，hash(key)+2^2……

**双重探测Double Hashing**

* 双重散列，意思是使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……,从第一个开始使用，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。

<u>**当散列表中空闲位置不多时，不管哪种方法，冲突概率都会大大提高。**</u>

为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用**装载因子**（load factor）来表示空位的多少。

```
装载因子 = 填入表中的元素个数/散列表长度
```

装载因子越大，空位越少，冲突越多。

### 链表法

在散列表中，每个桶or槽对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。如下图所示

<img src='/images/img/链表法.jpg'>

* 当插入时，只需要通过散列函数计算对应的散列槽位，将其插入到对应链表中即可，插入时间复杂度为O(1).
* 当查找、删除一个元素时，同样通过散列函数计算对应的槽，然后遍历链表删除。

## 冲突解决办法选择

* 开放寻址：数据量较小、装载因子小，适合采用开放寻址法【 Java 中ThreadLocalMap使用开放寻址法解决散列冲突的原因】。
* 链表法：**适合存储大对象、大数据量的散列表，比起开放寻址法，更加灵活，支持更多的优化策略，比如用红黑树代替链表**。

## 工业级别水平的散列表

* 散列表的查询效率并不能笼统地说成是 O(1)，如果设计的不好，可能导致散列冲突发生概率升高 & 查询效率下降，查询效率急速退化到O(n)。
* 如果散列表中有 10 万个数据，退化后查询效率下降 10 万倍。【之前运行 100 次查询需要 0.1 秒，那现在就需要 1 万秒】。这样就可能因为查询消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，达到拒绝服务攻击的目的。这就是散列表碰撞攻击的基本原理。

### 散列函数设计

设计的好坏决定了散列表冲突概率大小&散列表性能。主要要点如下：

* 散列函数的设计不能太复杂（避免消耗过多计算时间）
* 散列函数生成的值要尽可能随机且均匀分布。（均分分布，即使出现冲突，每个槽内的数据也会相当平均）
* 其他因素：关键词长度、特点、分布、散列表大小等。

### 装载因子过大

* 动态扩容，增大散列表。（动态扩容过程中，需要重新计算每个位置存储的散列值）
  * 当散列表中数据逐渐删除时，可以动态缩容。
* 低效扩容避免
  * 达到极限时，避免一次性扩容。将扩容操作穿插在插入操作中，分批完成。
  * 这样边插入，边操作，将插入时间复杂度降低为O(1)。

# 哈希算法

如何用hash算法解决问题

* 定义：**将任意长度的二进制值串映射为固定长度的二进制值串**，这个映射的规则就是**哈希算法**，而通过原始数据映射之后得到的二进制值串就是**哈希值**。
* 一个优秀的哈希算法需要满足的几点要求：
  - 从<u>**哈希值不能反向推导出原始数据**</u>（所以哈希算法也叫单向哈希算法）；
  - 对输入数据非常敏感，原始数据只修改了一个 Bit，最后得到的哈希值也大不相同；
  - 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
  - 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。

## 应用

* 安全加密：最常用的加密算法MD5(Message-Digest Algorithm，消息摘要算法) 、 SHA (Secure Hash Algorithm 安全散列算法)、DES（Data Encryption Standard, 数据加密标准）、AES（Advanced Encryption Standard 高级加密标准）
  * hash值越长的hash算法，散列重提概率越低。
  * 没有绝对安全的加密，只有在有限资源和时间内无法攻破的加密。
  
* 唯一标识/数据校验：通过比较md5的值确定文件是否被篡改。

* 散列函数计算hash值

* 负载均衡：**通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。** 这样就可以把同一个 IP 过来的所有请求都路由到同一个后端服务器上。

* 数据分片

* 分布式存储：hash运算对数据区hash值，然后对机器个数取模，决定存储的机器编号。

  PS:当机器数量增加时，所有缓存需要重新计算hash值存到新机器，这样之前的缓存都会失效，所有请求都会直接穿透缓存，查库，发生雪崩效应。---- 一致性hash算法

## 一致性哈希算法

* 假设有 k 个机器，数据的哈希值的范围是 [0, MAX]。将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。
* 当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。

# 树

* 定义：每个元素叫做“节点”，相邻节点的连线关系叫做“父子关系”。【父节点、子节点、兄弟节点、根节点、叶节点】
* 节点高度：节点到叶节点的**最长路径**（边数）
* 节点深度：根节点到节点经历的**边的个数**
* 树的节点层数：节点深度+1
* 树的高度：根节点的高度

## 二叉树

* 定义：每个节点最多有两个子节点，分别是**左子节点和右子节点**。不过，二叉树并不要求每个节点都有两个子节点。
* 满二叉树：除叶子节点之外，每个节点都有左右两个子节点。
* 完全二叉树：叶子节点都在**最底下两层**，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大。

两种存储方式：基于指针or引用的二叉链式存储法、基于数组的顺序存储法

* 链式：每个节点3个字段，一个存数据、另外两个存左右子节点指针。【根节点就可到达所有节点】

* 顺序：根节点存在数组下标为i=1的位置，左节点：2\*i=2，右子节点：2\*i+1=3的位置。以此类推。【上述情况为完全二叉树，仅浪费一个位置，非完全二叉树浪费空间较多】

  **完全二叉树使用顺序数组存储，比链式节省空间（不需要存左右指针）**

### 遍历

| 遍历方式 | 遍历顺序 | 时间复杂度(每个节点最多遍历两次) |
| -------- | -------- | -------------------------------- |
| 前序     | 根左右   | O(n)                             |
| 中序     | 左根右   | O(n)                             |
| 后序     | 左右根   | O(n)                             |

遍历方式递归进行，对于每一个节点都执行同样遍历顺序

## 二叉查找树 Binary search Tree

为了实现快速查找、插入、删除而产生的二叉查找树。

相比于散列表，可以通过中序遍历直接输出有序数据；查询时间复杂度稳定为O(logn);同时，可避免较大的散列冲突。

* **结构要求：树中任意节点的左子树的每个节点值都小于等于当前节点，右子树所有值都大于等于当前节点**

* **查找**：根节点开始，与查找值比较：1）等于，返回；2）小于查找值，右子树；3）大于查找值，左子树；
* **插入**：根节点开始，与插入值比较，1）小于插入值，且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。2）大于插入值，且节点的左子树为空，新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。
* **删除**：
  * 待删除节点无子节点，将其父节点指向该节点的值改为Null 。
  * 待删除节点有一个子节点，将待删除的点的父节点直接指向子节点 。
  * 待删除节点有两个子节点，找到这个节点右子树最小节点换到要删除节点。

* **中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效**。

支持重复数据的二叉搜索树

* 二叉查找树中存储的可能是包含多个字段的对象，可将其中一个字段作为key来构建二叉查找树，其他字段叫做卫星数据。
* 若遇到相同数据需要存储，将其放到当前节点的右子树【查找及相关操作时，考虑数据相同的情况，一直找到叶节点为止】

<u>**二叉查找树时间复杂度与树高成正比O(height)**</u>

**完全二叉树高度≤log2n,n为节点个数**

## 红黑树-平衡二叉查找树的一种

​       二叉树支持快速插入、删除、查找，各操作时间复杂度与树高成正比，理想情况下O(logn), 但在频繁动态更新过程中，会出现树高远大于log2n的情况，导致各操作效率下降，极端情况下时间复杂度退化到O(n)。【为了解决上述动态更新出现复杂度退化的问题，提出平衡二叉树】

* **平衡二叉树**：二叉树中任意一个节点左右子树高度相差不能大于1。

  PS: 完全二叉树、满二叉树都属于平衡二叉树，非完全二叉树也可能是平衡二叉树。

* **平衡二叉查找树：**平衡二叉树+二叉查找树。【AVL树：最先发明的平衡二叉查找树】

  PS: 很多平衡二叉查找树并没有严格符合上述定义【红黑树：从根节点到各叶子节点的最长路径可能比最短大一倍。】。因此，**<u>只要树高不比Log2n 大很多，仍然可以认为是平衡二叉查找树。

### 红黑树定义 R-B tree

**"近似平衡而不是严格平衡"**的二叉查找树，性能更加稳定。树中有两类节点：红色、黑色。同时需要满足以下要求：

```
* 根节点是黑色的。
* 每个叶子节点都是黑色的空节点NULL【叶子节点不存数据，同时简化红黑树代码】。
* 任何相邻节点都不能同时为红色【红色节点被黑色节点隔开，**相邻可以为黑色**】。
* 从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。
```



### 近似平衡证明

要证明红黑树是近似平衡的，只需证明红黑树的高度是否比较稳定地趋近 log2n 

* **黑树高度：**若将红色节点从红黑树中去掉，部分节点没有父节点，它们会直接将父节点的父节点作为当前父节点【此时二叉树可能会产生4叉树】

<img src='/images/img/红黑树去掉红色节点.jpg'>

根据上述红黑树的定义第4条，从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。【**<u>完全二叉树的高度近似 log2n，这里的四叉“黑树”的高度要低于完全二叉树，所以去掉红色节点的“黑树”的高度不会超过 log2n</u>**】。

* **加入红色节点后红黑树高度**：因为红色不能相邻，因此有一红必有一黑，黑色最多log2n，<u>**加入红色后，高度近似2log2n**</u>

### 红黑树的实现

* 基本思想：遇到什么样的节点排布，就对应去调整。【按照固定的规则调整，就能将非平衡红黑树调整为平衡】
* 红黑树的定义中的第三、第四两点要求可能在插入、删除节点过程中被破坏。

**左旋 rotate left**:【围绕某个节点左旋】：某个节点归入其左子树，其右子树作为根节点，右子树的左子树作为“某节点”右子树。

**右旋 rotate right**:【围绕某个节点右旋】：某个节点归入其右子树，其左子树作为根节点，左子树的右子树作为“某节点”左子树。

左右旋示例图如下：

<img src='/images/img/红黑树左右旋.jpg'>

#### 插入操作的平衡调整

<u>**红黑树插入节点必须是红色的，且二叉查找树新插入的节点都放在叶子节点上。**</u>

因此关于插入的两种平衡操作调整如下：

* 两种特殊情况：
  * 若插入的是根节点，将红色调整为黑色即可【满足定义】
  * 若插入节点父节点是黑色，无需操作。

* 需要调整的情况：【关注节点：正在处理的节点】

  * case1：**若关注节点是a，叔叔节点d是红色**，依次执行：

    * 将关注节点a的父节点b和叔叔节点d都设置成黑色；
    * 将关注节点a的祖父节点c设置成红色；
    * 关注节变成a的祖父节点c；
    * 跳到CASE2 或3

  * case2：**若关注节点是 a，叔叔节点 d 是黑色（父节点不一定是黑色），关注节点 a 是其父节点 b 的右子节点**，我们就依次执行下面的操作：

    * 关注节点变成a的父节点b；
    * 围绕新的关注节点b左旋；
    * 跳到CASE3

    <img src='/images/img/红黑树case2.jpg'>

  * case3：**若关注节点是 a，叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的左子节点**，我们就依次执行下面的操作：

    * 关注节点 a 的祖父节点 c 右旋；
    * 将关注节点 a 的父节点 b、兄弟节点 c 的颜色互换。
    * 调整结束。

    <img src='/images/img/红黑树case3.jpg'>

### 红黑节点的意义及区分

区分

* 新插入节点都是红色的，一棵树不会全黑
* 根节点、叶节点一定黑色
* 每个红色节点的两个子节点都是黑色（隔开）

## 递归树计算时间复杂度

* 递归问题分解过程：根节点为原问题，逐渐向下分解，直到叶节点为已知结果的问题，最后逐步向上递归。
* 时间复杂度估算：树高h，每层消耗n，总复杂度O(n*h)
  * 归并：满二叉树高--- h=log2n，因此O(nlogn)
  * 快排：非满二叉，但树高h约等于logn，因此O(nlogn)

## 线段树

* 定义：类似于二叉树，每个父节点存两个子节点的值，构成线段树的结构。
* 与树状数组的区别：树状数组能解决的问题线段树都可以解决，但树状数组的系数比线段树少很多。
* 两者构建：线段树：类似于二叉树，每个父节点存两个子节点的值，构成线段树的结构。

## 树状数组 

* binary Index Tree/ 二进制下标树 Fenwick Tree

* 定义：数组来模拟树形结构，解决大部分基于区间上的更新及求和问题。数组的下标模拟二进制的方式来进行求和计算。

* 功能及时间复杂度

  * 单点修改：更改数组中的一个元素值

  * 区间查询：查询一个区间内所有元素的和

    PS: **两者时间复杂度均为：O（logN）**【PS:** **相比于直接数组单点修改的O (1)** **和区间求和的O(n),** **或者前缀和的单点修改的O (n)** **和区间求和的O(1)****，当两种功能都需要实现的时候树状数组综合较好】**

* 区别于线段树

  * 树状数组能解决的线段树都可以解决，但树状数组系数比线段树少很多。
  * 树状数组比传统数组读写快，但复杂区间问题不容易解决。

* 树状数组的构建 --- 比正常数组多一个下标0,因此第一个0不用。

  利用“二进制”进行分段。树状结构大致如下：

  <img src="/images/img/树状数组实例图.png">

   例: 求前11项的和

     1） Step1：计算11转2进制，得到1011

     2）Step2：前11项的和转化为 (0000，1000],(1000,1010],(1010,1011]

     3）对上述三个区间求和，三个区间的由来：不断去掉最右边的1改为0

  <img src="/images/img/树状数组实例区间图.png">

  ​    **Ps:** 前8项的和可以参照上图，(0000，0100],(0100,0110],(0110,0111]

  * Lowbit(x) 的定义：二进制最低位置的1和其后的0一起作为lowbit（x），那么我们用Ci 维护区间 (Ai-lowbit(Ai),Ai]，这样显然查询前n项和时需要合并的区间数是少于log2N的。

  ​       PS: 关于上述Ci 维护区间 (Ai-lowbit(Ai),Ai] 的定义，不要受到最顶上二叉树图的影响

  * lowbit 计算：lowbit(x) = (x) & (-x)

    说明：如公式：计算机的有符号数一般以补码形式存储，-x相当于按位取反+1（原码=反码+1），因此如果x=1000,-x=0111+1=1000,前面完全取反。【不可能绕开最低位的1】---- 按位与，两个位置都是1，最后的结果才是1，因此可以直接计算lowbit

  * C0= (A0-lowbit(A0),A0] = (0,0]  ---- 0000-0000

  ​       PS:假如数组第一个0不用，计算方式还是 Ci 维护区间 (Ai-lowbit(Ai),Ai]

  * C1 =(A1-lowbit(A1),A1] = (0001-00001,0001] = (0,1] = A[1]

  * C2 =(A2-lowbit(A2),A2] = (0010-0010,0010] = (0,2] = A[1]+A[2]

  * C3 = (A3-lowbit(A3),A3] = (0011-0001,0011] = (2,3] = A[3]

  * C4 =(A4-lowbit(A4),A4] = (0100-0100,0100] = (0,4]=A[1]+A[2]+A[3]+A[4]

  * C5 = (0101-0001,0101] = (4,5] = A[5]

  * C6 = (0110-0010,0110] = (4,6] = A[5]+A[6]

  * C7 = (0111-0001,0111] = (6,7] = A[7]

  * C8 = (10000-10000,10000] = (0,8] = A[1]+…+A[8]

    C[i] = A[i-2^k+1]+A[i-2^k+2]+…+A[i] //k 为二进制最低位到最高位连续0的长度

* 树状数组的更新

  ---爬树过程：一路往上更新，直到树状数组容量值

  * 例子：以二进制数100110 为例

  ​       (100100,100110] 最小区间；（100000，1010000] （100000，110000] （000000，100000]； 为lowbit的较大范围区间，如下图：**左边为****lowbit计算方式**，**右边为每次加上lowbit后进位**, 这样更新区间不会超过log（maxN），能够以O（logN）进行查询的和修改的数据结构便完成了。

  <img src="/images/img/树状数组更新示例.png">

## 前缀树/字典树Trie

* 定义：字典树（字典套字典递归）Hash Tree 变种，查询快、内存消耗小，多用于处理字符串匹配情况。

* 优点：利用字符串公共前缀减少查询时间、最大限度减少无谓字符串比较，查询效率高于hash

* 典型应用：统计、排序和保存大量字符串（不限于字符串），经常被搜索引擎用于文本词频统计。

  PS: 其他应用--单词自动补全、拼写检查、最长前缀匹配、前缀预测单词、统一前缀的全部键值等

* 主要操作：插入、查找、删除

  * 查找某字符串时间复杂度：O(k), k 为字符串长度。
  * 在字符串前缀重合比较多的情况适用，否则对内存消耗极大。

* **主要性质**

  * 根节点不包含字符，其他每个节点只包含一个字符
  * 从根节点到某个节点开始，路径上经过的字符连接起来为该节点对应字符串
  * 每个节点的所有子节点包含字符不相同（都含有26个可能的子节点表示26个字母）

* 前缀树例子:假设有b,abc,bcd,abcd,efg,hii 6个单词创建就得到下述前缀树

  <img src="/images/img/前缀树例子.png">

* 实现步骤

  * 字典树实质：字典套字典，并在结束位置加上一个字符表明该字符串已结束。

  * 更新字典时加上递归遍历字典。

    ```python
    class Trie:
        # 字典树：根节点不存值，每个根节点的子节点都是26个字符中的1个，字典套字典
        def __init__(self):
            self.tree = dict()
    
        # 字典套字典层层遍历
        def insert(self, word: str) -> None:
            tree = self.tree
            for s in word:
                if s not in tree:
                    tree.update({s: {}})
                tree = tree[s]
            # 字典单词结束位置记录：#
            tree.update({"#": True})
    
        def search(self, word: str) -> bool:
            tree = self.tree
            for s in word:
                if s not in tree:
                    return False
                tree = tree[s]
            return True if "#" in tree else False
    
        def startsWith(self, prefix: str) -> bool:
            tree = self.tree
            for s in prefix:
                if s not in tree:
                    return False
                tree = tree[s]
            return True
    ```

* 具体实现 ---leetcode 307

  * 建立BIT: BIT =[0]+原数组 # 2进制原因第一个下标不用
  * 建立lowbit函数：x &(-x)
  * 初始化BIT：BIT下标index 从1开始，计算 j = index+lowbit(index),若j在BIT 长度以内，BIT[j]+=BIT[i]
  * 自定义前缀和函数presum，通过lowbit对下标递减更新需要求和的区间
  * 区间求和 presum（right）-presum(left-1)
  * 更新某个位置的元素：sumRange（x,x)

  ```
  class BIT_tree:
      def __init__(self, nums):
          self.BIT = [0] + nums  # 构造bit树，bit树比原来数组多一个空间，第一个0下标不用
          # 跳过第一个0后使用lowbit函数来更新整个树状数组
          for i in range(1, len(self.BIT)):
              j = i + self.lowbit(i)
              if j < len(self.BIT):  # 加上lowbit之后放大的区间在范围内，则更新
                  self.BIT[j] += self.BIT[i]
  
      def lowbit(self, x):
          return x & (-x)
  
      # 自定义前缀和函数
      def preSum(self, index: int) -> int:
          i = index + 1  # index 为原来nums当中的下标，+1为bit中的位置
          preSum = 0  # 前缀和
          while i:
              preSum += self.BIT[i]
              i -= self.lowbit(i)
          return preSum
  
      # 区间求和
      def sumRnage(self, left: int, right: int) -> int:
          return self.preSum(right) - self.preSum(left - 1)  # right 对应的前缀和
  
      # 更新元素位置
      def update(self, index: int, val: int) -> None:
          preVal = self.sumRnage(index, index)  # 求BIT 当中原来的值
          diff = val - preVal  # 变更值
          pos = index + 1
          while pos < len(self.BIT):
              self.BIT[pos] += diff
              pos += self.lowbit(pos)
  ```

  PS: # 208/211 如何按照题意进行递归搜索

# 堆Heap

堆排序:原地、时间复杂度O(nlogn)的排序算法。存储堆，多用数组进行，方便快速查找。

堆的定义：

* 堆是一个完全二叉树。
* 堆中**每个节点的值都必须大于等于or小于等于其子树中每个节点的值**
  * 小根堆：父节点**小于等于**所有子节点
  * 大根堆：父节点**大于等于**所有子节点

## python 实现：heapq库

* **默认为小根堆 == 最小值为父节点，根堆list第一个元素永远最小**

```
import heapq

nums, k = list(), 10
q = [(-nums[i], i) for i in range(k)] # 初始化k窗口大小的list
print(q)
heapq.heapify(q) # list 转为堆，默认小根堆
print(q)
# 入队：heapq.happush(q, (-nums[i], i))  # 入队新的元素
# 出队：heapq.heappop(q)
```

## 堆支持的操作

* 插入元素：将新插入的节点与其父节点比较大小，根据大小根堆的特性，对不满足的情况进行互换，重复操作，直到新插入节点换到适当的地方。
* 删除堆顶元素：堆顶一定是最大or最小值，删除堆顶元素后，**将堆底最右侧元素移到堆顶，再根据大小根堆特性，与左右子树的点迭代比较替换，直到满足条件or到达堆底。**【这样操作使得堆化后能满足完全二叉树特性】

对于完全二叉树（树高为log2n),堆化后插入和删除的时间复杂度都是O(logn)

## 堆排序-O(nlogn)

* 堆排序由两步构成：建堆+排序

* 建堆时间复杂度O(n),排序O(nlogn),总体O(nlogn)
* 因为排序过程可能存在顺序互换，因此是不稳定排序。

### 建堆

将数组原地建成一个堆，参考上述代码中 heapq.heapify(list),省略了具体过程。

* 实际：可从第二个元素开始，通过不断插入元素的方式构成堆
* 时间复杂度：O(n)

### 排序

完成建堆之后，list中数据已完全按照大根堆来组织【堆顶元素最大】

**排序步骤**

* **将堆顶元素与最后一个交换，当前最大值就放到了数组下标为n的地方**
* **堆化剩下n-1个元素**
* **重复上述两步，直到最后堆中只剩下下标为1的元素结束排序**

## 堆应用

* 优先级队列：优先级最高的最先出队。

  根据堆的特性，数据入堆（插入）后，根据优先级（eg：数据大小）会被放到队列适当的位置，每次出队的都是堆顶元素。

* 获取topK【下面是前k大，前k小相反】时间复杂度： O(nlogK)

  维护一个大小为k的小根堆，顺序遍历数组，堆未满则插入，堆满后，将数据与堆顶比较：

  * 大于堆顶元素，则删除堆顶元素后将该数据插入。
  * 小于堆顶元素，则不做任何处理。

* 计算流数据中的中位数：【求其他位置的数，只需要修改两个根堆的数据量】

  **维护一个大根堆【存储前半部分数据】，一个小根堆【存储后半部分数据】，且小根堆中的数据都大于大根堆中数据。**

  * n 个数据，从小到大排列，**前n/2 or n/2+1在大根堆，后n/2在小根堆**，大根堆堆顶即为中位数
  * 加入新数据后对两个堆的调整：
    * 若新数据≤大根堆堆顶，则将新数据插入大根堆；
    * 若新数据≥小根堆堆顶，则将新数据插入小根堆；
    * 若不满足上述两个堆的大小条件，则移动两个堆中的元素来进行数据平衡；

# 图Graph

非线性表数据结构

## 明确几个定义

* 顶点vertex：图中的元素。
* 边edge：顶点之间的关系。

* 无向图：不带方向的图。
  * 度degree：顶点相连的边条数。
* 有向图：带方向的图，顶点只能顺边的方向遍历访问。【微信、微博】
  * 入度 in-degree：多少条边指向这个顶点。
  * 出度 out-degree：多少条边以这个顶点为起点指向其他顶点。
* 带权图weighted graph：每条边都有一个权重值，用来表示两个用户之【QQ聊天亲密度】

## 图的存储方法

**邻接矩阵Adjacency Matrix**

* 基于二维数组：
  * 无向图：若顶点i和j之间有边，则A\[i][j]和A\[j][i]均为1，其他为0；
  * 有向图：若顶点i指向j有边，则A\[i][j]为1，其他为0；
  * 带权图：上述情况中的值从1修改为对应的权重。
* 简单直观，但浪费存储空间。【无向图中的重复、稀疏图边少】

**邻接表存储Adjacency List**

* 类似于散列表，每个顶点对应一条链表【存储与这个顶点相连的其他顶点】。

* 相对而言，邻接表遍历和操作更加消耗时间& 链表存储方式堆缓存不太友好 & 链表长短不定。

* 可将链表更换为其他更加高效的结果，如平衡二叉查找树等。

* 逆邻接表，可以用存储类似（微博：被关注对象), 简化查询过程。


## 二分图

* [模型定义](https://web.ntnu.edu.tw/~algo/BipartiteGraph.html)：设G=(V,E)是无向图，若顶点V可分割为两个互不相交的子集(A,B),且图中的每条边（i,j)所关联的两个顶点i和j分别属于这两个子集，则图G为二分图。【decision variable 和变量值各在一个子集，通过边相连】
* matching：二分图中的任意两条边，不连接在相同的顶点上的情况。
* maximum matching：若二分图中所有matching等于变量个数，那么该解有效（变量和值一一对应）
  * maximum matching的寻找过程：从任意边开始，对所有未match的边进行替换，直到无法再优化。






------

