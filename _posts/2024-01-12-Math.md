---
title: 'Math-notes'
date: 2024-01-12
permalink: /posts/2020/12/Math-notes/
tags:
  - cool posts
  - category1
  - category2
---

Quick Review of: Linear Algebra、Probability Theory、Mathematical Statistics(include: Statistical Inference)、Optimization、etc.

# Linear Algebra

**标向量&矩阵&张量**

标量Scalar-0维 -> 向量Vector-1维-> 矩阵Matrix-2维-> 张量tensor-n 维

* 多个标量组成向量；多个向量组成矩阵；
* 张量Tensor：矩阵中的标量替换为向量得到n维高阶矩阵。
* **向量：语音信号；矩阵：灰度图像；张量：RGB图像(3通道) & 视频。**

**范数Norm**

对于单个向量大小的度量，将向量映射为一个非负值。

* **L1 norm**:   向量中所有元素绝对值之和。
* **L2 norm**：计算向量长度。【向量终点到起点的距离。】

**内积Inner product**

计算两个向量的关系，**对应元素乘积求和，**内积表达式如下：
$$
<x,y>=∑_ix_i.y_i
$$

* 两个向量之间的相对位置--夹角。
* 正交orthogonality：<x,y> = 0 【向量x,y夹角90度。】
  * 两正交向量线性无关，相互独立【无法映射到对方】

**线性空间Linear space**

* 线性空间：**具有相同维度向量的元素**，且定义了加法和数乘等结构化运算的集合。
  * **线性空间中，任意一个向量代表的都是 n 维空间中的一个点；反过来， 空间中的任意点也可唯一地用一个向量表示**
* 内积空间：定义了内积运算的线性空间。内积空间中，一组两两正交的向量构成这个空间的正交基-orthogonal basis。
  * 若正交基中基向量L2 norm单位长度为1，这组正交基为标准正交基。
  * 描述内积空间的正交基可以有多个，不唯一。

**特征值&特征向量**

特征值-eigenvalue和特征向量-eigenvector：描述矩阵的⼀对重要参数。对于给定矩阵 A，假设其特征值为λ，特征向量为 x，它们的关系如下：Ax=λx

**矩阵代表向量的变换**(对原始向量同时施加方向和尺度变化)。特征向量仅对矩阵尺度变化有影响，对矩阵方向无影响。【特征值：特征向量尺度变化系数。】

* 特征值分解：求解给定矩阵的特征值和特征向量的过程。
  * 能进行特征值分解的矩阵必须是 n 维方阵。
  * 特征值分解推广到所有矩阵上，就是奇异值分解。

# Probability Theory

**无处不在的可能性**：对随机事件发生可能性进行规范的数学描述。

**条件概率conditional probability**

随机事件 A、B，A B 已经发生的条件下发生的概率为：**P(A|B)=P(AB)/P(B)**

* 若**两个随机事件相互独立**，则条件概率为自生概率: **P(A|B)=P(A)**

**联合概率joint probability**

两个随机事件同时发生的概率：**P(AB)**

* 若**两个随机事件相互独立**，则: **P(AB)=P(A)⋅P(B)**

**贝叶斯定理Bayes‘ theorem**
$$
P(A|B)=P(AB)/P(B)=P(B|A)*P(A)/P(B)
$$

* 第二个等式来源-P(B|A)=P(AB)/P(A)

* P(A)-先验概率(prior probability): 根据经验，在实验前就可以得到的概率。
* P(A|B)-后验概率(posterior probability): 已发生事件，向计算这件事发生原因是由某个因素引起的概率。

概率描述随机事件可信程度：预报明天下雨概率85%，则明天下雨这个事件可信度是 85%）。

**随机变量Random Variable**

* 离散随机变量每个可能取值的概率都大于 0。
  * **概率质量函数**-probability mass function: 离散型随机变量取值和概率一一对应关系。【非真实概率，而是不同取值可能性之间的相对关系】
  * **概率密度函数**-probability density function: 连续型随机变量的概率质量函数。【连续性数据需要积分后计算，否则分母趋于无穷后均为0】

**期望Expected value**: 均值：E(x) = x*p(x)

**方差Variance**：随机变量值与其期望的偏离程度。方差越大表示数据越分散。

**协方差Covariance**：描述两个r.v 之间的相互关系。



## 常见分布

- **伯努利分布**-Bernoulli distribution：**随机试验的结果是仅为两类**：发生or不发生。概率分别为 p 和 1−p。

- **二项分布**-Binomial distribution：满足参数为 p 的伯努利分布的随机试验独立重复 n 次，事件发生的次数即满足参数为 (n,p) 的二项分布。二项分布的表达式可以写成：
  $$
  P(x=k)=C_n^k.p^k.(1-p)^{n-k},0≤k≤n
  $$

- **泊松分布**-Poisson distribution：参数为 λ的泊松分布表达式为 
  $$
  P(x=k)=λ^k⋅e^{−λ}/(k!)
  $$

  * 二项分布中的 n 很大且 p 很小时，其概率值可以由参数为 λ=np 的泊松分布的概率值近似。

- **均匀分布**-uniform distribution：在区间 (a, b) 上满足均匀分布的连续型随机变量，其概率密度函数为 1 / (b - a)，这个变量落在区间 (a, b) 内任意等长度的子区间内的可能性是相同的。

- **正态分布**-normal distribution：
  $$
  f(x)=1/\sqrt{2π}σ.e^{-(x-μ)^2/2σ^2}
  $$

  * μ=0，σ=1时，上式为标准正态分布。

# Mathematical Statistics

**根据观察实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理估计和判断**

* 研究对象：未知分布的随机变量。

* 研究方法：对随机变量独立重复观察，**根据结果推断数据分布**。

TIPS: **数理统计可以看成是逆向的概率论**。

* 概率论：根据已知摇奖规律判断一注号码中奖的可能性
* 数理统计：根据之前多次中奖 / 不中奖的号码记录推测摇奖的规律。

数理统计的任务：根据样本推断总体的数据特征。

* **样本Sample**：有限的数据集。通常由对总体多次独立重复观测得到。

* **总体Population**：观察对象所有可能的取值。

## 统计推断-Statistical Inference

两类基本问题：Estimation theory、Hypothesis testing

### 参数估计

通过随机抽样来估计总体分布

**最大似然估计法Maximum Likelihood Estimation**

在给定概率分布模型的条件下，进行**模型参数估计**。

* 概率分布的模型参数θ已知，P(x;θ) 即为确定分布模型下的随机变量概率；

* 随机变量x已知，P(x;θ) 即为不同模型参数θ下，出现给定样本x的概率。

最大似然估计：求解**使抽样结果P(x;θ) 发生概率L(θ)最大的模型参数θ的值**

argmax L(θ) = argmax L(x_1,x_2,...,x_n;θ)=argmax P(x_1,x_2,...,x_n;θ)

* 样本似然函数：L(θ)。大多数情况下，n次抽样之间满足i.i.d

**最大似然估计在参数估计过程中的步骤**

* 建立似然函数L(θ)。
* 对L(θ) 取对数，logL(θ)。
* logL(θ)对θ求导并令其为0，计算极值点。
* 通过极值点求解得到模型参数θ。

**参数估计的三个基本标准**

* **无偏性**：估计量的数学期望等于未知参数的真实值；
* **有效性**：无偏估计量的方差尽可能小；
* **一致性**：样本容量趋近于无穷时，估计量依概率收敛于未知参数真实值。

**置信区间confidence interval**：对总体反复抽样多次，每次得到容量相同的样本，根据每一组样本值都可以确定出一个置信区间 (θ−,θ¯)，其上界和下界是样本的两个统计量，分别代表了置信上限和置信下限。【落在置信范围内的可信度更高】

### 假设检验

通常包含两个假设：H0-Null Hypothesis、H1-Alternative Hypothesis。理想情况：**H0为真并接受这个假设**。由于检验基于样本做出，可能导致两类错误：

* type I error：H0 为真，但被拒绝。
* type II error：H0为假，但被接受。

**具体证明方式**

要证明 H0 为真，可证明H1 为假(举出1个反例即可)。

PS: 此处的反例为“小概率事件(发生概率小于 1% 的事件)"。**若某次观测出现小概率事件，那么可以认为这不是真正意义上的小概率事件，H0 被推翻。**

PPS:  拒绝H0  == 接收H1, vice versa。

# Optimization

最优化理论：给定目标函数，判断最值是否存在并求解对应最值**

* **目标函数**：<u>极小值-min f(x)，极大值-max f(x) 或 min −f(x)</u>
  * 最优化算法目标：找到全局最优值。多数时候找到**可接受局部最优值**即可。【减少运算资源要求、时间消耗等】

**无约束优化unconstrained optimization**

对目标函数中的变量没有限制。

* 常用求解方式：梯度下降法gradient descent
  * 梯度方向：目标函数导数的反方向
  * **多元函数沿其负梯度方向下降最快**
  * 重要因素：每次更新的步长【过小无法跳出局部最优，过大错过全局最优-->**优化算法可根据迭代过程逐步缩短步长**】

**约束优化constrained optimization**

对目标函数中的变量明确了一定限制。【objective function subject to xxxxxx】

**批处理batch processing**

计算每个样本上目标函数的梯度，再将不同样本的梯度求和结果作为本次更新中目标函数的梯度。【每次更新都要遍历所有的样本，运算量大】

**随机梯度下降法 stochastic gradient descent**

每次更新只使用一个样本，在不断迭代的更新过程中实现对所有样本的遍历。有趣的是，事实表明当训练集的规模较大时，随机梯度下降法的性能更佳。

梯度下降法只用到了目标函数的一阶导数（first-order derivative），并没有使用二阶导数（second-order derivative）。一阶导数描述的是目标函数如何随输入的变化而变化，二阶导数描述的则是一阶导数如何随输入的变化而变化，提供了关于目标函数曲率（curvature）的信息。曲率影响的是目标函数的下降速度。当曲率为正时，目标函数会比梯度下降法的预期下降得更慢；反之，当曲率为负时，目标函数则会比梯度下降法的预期下降得更快。

梯度下降法不能利用二阶导数包含的曲率信息，只能利用目标函数的局部性质，因而难免盲目的搜索中。已知目标函数可能在多个方向上都具有增加的导数，意味着下降的梯度具有多种选择。但不同选择的效果显然有好有坏。








------

